[["index.html", "Notes for SCMA450: Data Modeling and Computing Chapter 1 Overview of Business Analytics and R Programming", " Notes for SCMA450: Data Modeling and Computing Dr. Liang (Leon) Xu 2021-09-07 Chapter 1 Overview of Business Analytics and R Programming Ever wondered how much data is created every day? Or perhaps youre keen to know how much data the average person uses at home? Thanks to the invention of mobile technology like smartphones and tablets, along with innovations in mobile networks and WiFi, the creation and consumption of data are constantly growing. In fact, there are 2.5 quintillion bytes of data created each day at our current pace, but that pace is only accelerating with the growth of the Internet of Things (IoT). To put things in perspective, Google on average processes more than 40,000 searches EVERY second (3.5 billion searches per day); Snapchat users share 527,760 photos every minute of the day; Users watch 4,146,600 YouTube videos every minute of the day; We send 16 million text messages and 103,447,520 spam emails every minute. Once you are aware of all the data you create as a single individual, you can start to imagine just how much data we collectively generate every single day. With such a tremendous amount data generated, the business world demands people equipped with the tools to analyze big data. In particular, data Analytics is a scientific process of combining data and models into insights for the purpose of making better decisions. For example, Netflix sets up an open competition to improve its algorithm to predict users rating of a movie, with a prize of 1million dollars. Similarly, Zillow (a real estate market place company) hosts a open competition with prize of 1million dollars to improve its prediction for house price. The scope of business analytics includes Descriptive Analytics: Condense tabular data into charts/reports/dashboard to be understandable. Predictive Analytics: use historical data to predict future. Prescriptive Analytics: Prescribe an action a business should take. The job market is also great for students with business analytical skills. Based on the search on keyword data analytics on Indeed.com in May 2020, the data analytics field has greatest amount of position in US. So, what are the skills needed to be competitive in the business analytical position? We have survey the data scientists and marketing analytics at Ameritas (a large insurance company). This is their response: If a course is focusing on data visualization, then I would highly recommend Tableau. If they are looking at data cleansing, then teaching either R or Python to their students, even on an introductory level, will give them a considerable boost when looking for positions. An in-depth understanding of Microsoft Excel should be understood, but it is much harder to recreate accessible and powerful commands in R or Python. In this course, we will focus on business analytical application with R. We will also talk a little about Python to give you a taste of the two different programming languages. Once you finished the course, we will fall in love with R because it is such a powerful tool for analyzing big data. R is a free software environment for statistical computing and graphics. R has the richest packages to perform data analysis. Around 12,000 packages available in CRAN (Comprehensive R Archive Network, open-source repository of R packages). R is open source: you can use the 12,000 packages for free! It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS. To download R, please choose your preferred CRAN mirror at: https://www.r-project.org/. RStudio is an Integrated Development Environment (IDE) for R, a programming language for statistical computing and graphics. You can write and run R code with Rstudio. Here is the website to download Rstudio: https://www.rstudio.com/products/rstudio/download/. The objective of this course is: Get excited about writing code! This is one of my most important goal since we have the stereotype that business major only know to write ppt, not code. This is not true. Everyone should learn to code in the 21th century. Learn the R syntax. Just like a language, R is a computer language and we need to know the basic grammar R language to be able to give instruction to computers. Write R Code for Data Analytics Independently. It is common that students follow along well in class, but cannot write code independently. So we will have many exercise to help you code independently. "],["chapter01.html", "Chapter 2 Define Variables 2.1 Use R as a Calculator 2.2 Define variables 2.3 Basic Variable Modes: integer, numeric, character, logical, factor. 2.4 Exercise 1: Greeting from R 2.5 Exercise 2: Mortgage Calculation 2.6 Summary", " Chapter 2 Define Variables Here is the guideline in how to read this document: The shaded area (with green backgroud) is R code. Within the shaded area, # The hashtag means This is a comment. The comment is to help yourself and others to understand what the code does. The line right next to the code (starting with ##) is the output of the code. 2.1 Use R as a Calculator R can be used as a powerful calculator by entering equations directly at the prompt in the command console. Simply type your arithmetic expression and press ENTER. # R is highly interactive, you can get instant feedback while coding. 3+6 ## [1] 9 # To execute the above command, put the mouse anywhere in the line and hit &quot;Ctrl+Enter&quot;. # the product of 2 and 4 2*4 ## [1] 8 # 6 divided by 7 6/7 ## [1] 0.8571429 # 2 to the power of 10 2^10 ## [1] 1024 # x%%y return the reminder of x divided by y 10%%3 ## [1] 1 2.2 Define variables The most basic concept in programming is variable. A variable allows you to store a value (e.g. 4) or an object (e.g. a function) in R; and you can then later use this variables name to easily access the value or the object that is stored within this variable. The &lt;- symbol (&lt; and -) or = symbol means Set the variable on the left equal to the thing on the right. In RStudio the keyboard shortcut for the assignment operator &lt;- is Alt + - (Windows) or Option + - (Mac). For example, you can save you age to a varaible my_age through the following code: # Assign the variable &quot;my_age&quot; the value of 30. # The &quot;&lt;-&quot; or &quot;=&quot; symbol assigns right hand side (RHS) to the left hand side(LHS) my_age&lt;-30 You have just defined a variable my_age with value of 30, which is stored in the computer now. Now look at the up right corner of R-studio, you should see the defined variable. To print my_age on the screen, type in my_age (without quotes) and hit Ctrl+Enter. It will show you the value. my_age ## [1] 30 Or we can use the print() function to print the variable on the screen. print(my_age) ## [1] 30 Now, you need to define two more variables to save the age of the father and mother: mother_age with value of 60, and father_age with value of 65. mother_age&lt;-60 father_age&lt;-65 The defined the variables are saved in computer memory, and you can use the defined variable for calculation. # How much the father is older than the child? father_age-my_age ## [1] 35 # How much the father is older than the mother? father_age-mother_age ## [1] 5 We can define a new variable through the calculation on the existing variables. Define a variable avg_age with the value the average age of the parents. avg_age=(father_age+mother_age)/2 avg_age ## [1] 62.5 Variable names are like our human names, which uniquely identify the value stored in computer. It is best practice to use variable names that can be easily understood. Try to use English word and \"_\" to make it self-explanatory. Also, try to use lower case consistently because R is case-sensitivity. It will be hard to remember which letter is upper case and which is not. 2.3 Basic Variable Modes: integer, numeric, character, logical, factor. In the physical world, we have different mode of data. E.g., 30 is a an integer (a more specific type of numeric); 51.67 is a number; Leon is a character; TRUE/FALSE is logical; The color like blue/green/red is categorical. In R such categorical variable is called factor. R use different variable mode to store these different type of data. 2.3.1 Integer and numeric Recall that we define my_age as a number. Type class(my_age) to checkt the model of the variable. Note: class(): is a built-in R function which returns the mode of a variable. We will talk later what is function. # Type class(my_age) to check the mode of variable &quot;my_age&quot; class(my_age) ## [1] &quot;numeric&quot; To specifically define a variable as an integer, add letter L after the number. Define my_age=30L and check its mode. my_age=30L class(my_age) ## [1] &quot;integer&quot; You may be wondering why bother to differentiate integer and numeric; after all, they are all numbers. However, there is an important thing to remember: computer uses combination of 0/1 to save any values. While integer can always be exactly represented in 0/1 byte, non-integer may not be and thus lead to rounding error. Lets look at one example. It should be a no-brainer that sqrt(3)^2 should equals to 3. However, this is not the case in R (or any other programming languages). Try sqrt(3)^2==3 to compare these two number and see the result: # == is the logical operator to compare whether LHS equals RHS sqrt(3)^2==3 ## [1] FALSE Surprisingly, the output is FALSE, i.e., the computer makes a verdict that sqrt(3)^2 does not equal 3. This is because computer first computes sqrt(3), which is a non-integer with infinite decimal. To save this number in memory, sqrt(3) will be rounded. As a result, its square is not exactly 3. But in most cases, we do not need to overly concerned about rounding errors because the difference is negligible. 2.3.2 Logical In the above example, we introduced the logical operator == to compare numbers. The logical operator returns logical variable which takes value of only TRUE or FALSE. The typical Logical Operators in R: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to !x Not x x | y x OR y x &amp; y x AND y isTRUE(x) test if X is TRUE Now, lets check the mode of the result of the comparison sqrt(3)^2==3 using the class() function. class(sqrt(3)^2==3) ## [1] &quot;logical&quot; Similarly, you define a logical variable by assigning the comparison results into a logical variable # type 3&gt;2 and assign the value to a variable compare # check the variable class of the variable compare compare&lt;-3&gt;2 compare ## [1] TRUE class(compare) ## [1] &quot;logical&quot; We can use &amp; (and) and | (or ) for logical calcuation. compare1&lt;-3==2 # 3 equals to 2 is FALSE compare2&lt;-3!=2 # 3 not equal to 2 is TRUE compare1 &amp; compare2 # x &amp; y: true only if both x and y are true; otherwise false. ## [1] FALSE compare1 | compare2 # x | y: false only if both x and y are false; otherwise true. ## [1] TRUE Now, compare the mother_age and father_age to see who is older? father_age&gt;mother_age ## [1] TRUE 2.3.3 Character Suppose we want to save my name into a variable, how can I do that? This involves a new type of variable: character (also known as string). The character variable is defined by \"; anything inside\" will be saved as the character. # Type my_name=&quot;Leon Xu&quot; and use class() to check its mode. my_name&lt;-&quot;Leon Xu&quot; my_name ## [1] &quot;Leon Xu&quot; class(my_name) ## [1] &quot;character&quot; Now, you need to define character variable first_name with value Leon; and define character variable last_name with value Xu;. first_name=&quot;Leon&quot; last_name=&quot;Xu&quot; Given, the first and last name, we can combine the two strings into the full name. Note that the Arithmetic operators (+-*/) are not defined over strings. We need to use string-specific functions. You can use paste() function to concatenate the two string together. full_name=paste(first_name, last_name, sep=&quot; &quot;) # sep=&quot; &quot; means to seperates the strings with a space. full_name ## [1] &quot;Leon Xu&quot; type ?paste to get the help document on this function. ?paste Again, string is defined using \"\". To better understand that, run the following code to see the difference: print(10+1) print(&quot;10+1&quot;) print(leon xu) print(10+1): 10+1 is not inside \"\", so computer will read and interpret 10+1 and return 11 as the result. print(10+1): 10+1 is insdie \"\", so computer knows this is a strings and will not interpret it and keep it intact. print(leon xu): leon xu is not inside \"\"; so computer will read it and try to interpret it. Because computer (which is good at number and computation) cannot understand the string, thus returns error. It is one of the most common mistake to forget \"\" when you actually means to define a character. 2.3.4 Type coercion Sometime we need to convert from one variable mode to another. This typically happens when we read data from computer or web (e.g., csv) into R because computer is not smart enough to guess the variable type correctly. The good news is that we can easily convert from one variable mode to another. e.g., define the following two variables: var1=&quot;3&quot; # this is a character var2=4 # this is a numeric class(var1) ## [1] &quot;character&quot; class(var2) ## [1] &quot;numeric&quot; # Type var1+var2 and run the code; this will cause an error because var1 is a character var1+var2 The code below shows how to convert a character to numeric. var1&lt;-as.numeric(var1) var1+var2 ## [1] 7 The type coercion functions: # convert character &quot;4&quot; to 4; as.numeric(&quot;4&quot;) ## [1] 4 # convert 4 to character &quot;4&quot; as.character(4) ## [1] &quot;4&quot; # convert logical to numeric as.numeric(FALSE) # FALSE is stored as 0 in the computer ## [1] 0 as.numeric(TRUE) # TRUE is stored as 1 in the computer ## [1] 1 # we can sum TRUE/FALSE since they are saved as 1/0 TRUE+TRUE+FALSE ## [1] 2 We cannot force type coercion when the it is clearly not possible. For example, we cannot change the character John to a number. In this case, the R will generate NA (missing value in R) var&lt;-as.numeric(&quot;John&quot;) # var will be assigned NA (not available) ## Warning: NAs introduced by coercion # is.na() is R function to check whether a variable is NA is.na(var) ## [1] TRUE # NA is contagious in R; operations over NA results in NA var+1 ## [1] NA In R programming, everything stored in your computer are Objects. The variable we defined are also objects. Look for your objects in the upper right of the RStudio area # type ls() to see the objects stored in the computer ls() ## [1] &quot;avg_age&quot; &quot;Boston&quot; &quot;cols&quot; &quot;compare&quot; &quot;compare1&quot; ## [6] &quot;compare2&quot; &quot;computer&quot; &quot;covid_world&quot; &quot;error&quot; &quot;father_age&quot; ## [11] &quot;first_name&quot; &quot;fit&quot; &quot;flights&quot; &quot;full&quot; &quot;full_name&quot; ## [16] &quot;g&quot; &quot;gapminder&quot; &quot;h&quot; &quot;jobData&quot; &quot;keyword&quot; ## [21] &quot;last_name&quot; &quot;mae&quot; &quot;mape&quot; &quot;model1&quot; &quot;mother_age&quot; ## [26] &quot;my_age&quot; &quot;my_name&quot; &quot;p&quot; &quot;position&quot; &quot;pred&quot; ## [31] &quot;rmse&quot; &quot;test&quot; &quot;text_dat&quot; &quot;train&quot; &quot;train_size&quot; ## [36] &quot;var&quot; &quot;var1&quot; &quot;var2&quot; &quot;world_map&quot; &quot;you&quot; # you can remove objects through rm(). Remove the variable firstname rm(var) 2.4 Exercise 1: Greeting from R In this exercise, we will create a customized greeting from R to you. We will write a code to let the computer ask your name and age and print a customized greeting. This seems first overwhelming. In computer programming, one philosophy is always to break a complicated task into small piece and build upon that. Lets do the version 1: # Send a customized greeting based on your name (e.g., Leon) and age (e.g., 30) # Suppose it takes 1 year to master R programming print(&quot;Hello Leon, welcome to the world of R!&quot;) ## [1] &quot;Hello Leon, welcome to the world of R!&quot; print(&quot;You will be empowered by R to do awesome data analytics by the age 31!&quot;) ## [1] &quot;You will be empowered by R to do awesome data analytics by the age 31!&quot; Well, this is simple, but we need to customize so that it print your name and age information. Lets do the version 2: # Define your name and age (change to your name and age) name=&quot;Leon&quot; age=30 # Send a customized greeting based on your name and age # We need to paste the name and age to the greeting greeting1&lt;-paste(&quot;Hello &quot;, name, &quot;, welcome to the world of R!&quot;, sep = &quot;&quot;) greeting2&lt;-paste(&quot;You will be empowered by R to do awesome data analytics by the age &quot;, age+1, &quot;!&quot;, sep=&quot;&quot;) print(greeting1) ## [1] &quot;Hello Leon, welcome to the world of R!&quot; print(greeting2) ## [1] &quot;You will be empowered by R to do awesome data analytics by the age 31!&quot; Lets do the version 3: we will make it more interactive by using a function readline(), which asks for input from user through keyboard. # Use the console to input your name and age: name=readline(&quot;What is your name: &quot;) age=readline(&quot;How old are you: &quot;) # readline() always return a character age&lt;-as.numeric(age) # Send a customized greeting based on your name and age # We need to paste the name and age to the greeting greeting1&lt;-paste(&quot;Hello &quot;, name, &quot;, welcome to the world of R!&quot;, sep = &quot;&quot;) greeting2&lt;-paste(&quot;You will be empowered by R to do awesome data analytics by the age &quot;, age+1, &quot;!&quot;, sep=&quot;&quot;) print(greeting1) print(greeting2) 2.5 Exercise 2: Mortgage Calculation Support you are working at a bank provides mortgage loan. One important task is to calculate the monthly mortgage payment for any given loan. You can use the following equation to calculate the monthly mortgage payment (not including taxes and insurance): \\[M = P ( i(1 + i)^n ) / ( (1 + i)^n - 1)\\] where P = principal loan amount i = annual_interest_rate/12, i.e., i is the monthly interest rate, which is the annual interest rate divided by 12 n = number of months required to repay the loan Develop a program to calculate the monthly mortgage payment for a loan with: P=350000; annual_interest_rate=3.25%; year_repay=30. Once you calculate the monthly payment, print the result on the screen \"Your month mortgage payment is: ***\" # Define the loan P=350000 # loan amount annual_rate=0.0325 # Convert % into decimal to avoid error year_repay=30 # number of year to repay n=12*30 # number of month to repay i=annual_rate/12 payment = P*( i*(1 + i)^n ) / ( (1 + i)^n - 1) payment = round(payment,digits=0) print(paste(&quot;Your monthy mortage payment is $&quot;, payment, sep=&quot;&quot;)) ## [1] &quot;Your monthy mortage payment is $1523&quot; 2.6 Summary Learn to do basic arithmetic operations (+,-,*,/,^,%%) in R. Learn to define variable and the rule in naming variables. Understand the different variable mode (or type): integer, numeric, logical, character Understand how convert from one tpye to another. Learn to create the first interactive program using print() and readline() functions. "],["data-structure-vector-matrix-list-and-factor.html", "Chapter 3 Data Structure: Vector, Matrix, List and Factor 3.1 Vector 3.2 Matrices 3.3 Factors 3.4 List 3.5 Summary", " Chapter 3 Data Structure: Vector, Matrix, List and Factor It is ineffecicent to store each data point as a single-value variable. Think of dataset with 1000 rows and 20 columns, it will requires 20,000 single-value variables to store the data. In R, there are more efficient ways to store an array of data. Here we will introduce a few common data structure in R, including vector, matrix, data frame, and list. In addition, we all also talk about the factor: the data mode to represent categorical variable in R. 3.1 Vector Vector is a one-dimensional array that can hold numeric data, character data, or logical data. # Assign x the value c(1, 2, 3, 4, 10) (Here, &quot;c&quot; means vector) x=c(1,2,3,4,10) Notice that, after running the code, it seems nothing happen on the output screen. This is because this code only tells the computer to create a vector x with value of c(1, 2, 3, 4, 10). You can type x to print it on the screen. One simple trick to print x is to put the code inside () as follow: (x=c(1,2,3,4,10)) ## [1] 1 2 3 4 10 You can also create vector using a R-function seq(): y=seq(from=1, length=5, by=2), which creates a vector with length of 5, starting from 1 with step of 2. seq() is a very useful function to generate sequence, specially when we construct for loop. (y=seq(from=1,length=5,by=2)) ## [1] 1 3 5 7 9 # check the help document of the function by typing ?seq. Now, suppose you are at SpaceX launching center, you want to count down from 10 to 1. Use seq() to generate a sequence from 10 to 1. seq(from=10,length=10,by=-1) ## [1] 10 9 8 7 6 5 4 3 2 1 seq(from=10,to=1,by=-1) ## [1] 10 9 8 7 6 5 4 3 2 1 There is another quick way to generate a sequence of integer with interval of 1 using colon: # type 1:10 to define a vector from 1 to 10 with interval of 1. 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 # type 10:1 to define a vector from 10 to 1 with interval of -1. 10:1 ## [1] 10 9 8 7 6 5 4 3 2 1 So far, the element of a vector is numerical. The element of a vector can also be character or logical. # define a character vector b=c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;) b=c(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;) b ## [1] &quot;one&quot; &quot;two&quot; &quot;three&quot; class(b) ## [1] &quot;character&quot; # define a logical vector c=c(TRUE,FALSE,FALSE) c=c(TRUE,FALSE,FALSE) c ## [1] TRUE FALSE FALSE class(c) ## [1] &quot;logical&quot; The element of a vector must be the same type. Otherwise, R will force type coercion: convert all elements into character. Of course, we should aviod such forced type coercion by making sure all elements in the vector are the same type. For example, type d=c(4,3) and e=c(a, TRUE) and then print these two vectors, which are forced to be converted to character. d=c(4,&quot;3&quot;) e=c(&quot;a&quot;, TRUE) d ## [1] &quot;4&quot; &quot;3&quot; e ## [1] &quot;a&quot; &quot;TRUE&quot; 3.1.1 Subsetting vectors The vector is like an one-dimensional container for data. Once we create the vector, we can retrieve the elements from the vector according to the index. # define x=c(1,2,3,4,10) x=c(1,2,3,4,10) # retrieve the 2rd element of x by typing x[2]. x[2] ## [1] 2 It is important to notice that [] is the operator for subsetting the vector. It is a common mistake to type x(2), which means calling the function named x and passing argument 2 to the function. You can also retrieve multiple element for a vector at the same time. # retrieve the 1st, 3rd and 5th element of x x[c(1,3,5)] ## [1] 1 3 10 # retrieve the 2rd to 5th elements of x using colon operator x[2:5] ## [1] 2 3 4 10 The negative index means to retrieve every element from a vector except the negative index one, e.g., # retrieve every element of x except the 2rd element x[-1] ## [1] 2 3 4 10 We can also retrieve elements from vector by logical operator. E.g., # retrieve elements of x that is less than 5 x[x&lt;5] ## [1] 1 2 3 4 We cannot retrieve element from a vector which is beyond its length. E.g., if you type x[6], that will return NA (i.e., missing value) because the x only have 5 elements. x[6] Thus, it is important to know the length of a vector to aviod such error. # Find the length of &quot;x&quot; using &quot;length(x)&quot; length(x) ## [1] 5 3.1.2 Vector calculus We can perform operation on vector just as we perform operation on their individual elements. E.g., when the element of vector is numeric, you can perform operation (+, -, *, /) on each element of a vector. n=4 x=c(1,2,3,4,10) # Multiply x * n x*n ## [1] 4 8 12 16 40 # divide x/n x/n ## [1] 0.25 0.50 0.75 1.00 2.50 # addition x+n x+n ## [1] 5 6 7 8 14 # minus x-n x-n ## [1] -3 -2 -1 0 6 # power x^n x^n ## [1] 1 16 81 256 10000 We can also perform operation (+,-,*,/) with two vectors x=c(1,2,3,4,10) y=1:5 # multiply x * y : the corresponding elements will multiply each other. x*y ## [1] 1 4 9 16 50 # divide x/y: the corresponding elements will divide each other. x/y ## [1] 1 1 1 1 2 # addition x+y: the corresponding elements will add each other. x+y ## [1] 2 4 6 8 15 # minus x-y: the corresponding elements will minus each other. x-y ## [1] 0 0 0 0 5 # x^y: the x to the power of the corresponding element in y x^y ## [1] 1 4 27 256 100000 We can also use built-in R function to get summry about a vector. # Find the mean, median, min, and max of the numebers in &quot;x&quot; mean(x) ## [1] 4 median(x) ## [1] 3 min(x) ## [1] 1 max(x) ## [1] 10 summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 2 3 4 4 10 3.2 Matrices A matrix is a two-dimensional array where each element has the same type (numeric, character, or logical). Matrices are created with the matrix function. The general format is: myymatrix = matrix(vector, nrow=number_of_rows, ncol=number_of_columns, byrow=logical_value, dimnames=list( char_vector_rownames, char_vector_colnames)) # type ?matrix to get the help document on matrix() ?matrix Lets make a matrix m=matrix(1:6, nrow = 3, ncol = 2). Note that we are not passing value to the arguments such as byrow and dimnames. In such case, the R will take the default value for these argumements. # with matrices, it always fills column by column by default. m = matrix(1:6, nrow = 3, ncol = 2) # print matrix m m ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 5 ## [3,] 3 6 Now, find the dimension of m so that we can retrieve its element. # type dim(m) to get the rows and columns dim(m) ## [1] 3 2 # type nrow(m) and ncol(m) to get the rows and columns respectively nrow(m) ## [1] 3 ncol(m) ## [1] 2 We can fill the matrix by rows by setting byrow to be TRUE # create a matrix, filling by rows matrix(1:6,nrow=3, ncol=2, byrow=TRUE) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 We can also stack multiple vector to become a matrix. Depending on whether we want to stack the vector by column or by row, we can use cbind() and rbind() function. # create matrix through column bind cbind(): m2=cbind(c(1,3,4) c(2,34,5)) m2=cbind(c(1,3,4),c(2,34,5)) m2 ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 34 ## [3,] 4 5 # create matrix through row bind rbind: m3=rbind(c(1,3,4), c(2,34,5)) m3=rbind(c(1,3,4), c(2,34,5)) m3 ## [,1] [,2] [,3] ## [1,] 1 3 4 ## [2,] 2 34 5 3.2.1 subsetting matrix Matrix is a 2-dimensional container to store the data point. Once the matrix is defined, we can retrieve element from the matrix according to its index. # To retrieve the &quot;element&quot; in the second row and first column we # type &quot;m[2,1]&quot;. Again, the square brackets are used to retrieve part of a vector or matrix. # If we want the entire second column we type in &quot;m[,2]&quot;. This means &quot;all rows, second column 2&quot; # If we want the entire third row, we type in &quot;m[3,]&quot;. This means &quot;third row, all columns&quot; m[2,1] ## [1] 2 m[,2] ## [1] 4 5 6 m[3,] ## [1] 3 6 You can also retrieve elements from matrix using colon operator, e.g., # retrieve elements at row 1-2, and 2th column m[1:2,2] ## [1] 4 5 Similar to vector, elements of a matrix have to be the same type. otherwise, R will force type coercion. E.g., # define a matrix with numerics m4=matrix(1:8,ncol=2) m4 ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 # define a matrix with character. char=matrix(c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;), nrow=4) char ## [,1] ## [1,] &quot;a&quot; ## [2,] &quot;b&quot; ## [3,] &quot;c&quot; ## [4,] &quot;d&quot; Now combine the two matrixes m4 and char through cbind() m5=cbind(m4,char) m5 ## [,1] [,2] [,3] ## [1,] &quot;1&quot; &quot;5&quot; &quot;a&quot; ## [2,] &quot;2&quot; &quot;6&quot; &quot;b&quot; ## [3,] &quot;3&quot; &quot;7&quot; &quot;c&quot; ## [4,] &quot;4&quot; &quot;8&quot; &quot;d&quot; All elements of the matrixes are converted to character. 3.2.2 matrix calculus You can perform operation (+, -, *, /) on each element of a vector. # Multiply m * n n=10 m = matrix(1:6, nrow = 3, ncol = 2) m*n ## [,1] [,2] ## [1,] 10 40 ## [2,] 20 50 ## [3,] 30 60 # divide x/n m/n ## [,1] [,2] ## [1,] 0.1 0.4 ## [2,] 0.2 0.5 ## [3,] 0.3 0.6 # addition x+n m+n ## [,1] [,2] ## [1,] 11 14 ## [2,] 12 15 ## [3,] 13 16 # minus x-n m-n ## [,1] [,2] ## [1,] -9 -6 ## [2,] -8 -5 ## [3,] -7 -4 We can also use the built-in R functions to perform matrix calcuation. The colSums and rowSums are two useful function for matrix. # type colSums(m) to obtain the sum of each column of matrix m colSums(m) ## [1] 6 15 # type rowSums(m) to obtain the sum of each row of matrix m rowSums(m) ## [1] 5 7 9 apply() is a handy function for matrix calcuation. The general format of apply() function is: apply(matrix, margin, function) margin=1 indicates applying each row of the matrix to the function. margin=2 indicate applying each column of the matrix to the function. # E.g., calculating the mean of each row apply(m,1,mean) ## [1] 2.5 3.5 4.5 # E.g., calculating the sum of each column apply(m,2,sum) ## [1] 6 15 3.3 Factors Many data are categorical: e.g., the patients diabetes type (type 1 or type 2), the movie type (action, sci-fi, comedy), the customer satisfaction level (poor, good excellent) to a call service. # define a vector of diabetes type diabetes = c(&quot;Type1&quot;, &quot;Type2&quot;, &quot;Type1&quot;, &quot;Type1&quot;) class(diabetes) ## [1] &quot;character&quot; We need to convert the diabetes vector into factor to represent that this is a categorical variable. We can do that through the factor() function: diabetes = factor(diabetes) class(diabetes) ## [1] &quot;factor&quot; str(diabetes) ## Factor w/ 2 levels &quot;Type1&quot;,&quot;Type2&quot;: 1 2 1 1 Note that str() is a very handy function in R. It examines the structure of any objects in R. We use this function to examine the factor. As shown, diabetes has two levels: Type1 and Type2. They are coded as 1 and 2 respectively. Take an another example. Suppose we are buidling a database for movies. The movie_type is the vector which stores the type of each movie in the database. # define a vector of movie type movie_type=c(&quot;sci-fi&quot;,&quot;comedy&quot;,&quot;sci-fi&quot;,&quot;action&quot;,&quot;action&quot;) str(movie_type) ## chr [1:5] &quot;sci-fi&quot; &quot;comedy&quot; &quot;sci-fi&quot; &quot;action&quot; &quot;action&quot; movie_type = factor(movie_type) str(movie_type) ## Factor w/ 3 levels &quot;action&quot;,&quot;comedy&quot;,..: 3 2 3 1 1 Note that both diabetes and movie are unordered because it does not make sense to compare each category in both cases. There are also situations we need ordered factor, i.e., age group, education level, etc. In marketing, we typically run customer satisfaction survey on a 1-5 scale, where 1 means poor, 2 fair, 3 good and 4 improved and 5 excellent. This is called ordered factor: they are categorical but you can compare different category. # Here is a vector of survey response from 6 different customers survey_response=c(1,3,5,2,3,5) # convert this vector into a vector of factor with the corresponding labels survey_response2=factor(survey_response,order=TRUE,levels=c(1,2,3,4,5),labels=c(&quot;poor&quot;,&quot;fair&quot;,&quot;good&quot;,&quot;improved&quot;, &quot;excellent&quot;)) survey_response2 ## [1] poor good excellent fair good excellent ## Levels: poor &lt; fair &lt; good &lt; improved &lt; excellent str(survey_response2) ## Ord.factor w/ 5 levels &quot;poor&quot;&lt;&quot;fair&quot;&lt;..: 1 3 5 2 3 5 In the above code, levels and labels describe the level and their corresponding labels for this categorical variable. Note you need to put levels in their ascending order. As seen, the original response is coded in 1-5. Without proper labels, it would be very difficult to understand what 1-5 means. Thus, the R Factor is very useful to deal with such coded categorical variables. Since this is a ordered factor, we can compare between different level survey_response2[1] # no longer a number, but a coded level. ## [1] poor ## Levels: poor &lt; fair &lt; good &lt; improved &lt; excellent survey_response2[2] ## [1] good ## Levels: poor &lt; fair &lt; good &lt; improved &lt; excellent # we can compare level since this is a ordered factor survey_response2[1]&gt;survey_response2[2] ## [1] FALSE As mentioned, everything in R (e.g., the variable we defined and the function) is an object. We can use str() to examine the structure of any objecxt. # use str() to examine data in R str(movie_type) ## Factor w/ 3 levels &quot;action&quot;,&quot;comedy&quot;,..: 3 2 3 1 1 str(survey_response2) ## Ord.factor w/ 5 levels &quot;poor&quot;&lt;&quot;fair&quot;&lt;..: 1 3 5 2 3 5 # use str() to examine function str(factor) ## function (x = character(), levels, labels = levels, exclude = NA, ordered = is.ordered(x), ## nmax = NA) str(str) ## function (object, ...) 3.4 List Lists are the most complex of the R data types. Basically, a list is an collection of all kinds of objects (components). The many complex function returns list as function output, thus it is very important to know this data structure. Create a list: list(obj1, obj2, ) obj1, obj2,  can be DIFFERENT data types. # type movielist=list(&quot;end game&quot;, 2019, 2783), # the 1st is movie title, the 2nd is year, the 3rd is box office movielist=list(&quot;end game&quot;, 2019, 2783) # print the list movielist ## [[1]] ## [1] &quot;end game&quot; ## ## [[2]] ## [1] 2019 ## ## [[3]] ## [1] 2783 # type str(movielist) to see its structure str(movielist) ## List of 3 ## $ : chr &quot;end game&quot; ## $ : num 2019 ## $ : num 2783 Name each objects in the list using names() function names(movielist)=c(&quot;title&quot;,&quot;year&quot;,&quot;boxoffice&quot;) movielist ## $title ## [1] &quot;end game&quot; ## ## $year ## [1] 2019 ## ## $boxoffice ## [1] 2783 It is a best practice to create list with variable name in the first place to enhance the readability of your code. movielist=list(title=&quot;end game&quot;, year=2019, boxoffice=2783) str(movielist) ## List of 3 ## $ title : chr &quot;end game&quot; ## $ year : num 2019 ## $ boxoffice: num 2783 We can also retrieve the element from list by index; however, we need to use DOUBLE square brackets for retrieve element in list!! # Type movielist[[2]] to retrieve the 2rd element movielist[[2]] ## [1] 2019 The objects in a list is named, we can also retrieve element from list by name. The $ is used to retrieve list element by name. movielist$title ## [1] &quot;end game&quot; Add an element to a list ia also very simple. For example, we want to add a duration of the movie, we can do type movielist$duration=3. movielist$duration=3 str(movielist) ## List of 4 ## $ title : chr &quot;end game&quot; ## $ year : num 2019 ## $ boxoffice: num 2783 ## $ duration : num 3 3.5 Summary The common data structure in R: vector, matrix, list learn to retrieve elements from vector or matrix with index learn to retrieve elements from list with [[]] or $. Learn to use factor to represent both ordered and unodered categorical variables. All elments in vector and matrix must be the same type; list can contain any different type of data. "],["data-frame.html", "Chapter 4 Data Frame 4.1 Define a data.frame manually 4.2 Subsetting data.frame 4.3 Import csv data as data.frame. 4.4 Subsetting data frame 4.5 Write a data frame to your computer 4.6 Summary", " Chapter 4 Data Frame A data frame is more general than a matrix in that different columns can contain different modes of data (numeric, character, factor, logical etc.). We can think of a data frame as a excel sheet where each row represents an observation, while each column represents a variable associated with the observation. Data frames are the basic data structure youll deal with in R. 4.1 Define a data.frame manually Our first exercise is to define a data frame manually to help you familize the functions related to data frame. Suppose we are building a database of diabetes patients. For simplicity, suppose we have only four patients, with the following patient information: patientID = c(1, 2, 3, 4) age = c(25, 34, 28, 52) diabetes = c(&quot;Type1&quot;, &quot;Type2&quot;, &quot;Type1&quot;, &quot;Type1&quot;) status = c(&quot;Poor&quot;, &quot;Median&quot;, &quot;Good&quot;, &quot;Poor&quot;) We want to combine these information into a data frame. The function to construct data fram is as below: mydata = data.frame(col1, col2, col3,) where col1, col2, col3,  are the column of the data frame. Names for each column can be provided with the names function. The following code makes this clear. # this create an empty data frame. # We sometime need to do that when we do not know the ultimate dimension of the data frame. patientdata = data.frame() patientdata = data.frame(patientID, age, diabetes, status) # display the data frame on the screen to visually check the dataset patientdata ## patientID age diabetes status ## 1 1 25 Type1 Poor ## 2 2 34 Type2 Median ## 3 3 28 Type1 Good ## 4 4 52 Type1 Poor We can use str() to examine the structure of the data frame. str() is a very useful function to examine an object in R. We will use this function often. str(patientdata) ## &#39;data.frame&#39;: 4 obs. of 4 variables: ## $ patientID: num 1 2 3 4 ## $ age : num 25 34 28 52 ## $ diabetes : chr &quot;Type1&quot; &quot;Type2&quot; &quot;Type1&quot; &quot;Type1&quot; ## $ status : chr &quot;Poor&quot; &quot;Median&quot; &quot;Good&quot; &quot;Poor&quot; Note that each column of a data frame must have have same data mode, but you can put columns of different modes together to form the data frame. Because data frames are close to what analysts typically think of as datasets, well use the terms columns and variables interchangeably when discussing data frames. 4.2 Subsetting data.frame There are several ways to identify the elements of a data frame. You can use the index notation you used before (for example, with matrices) or you can specify column names. Subset the first 2 column of the patientdata by index: patientdata[,1:2] ## patientID age ## 1 1 25 ## 2 2 34 ## 3 3 28 ## 4 4 52 Subset the first two columns of the data frame by variable names: patientdata[c(&quot;diabetes&quot;, &quot;status&quot;)] ## diabetes status ## 1 Type1 Poor ## 2 Type2 Median ## 3 Type1 Good ## 4 Type1 Poor Subset one specific column (e.g., age) by its name: patientdata$age ## [1] 25 34 28 52 Now, change diabetes and status into factors because they are categorical variables: patientdata$diabetes&lt;-factor(patientdata$diabetes) patientdata$status&lt;-factor(patientdata$status, ordered = TRUE, levels = c(&quot;Poor&quot;, &quot;Median&quot;,&quot;Good&quot;) ) patientdata ## patientID age diabetes status ## 1 1 25 Type1 Poor ## 2 2 34 Type2 Median ## 3 3 28 Type1 Good ## 4 4 52 Type1 Poor str(patientdata) ## &#39;data.frame&#39;: 4 obs. of 4 variables: ## $ patientID: num 1 2 3 4 ## $ age : num 25 34 28 52 ## $ diabetes : Factor w/ 2 levels &quot;Type1&quot;,&quot;Type2&quot;: 1 2 1 1 ## $ status : Ord.factor w/ 3 levels &quot;Poor&quot;&lt;&quot;Median&quot;&lt;..: 1 2 3 1 4.3 Import csv data as data.frame. In most application, we will import dataset (i.e., csv, txt, excel files) from local computer into R as data frame. Here we will learn how to read data into R as data frame. The famous Auto.csv is a very standard dataset for R programming. The Auto.csv contains the basic information of over 300 kinds of Auto. We will learn to import this dataset. I have uploaded the Auto.csv file in the R-studio cloud to aviod the trouble of finding the file in each individuals computer. Look at the files in the bottom right pannel. The file is in csv format, thus we will use read.csv() function. Auto=read.csv(&quot;data/Auto.csv&quot;) # examine the structure of Auto dataset str(Auto) ## &#39;data.frame&#39;: 392 obs. of 10 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ rating : int 3 1 3 2 1 2 1 2 1 1 ... ## $ origin : chr &quot;US&quot; &quot;US&quot; &quot;US&quot; &quot;US&quot; ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... Unlikely excel which shows data in a tabular format, R uses different ways for us to visually examine the data. We can use head() and tail() to examine first/last 6 rows of the data. # print the first 6 rows for visually examining the dataset head(Auto) ## mpg cylinders displacement horsepower weight acceleration year rating origin ## 1 18 8 307 130 3504 12.0 70 3 US ## 2 15 8 350 165 3693 11.5 70 1 US ## 3 18 8 318 150 3436 11.0 70 3 US ## 4 16 8 304 150 3433 12.0 70 2 US ## 5 17 8 302 140 3449 10.5 70 1 US ## 6 15 8 429 198 4341 10.0 70 2 US ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 tail(Auto) ## mpg cylinders displacement horsepower weight acceleration year rating ## 387 27 4 151 90 2950 17.3 82 2 ## 388 27 4 140 86 2790 15.6 82 3 ## 389 44 4 97 52 2130 24.6 82 3 ## 390 32 4 135 84 2295 11.6 82 3 ## 391 28 4 120 79 2625 18.6 82 2 ## 392 31 4 119 82 2720 19.4 82 4 ## origin name ## 387 US chevrolet camaro ## 388 US ford mustang gl ## 389 EU vw pickup ## 390 US dodge rampage ## 391 US ford ranger ## 392 US chevy s-10 We can use summary() function to quickly examine the distribution of each variable. summary(Auto) ## mpg cylinders displacement horsepower weight ## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 46.0 Min. :1613 ## 1st Qu.:17.00 1st Qu.:4.000 1st Qu.:105.0 1st Qu.: 75.0 1st Qu.:2225 ## Median :22.75 Median :4.000 Median :151.0 Median : 93.5 Median :2804 ## Mean :23.45 Mean :5.472 Mean :194.4 Mean :104.5 Mean :2978 ## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:275.8 3rd Qu.:126.0 3rd Qu.:3615 ## Max. :46.60 Max. :8.000 Max. :455.0 Max. :230.0 Max. :5140 ## acceleration year rating origin ## Min. : 8.00 Min. :70.00 Min. :1.00 Length:392 ## 1st Qu.:13.78 1st Qu.:73.00 1st Qu.:2.00 Class :character ## Median :15.50 Median :76.00 Median :2.00 Mode :character ## Mean :15.54 Mean :75.98 Mean :2.48 ## 3rd Qu.:17.02 3rd Qu.:79.00 3rd Qu.:3.00 ## Max. :24.80 Max. :82.00 Max. :4.00 ## name ## Length:392 ## Class :character ## Mode :character ## ## ## read.csv() is the function to read csv file into R as a data.frame. When using the function, you need to tell exactly where to find the data file. For example, if Auto.csv is saved in my computer at: C:/Users/lxu25/Dropbox/Teaching/UNL Courses/SCMA 450/code/Auto.csv. We can use the following code to read the file: Auto=read.csv(&quot;C:/Users/lxu25/Dropbox/Teaching/UNL Courses/SCMA 450/code/Auto.csv&quot;) Notice, in the the file directory is /, not \". However, locating this data file can be a hassle, especially when your code is shared with others in your team because the data has a totally different directory in otherss computer. Therefore, it is the best practice to always create a seperate folder for your project and save both code and data in that folder. Since your data is in the same folder of your code, we can simply use the filename to locate the data file. Then share the folder together to others. 4.4 Subsetting data frame Select by rows by index. # type Auto[2,] to select the 2nd row (or called 2nd observation) Auto[2,] ## mpg cylinders displacement horsepower weight acceleration year rating origin ## 2 15 8 350 165 3693 11.5 70 1 US ## name ## 2 buick skylark 320 # type Auto[c(3,5,7),] to select the 3rd, 5th, and 7th rows. Auto[c(3,5,7),] ## mpg cylinders displacement horsepower weight acceleration year rating origin ## 3 18 8 318 150 3436 11.0 70 3 US ## 5 17 8 302 140 3449 10.5 70 1 US ## 7 14 8 454 220 4354 9.0 70 1 US ## name ## 3 plymouth satellite ## 5 ford torino ## 7 chevrolet impala Select column by index. # type Auto[,4] to select the 4th column (i.e., horsepower); head(Auto[,4]) ## [1] 130 165 150 150 140 198 # tpye hist(Auto[,4]) to plot the distribution of horsepower hist(Auto[,4]) # You can customize the histgram to make it look nicer hist(Auto[,4],xlab=&quot;horsepower&quot;, col=&quot;gray&quot;,breaks = 50, main= &quot;frequency of horsepower&quot;) hist() plot the histogram of a variable. You can use ?hist to examine the detail of the function. ?hist # a quick way to look at how to use this function of We can select multiple columns (variables) at the same time. # select the 2nd and 4th column (i.e., cylinders and horsepower ) head(Auto[,c(2,4)]) ## cylinders horsepower ## 1 8 130 ## 2 8 165 ## 3 8 150 ## 4 8 150 ## 5 8 140 ## 6 8 198 We can create a scatter plot between these two variables to see their relationship. This also shows the reason why want to subset data frame. plot(Auto[,c(2,4)]) The scatter plot clearly demonstrates that a higher horsepower is assoicated with a larger number of cylinders. We can also subset the dataset by variable name. We need to use $ for subsetting one variable with its variable name. # Auto$mpg selects the variable mpg # type hist[Auto$mpg] to get its distribution hist(Auto$acceleration) # select multiple columns by name. head( Auto[,c(&quot;horsepower&quot;,&quot;mpg&quot;,&quot;weight&quot;)] ) ## horsepower mpg weight ## 1 130 18 3504 ## 2 165 15 3693 ## 3 150 18 3436 ## 4 150 16 3433 ## 5 140 17 3449 ## 6 198 15 4341 Filter rows by condition: # select all rows with mpg less equal to 12 Auto[Auto$mpg&lt;=12,] ## mpg cylinders displacement horsepower weight acceleration year rating ## 26 10 8 360 215 4615 14.0 70 2 ## 27 10 8 307 200 4376 15.0 70 2 ## 28 11 8 318 210 4382 13.5 70 2 ## 29 9 8 304 193 4732 18.5 70 2 ## 42 12 8 383 180 4955 11.5 71 1 ## 67 11 8 429 208 4633 11.0 72 2 ## 69 12 8 350 160 4456 13.5 72 1 ## 90 12 8 429 198 4952 11.5 73 2 ## 95 12 8 455 225 4951 11.0 73 1 ## 103 11 8 400 150 4997 14.0 73 1 ## 104 12 8 400 167 4906 12.5 73 2 ## 106 12 8 350 180 4499 12.5 73 2 ## 124 11 8 350 180 3664 11.0 73 2 ## origin name ## 26 US ford f250 ## 27 US chevy c20 ## 28 US dodge d200 ## 29 US hi 1200d ## 42 US dodge monaco (sw) ## 67 US mercury marquis ## 69 US oldsmobile delta 88 royale ## 90 US mercury marquis brougham ## 95 US buick electra 225 custom ## 103 US chevrolet impala ## 104 US ford country ## 106 US oldsmobile vista cruiser ## 124 US oldsmobile omega # select all rows with mpg less equal to 12 and greater equal to 11 Auto[Auto$mpg&gt;=11 &amp; Auto$mpg&lt;=12,] ## mpg cylinders displacement horsepower weight acceleration year rating ## 28 11 8 318 210 4382 13.5 70 2 ## 42 12 8 383 180 4955 11.5 71 1 ## 67 11 8 429 208 4633 11.0 72 2 ## 69 12 8 350 160 4456 13.5 72 1 ## 90 12 8 429 198 4952 11.5 73 2 ## 95 12 8 455 225 4951 11.0 73 1 ## 103 11 8 400 150 4997 14.0 73 1 ## 104 12 8 400 167 4906 12.5 73 2 ## 106 12 8 350 180 4499 12.5 73 2 ## 124 11 8 350 180 3664 11.0 73 2 ## origin name ## 28 US dodge d200 ## 42 US dodge monaco (sw) ## 67 US mercury marquis ## 69 US oldsmobile delta 88 royale ## 90 US mercury marquis brougham ## 95 US buick electra 225 custom ## 103 US chevrolet impala ## 104 US ford country ## 106 US oldsmobile vista cruiser ## 124 US oldsmobile omega We can perform operation on existing variable and create new variables and add these new variable into the data frame. E.g., The following code creates a new variable called horsepowerpercylinder, which is horsepower divided by # of cylinders, # and adds to the data frame. Auto$horsepowerpercylinder=Auto$horsepower/Auto$cylinders str(Auto) ## &#39;data.frame&#39;: 392 obs. of 11 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement : num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ rating : int 3 1 3 2 1 2 1 2 1 1 ... ## $ origin : chr &quot;US&quot; &quot;US&quot; &quot;US&quot; &quot;US&quot; ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... ## $ horsepowerpercylinder: num 16.2 20.6 18.8 18.8 17.5 ... We can also rename variables using the names() function. # eamine the existing name of the Auto datafile. names(Auto) ## [1] &quot;mpg&quot; &quot;cylinders&quot; &quot;displacement&quot; ## [4] &quot;horsepower&quot; &quot;weight&quot; &quot;acceleration&quot; ## [7] &quot;year&quot; &quot;rating&quot; &quot;origin&quot; ## [10] &quot;name&quot; &quot;horsepowerpercylinder&quot; # change the 11th variable name to power_per_cylinder names(Auto)[11]=&quot;power_per_cylinder&quot; str(Auto) ## &#39;data.frame&#39;: 392 obs. of 11 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement : num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ rating : int 3 1 3 2 1 2 1 2 1 1 ... ## $ origin : chr &quot;US&quot; &quot;US&quot; &quot;US&quot; &quot;US&quot; ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... ## $ power_per_cylinder: num 16.2 20.6 18.8 18.8 17.5 ... To delete a variable from data frames: # Delete the variable power_per_cylinder Auto=Auto[,-c(11)] str(Auto) ## &#39;data.frame&#39;: 392 obs. of 10 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ rating : int 3 1 3 2 1 2 1 2 1 1 ... ## $ origin : chr &quot;US&quot; &quot;US&quot; &quot;US&quot; &quot;US&quot; ... ## $ name : chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... To access the mpg in Auto data frame, we must use $ (e.g., Auto$mpg). If you simply use mpg, R would not know which data frame to look for mpg. But this causes the code too long and hard to read. We can use with() function to avoid that. E.g., plot the scatter plot between weight and mpg with(Auto, plot(weight, mpg) ) The general format of with() function is to: with(DATA FRAME, CODE USING THE DATA FRAME) 4.5 Write a data frame to your computer We can use write.csv() to write the data frame to your local computer. write.csv(Auto,&quot;data/Auto2.csv&quot;, row.names =FALSE) Note, row.names = TRUE will generate an automatic ID (i.e., 1,2,3, ) for each rows. 4.6 Summary While all elements in vector and matrix must be the same type, data.frame allows different columns (i.e., variable) to be different types. use Base R for creating plots. subsetting data.frame using index and variable names. Use read.csv() to read csv file into R as data.frame; learn to subset data.frame. Use write.csv() to write data.frame from R to your local computer. "],["functions.html", "Chapter 5 Functions 5.1 Get the Help Document of A Function 5.2 Call build-in functions in R 5.3 Write Your Own Functions 5.4 Flexible Function 5.5 Excercise 1 5.6 The apply() function family 5.7 Scoping Rule in R 5.8 Excercise", " Chapter 5 Functions Functions are the building blocks in R. There are two ways to pass arguments to a function: by argument names and by position. You will learn to call function through these two ways. In addition, you also will Learn to examine the help document of each function by using ?function_name. Understand the default value of arguments. Open the black box of functions by learning to write your own functions. Understand the scoping rule of R. 5.1 Get the Help Document of A Function You cannot memorize all functions and their usage. We rely on the help documents of the function. Take seq() function as an example. This function can generate a sequence of number. We can type ?seq to get the help document of the function ?seq 5.2 Call build-in functions in R The help document of seq() shows the augments of the function is: seq(from = 1, to = 1, by , length , ) Here from, to, by, length are the argument of the functions. Once we pass value to these arguments, the function will then process these arguments according to its code inside the function. E.g., we want to generate a sequence of even number from 2 to 12. by argument name. seq(from=2,to=12, by=2) ## [1] 2 4 6 8 10 12 by position seq(2,12, 2) ## [1] 2 4 6 8 10 12 5.3 Write Your Own Functions The general format for defining function is as follow: function_name=function(argument1,arugment2, ...){ # Code to Process the arguments return(results) } E.g., our first task is to write a function to calculate the sum square of two numbers. myfun1=function(x1,x2){ return(x1^2+x2^2) } By default, the result from the last line of the function will be the returned result. So we can change the last line as: \\(x_1^2+x_2^2\\). Once we define the function, it is ready to be used just as other functions built-in R. call function by argument name myfun1(x2=2,x1=1) ## [1] 5 call function by position myfun1(1,2) ## [1] 5 5.4 Flexible Function You may notice that some functions have an argument of . This is called flexible function. Lets take a look at an example: We will define a plot function to make a scatter plot between two variable, and add a linear fitted linear to the plot. abplot=function(x,y,...){ plot(x,y,... ) # create a scatter plot abline(lm(y~x)) # add a fitted line } Essentially, anything you passed in the position of  will be passed to the plot() function. We can now call abplot() function with additional arguments as below: Auto=read.csv(&quot;data/Auto.csv&quot;) abplot(Auto$weight, Auto$mpg, type=&quot;p&quot;, xlab=&quot;weight&quot;, ylab=&quot;mpg&quot;,col=&quot;red&quot;) 5.5 Excercise 1 Write a function to calculate the descriptive statistics of an array of numbers (i.e., mean, standard deviation and skewness). Skewness is a measure of symmetry, or more precisely, the lack of symmetry. Below is the formula for computing skewness of a variable. \\[skewness =\\frac{\\sum_{i=1}^n(x_i  \\bar x)^3}{n*\\sigma^3}\\] A variable with skewness close to 0 tend to be symmetry; A variable with negative skewness tend to left skewed; A variable with positive skewness tend to right. mystat=function(x){ avg=mean(x) sd=sd(x) skewness=mean((x-avg)^3)/sd^3 return(c(avg, sd, skewness)) # return mean and std dev as a vector } # acceleration is not as skewed mystat(Auto$acceleration) ## [1] 15.5413265 2.7588641 0.2893592 # horsepower is right skewed mystat(Auto$horsepower) ## [1] 104.469388 38.491160 1.079019 5.6 The apply() function family The apply() function family is very useful for data analysis. The function looks like below: apply(X, MARGIN, FUN, ), where X is 2-dimensional data array, e.g., matrix or a data.frame; MARGIN is a variable defining how the function is applied: when MARGIN=1, it applies over rows, whereas with MARGIN=2, it works over columns FUN, which is the function that you want to apply to the data. apply(Auto[c(&quot;mpg&quot;,&quot;weight&quot;,&quot;horsepower&quot;)], 2, mystat) In addition, lapply() and sapply() are the two variants of apply(). lapply() and sapply() both work over column and return results in different format. # apply function on the column and return results as list lapply(Auto[c(&quot;mpg&quot;,&quot;weight&quot;,&quot;horsepower&quot;)], mystat) ## $mpg ## [1] 23.4459184 7.8050075 0.4536001 ## ## $weight ## [1] 2977.584184 849.402560 0.515616 ## ## $horsepower ## [1] 104.469388 38.491160 1.079019 # apply function on column and return a simplified results sapply(Auto[c(&quot;mpg&quot;,&quot;weight&quot;,&quot;horsepower&quot;)], mystat) ## mpg weight horsepower ## [1,] 23.4459184 2977.584184 104.469388 ## [2,] 7.8050075 849.402560 38.491160 ## [3,] 0.4536001 0.515616 1.079019 5.7 Scoping Rule in R Scoping Rule determines where R should search the value of free variable inside a function. Free variables inside a function is the variables that is neither defined in a function nor the functions argument. Lets look at an example. power=2 myfun2=function(x){ x^power } myfun2(5) ## [1] 25 In the above code, power is a free variable. A good practice is to avoid free variables. In other word, we should define variables needed by a function either through arguments or define locally within the function. For example, we can revise the above function as below: power=2 myfun2=function(x){ power=3 # define power inside the function x^power } # guess, what is the value of myfun2(2) myfun2(2) ## [1] 8 # guess, what is the value of power power ## [1] 2 The scoping rule in R determines that what is the value of power when we call myfun2(2). In R, the program will first search inside the function whether power is defined. If yes, then use the value defined inside the function. Otherwise, the program examine whether power is defined outside the function. Changing the value inside the function will not change the variable outside the function. 5.8 Excercise Support you are working at a bank provides mortgage loan. One important task is to calculate the monthly mortgage payment for any given loan. You can use the following equation to calculate the monthly mortgage payment: \\[M = P ( i(1 + i)^n ) / ( (1 + i)^n - 1)\\] where P = principal loan amount i = annual_interest_rate/12, i.e., i is the monthly interest rate, which is the annual interest rate divided by 12 n = number of months required to repay the loan In this exercise, your goal is define a function for calculating monthly mortgage payment. The function takes three arguments: P(principal), annual_rate, year_repay. The default year_repay is 30, which is the most common mortgage term. "],["flow-control.html", "Chapter 6 Flow Control 6.1 ifelse Statement 6.2 ifelse ifelse Statement 6.3 Exercise 6.4 for loop 6.5 Break Your Loops With break 6.6 Jump to the next cycly with next 6.7 Exercise 6.8 While Loop", " Chapter 6 Flow Control You have written over 500 lines of code. This is a great achievement. All the code we write so far runs from up to the bottom in a linear fashion. Today, we will learn the if, for, and while statements to control the flow of your code. 6.1 ifelse Statement To be or not to be, this is a problem? When solving real problems using computer codes, we will inevitably have to branch our code in different directions based on some criteria. The simplest format for IF-statement in R is: if (logical_expression) { # statement will only be executed if logical_expression is true statement } We can also have else statement as below: if (logical_expression) { # statement1 will be executed if logical_expression is true statement1 } else { # statement2 will be executed if logical_expression is false statement2 } In the above case, statement1 will be excuted if the logical_expression is true; otherwise, statement2 will be executed. Take one example, we will create a door with password. The door will open if you input the correct password, but the door will be closed if you input the incorrect password. pw=&quot;2497&quot; pw_entered=readline(&quot;Please enter the password: &quot;) if (pw_entered==pw){ print(&quot;The door is opened. Welcome!&quot;) } else { print(&quot;Password wrong. Door closed!&quot;) } In this case, the logical expression evaluates whether pw_entered==pw is true or not. If true, then the program proceeds to print(The door is opened. Welcome!); If not true, then the program proceeds to print(Password wrong. Door closed!). 6.2 ifelse ifelse Statement The ifelse statement is great to program the binary outcome (e.g., correct password or incorrect password). However, sometime we need to deal with more than two outcomes. In this case, we need to use the ifelse ifelse statement: if ( logical_expression1) { statement1 } else if ( logical_expression2) { statement2 } else { statement3 } logical_expression1 and logical_expression2 are a set of mutually exclusive criteria; only one statement will be executed depending on which logical_expression is true. You can have more else as needed. 6.3 Exercise In data analysis, we often want to bin a continuous variable into categorical of low, median, high or even more category. For example, we want to classify Auto as low, median, high gas efficiency based on mpg: if mpg&lt;22, the efficiency is low; if 26&gt;mpg&gt;=22, then the efficiency is median; if mpg&gt;=26, then the efficiency is high. mpg=23 if (mpg&lt;22) { efficiency=&quot;low&quot; } else if (mpg&lt;26) { efficiency=&quot;median&quot; } else{ efficiency=&quot;high&quot; } efficiency ## [1] &quot;median&quot; Note that we did not need to write mpg&gt;=22 &amp; mpg&lt;26 in the second criteria since the second criteria will only be evaluated when mpg&gt;=22. 6.4 for loop In computer programming, we sometime want to do things repetitively. For example, there are 10 data files, each corresponding to a specific year. Want to read all these 10 files into R; or we will visit 100 different websites to scrape the data from web. Loop allows the program to go back to previous code and do things repetitively. In R programming, a for-loop is used to iterate over a vector. The syntax of for loop is for (val in sequence){ statement } Here, sequence is a vector and val takes on each of its value during the loop. val is also called iterator because it will iterate every elements in sequence. In each iteration, statement is evaluated. Lets look at one simple example. For example, print 1 to 10 on the screen. This is a repetitive task, i.e., printing a number on screen for 10 times. We can use for loop to achieve this. for (i in 1:10){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 In this example, 1:10 generates a vector from 1 to 100. i will iterate every value from 1 to 10 and be printed on the screen. The sequence can also be a character vector. For example, the following code will print every elements of the character vector on the screen. students=c(&quot;John&quot;,&quot;Lily&quot;,&quot;Leon&quot;,&quot;Brandon&quot;) for (i in students){ print(i) } ## [1] &quot;John&quot; ## [1] &quot;Lily&quot; ## [1] &quot;Leon&quot; ## [1] &quot;Brandon&quot; In the above code, i will iterate over the students vector; and print(i) will be excuted for 4 times, with i taking the value of John, Lily, Leon, Brandon. For example, we want to calculate the sum square from 1 to 100. sum=0 # initize the sum as 0 # we will add the square of a number (1-100) to sum for (i in 1:100){ sum=sum+i^2 } sum ## [1] 338350 In this example, i will iterate every value from 1 to 100; and the statement will thus be excuted for 100 times, with i taking the value from 1 to 100. 6.5 Break Your Loops With break As seen in above examples, for-loop requires the program to iterate every element in the vector. However, you can break your loop with break. When the R encounters a break, it will exit the loop immediately. For example, in the printing students name example, we will exit the loop if the iterator takes the value Leon and print Leon is not student. students=c(&quot;John&quot;,&quot;Lily&quot;,&quot;Leon&quot;,&quot;Brandon&quot;) for (i in students){ if (i==&quot;Leon&quot;){ print(&quot;Leon is not student.&quot;) break } else{ print(i) } } ## [1] &quot;John&quot; ## [1] &quot;Lily&quot; ## [1] &quot;Leon is not student.&quot; 6.6 Jump to the next cycly with next The above code shows that we can use break to end the loop if the iterator meets certain criteria. However, in some cases, we do not want to end the loop completely, but to skip the particular iteration. We can use next to discontinue a particular iteration and jumps to the next cycle. students=c(&quot;John&quot;,&quot;Lily&quot;,&quot;Leon&quot;,&quot;Brandon&quot;) for (i in students){ if (i==&quot;Leon&quot;){ print(&quot;Leon is not student!&quot;) next } else{ print(i) } } ## [1] &quot;John&quot; ## [1] &quot;Lily&quot; ## [1] &quot;Leon is not student!&quot; ## [1] &quot;Brandon&quot; 6.7 Exercise Lets revisit the door password problem. In this exercise, we want to design the password in such way that you have three chances to input the passwords. If you enter the password correctly within the three times, the door will open; you have the opportunity to re-enter the password before you running out of the three chances; if you did not enter the correct password within the three trials, the door will be closed. This is a repetitive task because you are asked to enter the password for three times. We can thus use for-loop for this purpose. Verison 1: pw=&quot;2497&quot; for (i in 1:3){ pw_entered=readline(&quot;Please enter the password: &quot;) if (pw_entered==pw){ print(&quot;The door is opened. Welcome!&quot;) break } else { print(&quot;Password wrong. Please re-enter the password: &quot;) } } Verison 2: When i=3, the code should show that the door is locked due to too many failed trials, rather than asking for re-enter the password. pw=&quot;2497&quot; for (i in 1:3){ pw_entered=readline(&quot;Please enter the password: &quot;) if (pw_entered==pw){ print(&quot;The door is opened. Welcome!&quot;) break } else if(i!=3){ print(&quot;Password wrong. Please re-enter the password: &quot;) } else { print(&quot;Password wrong. Door closed.&quot;) } } 6.8 While Loop In R programming, while loops are used to loop until a specific condition is met. The while loop is also used for repetitive task. The syntax for while-loop is as below: while (test_expression) { statement } Here, test_expression is evaluated and the body of the loop is entered if the result is TRUE. Lets look at how to calculate the sum square from 1 to 100 using while loop. sum=0 # initize the sum as 0 i=1 # initize the iterator # we will add the square of a number (1-100) to sum while (i&lt;=100){ sum=sum+i^2 i=i+1 } sum ## [1] 338350 Lets revist the password door exercise. You will have 3 chances to enter the correct password to open the door. This time, implement this with while-loop. pw=&quot;2497&quot; i=1 # initialize the iterator while (i &lt;= 3){ pw_entered=readline(&quot;Please enter the password: &quot;) if (pw_entered==pw){ print(&quot;The door is opened. Welcome!&quot;) break } else if(i!=3){ print(&quot;Password wrong. Please re-enter the password: &quot;) } else { print(&quot;Password wrong. Door closed.&quot;) } i=i+1 # increase the iterator by 1 at the end } As seen, it is much easily to use for-loop for iterating over a fixed sequence because we do not need to manually update the iterator. Typically, we use while-loop if we do not how many iteration will be conducted; and use for-loop if we know how many iteration to be conducted. Also, break/next work similarly with while-loop to exit the while-loop or to jump to the next iteration within the while-loop, respectively. "],["r-markdown-for-documentation.html", "Chapter 7 R markdown for documentation 7.1 rock, paper, scissors 7.2 verison 1: 7.3 verison 2 7.4 Version 3 7.5 verision 4 7.6 version 5", " Chapter 7 R markdown for documentation We have been writing code in Rscript and all the result of your code is printed at the console. From now on, we will learn to use a powerful interactive notebook interface to write code and display results. In addition, you can also use multiple languages including R, Python, and SQL in R-markdown. We will later introduce how to insert python code in r-markdown. Next, we will write code in r-markdown to implement some small applications. In particular, we have learned the statement of ifelse if for branching; the for/while-loop for repetitive task. Today, we will use these statements to write the first computer game in R. 7.1 rock, paper, scissors Lets write a program to play the rock, paper, scissors with the computer. At each round, the computer and you will choose one from rock, paper, scissors. Then you compare the computers and your choice to determine 1) you won 2) computer won 3) tie. Since there are three different outcomes, we need ifelse ifelse statement. 7.2 verison 1: you=&quot;rock&quot; computer=&quot;paper&quot; if (computer==you){ print(&quot;this is a tie.&quot;) } else if((computer==&quot;rock&quot; &amp; you==&quot;paper&quot;)|(computer==&quot;paper&quot; &amp; you==&quot;scissors&quot;)|(computer==&quot;scissors&quot; &amp; you==&quot;rock&quot;) ){ print(&quot;You won!&quot;) } else{ print(&quot;Oops, computer won!&quot;) } ## [1] &quot;Oops, computer won!&quot; While version 1 is a boring game because you have already know the outcome of the game. In order not to have the pre-determined outcome, we need to let you and computer randomly choose from rock, paper, scissors. There is a convenient function in R for random sampling: sample(vector, n, replace=TRUE) samples n elements from vector with replacement. With replacement means that the sampled element is put back for the next sampling. E.g., sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 4, replace=TRUE) ## [1] &quot;scissors&quot; &quot;paper&quot; &quot;rock&quot; &quot;scissors&quot; 7.3 verison 2 With the sample() function, we can improve the program as below: #you=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) you=readline(&quot;please enter your action: rock, paper, scissors &gt; &quot;) computer=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) print(paste(&quot;your action is&quot;, you)) print(paste(&quot;computer action is&quot;, computer)) if (computer==you){ print(&quot;this is a tie.&quot;) } else if((computer==&quot;rock&quot; &amp; you==&quot;paper&quot;)|(computer==&quot;paper&quot; &amp; you==&quot;scissors&quot;)|(computer==&quot;scissors&quot; &amp; you==&quot;rock&quot;) ){ print(&quot;You won!&quot;) } else{ print(&quot;Oops, computer won!&quot;) } 7.4 Version 3 Lets revisit the rock, paper, scissors game. This time, you and computer will play rock, paper, scissors for three rounds. Whoever won more rounds will be the winner. Playing 3 rounds is a repetitive task, we can use for-loop. Since the game will be playing multiple rounds, we will need to implement a score board to keep track of the score at each round (think of the score board in a basketball game). you_score=0 computer_score=0 for (i in 1:3){ #you=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) you=readline(&quot;please enter your action: rock, paper, scissors &gt; &quot;) computer=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) print(paste(&quot;your action is&quot;, you)) print(paste(&quot;computer action is&quot;, computer)) if (computer==you){ print(&quot;this is a tied round.&quot;) } else if((computer==&quot;rock&quot; &amp; you==&quot;paper&quot;)|(computer==&quot;paper&quot; &amp; you==&quot;scissors&quot;)|(computer==&quot;scissors&quot; &amp; you==&quot;rock&quot;) ){ print(&quot;You won in this round!&quot;) you_score=you_score+1 } else{ print(&quot;Oops, computer won in this round!&quot;) computer_score=computer_score+1 } } # determine who won finally print(&quot;The final score is: &quot;) print(paste(&quot;you: &quot;, you_score, &quot; V.S. &quot;, &quot;computer: &quot;, computer_score, sep=&quot;&quot;)) if (you_score&gt;computer_score){ print(&quot;You won!&quot;) } else if (you_score==computer_score){ print(&quot;tie game&quot;) } else { print(&quot;Computer won!&quot;) } 7.5 verision 4 Define a rps function for re-use: # determine the winner based on action rps=function(you, computer){ computer_score=0 you_score=0 print(paste(&quot;your action is&quot;, you)) print(paste(&quot;computer action is&quot;, computer)) if (computer==you){ print(&quot;this is a tied round.&quot;) } else if((computer==&quot;rock&quot; &amp; you==&quot;paper&quot;)|(computer==&quot;paper&quot; &amp; you==&quot;scissors&quot;)|(computer==&quot;scissors&quot; &amp; you==&quot;rock&quot;) ){ print(&quot;You won in this round!&quot;) you_score=you_score+1 } else{ print(&quot;Oops, computer won in this round!&quot;) computer_score=computer_score+1 } return(c(computer_score,you_score)) } # the function to print the score winner=function(computer_score,you_score){ print(&quot;The final score is: &quot;) print(paste(&quot;you: &quot;, you_score, &quot; V.S. &quot;, &quot;computer: &quot;, computer_score, sep=&quot;&quot;)) if (you_score&gt;computer_score){ print(&quot;You won!&quot;) } else if (you_score==computer_score){ print(&quot;tie game&quot;) } else { print(&quot;Computer won!&quot;) } } #you=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) you=readline(&quot;please enter your action: rock, paper, scissors &gt; &quot;) computer=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) score=rps(you,computer) winner(score[1],score[2]) 7.6 version 5 Lets revisit the rock, paper, scissors game. After each round, the program will ask you whether you want to continue to play another round. The program ends if you enter no; otherwise, the program will continue to the next round. This is also an iterative task, but we do not know in advance how many iteration will be. Thus, we can use while-loop. continue=&quot;yes&quot; # the logical expression in the while loop total_score=c(0,0) while (continue!=&quot;no&quot;){ you=readline(&quot;please enter your action: rock, paper, scissors &gt; &quot;) computer=sample(c(&quot;rock&quot;,&quot;paper&quot;,&quot;scissors&quot;), 1, replace=TRUE) score=rps(you,computer) total_score=total_score+score continue=readline(&quot;Do you want to play another round (yes/no): &quot;) } winner(total_score[1],total_score[2]) Congratulation! You have just created your first computer game in R! "],["use-data.table-package-for-big-data-i.html", "Chapter 8 Use data.table Package for Big Data I 8.1 Install R Packages 8.2 fread(): Read csv file into R as data.table 8.3 Quick summary of the data 8.4 Data Manipulation with data.table 8.5 Select Columns 8.6 Calcuation by Group 8.7 Summary", " Chapter 8 Use data.table Package for Big Data I Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. There are currently 120,000 packages in R, which empower you to do all kinds of data analysis. This huge variety of packages is one of the reasons that R is so successful: the chances are that someone has already solved a problem that youre working on, and you can benefit from their work by downloading their package. The data.table package in R provides an enhanced version of data.frame that allows you to do blazing fast data manipulations. The data.table package is being used in different fields such as finance and genomics which encounter large data sets (for example, 1GB to 100GB). Why we are interested in learning data.table package among all these packages? The data.table package in a sense helped to save R. At one point a few years ago, there were major concerns that R could not handle Big Data, with calls from some in favor of Python instead. The data.table package showed that R could do quite well in large datasets. This really aided in Rs being adopted by various large companies  Norm Matloff, Prof. of Computer Science, UC Davis 8.1 Install R Packages There are over 120,000 packages in R. As you can imagine, it is not wise to install every package in R, which will take enormous computer storage space. In fact, when installing R, we will install the most essential packages, which are known as base R. For example, the functions learned so far are all from base R (e.g., data.frame, plot, loop, if ). Base R can perform the basic functions. But we can enhance the power of R by installing packages. We can install R packages as needed through the following syntax: install.packages(&quot;package_name&quot;) # download and install the package library(package_name) # load the package into memory The packages are typically saved at the Comprehensive R Archive Network (CRAN); we do not need to worry about where the package is saved exactly on the Internet; R will take care of that automatically. Lets install the data.table package by running following code: install.packages(&quot;data.table&quot;) As you will see, downloading and installing the package takes times, especially when the package is big. Thus, I will run this code just once to install the package. Every time when you reopen your R project, you just need to load the package into your memory through the following code: library(&quot;data.table&quot;) ## Warning: package &#39;data.table&#39; was built under R version 4.0.5 ## ## Attaching package: &#39;data.table&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## .N Now that you have loaded the package into the computer memory, you can use all the powerful functions from the package. On the right bottom corner of R-studio, you should be able to examine all the packages that are installed. The packages that are checked are the ones loaded into the memory. As said, data.table is an enhanced venison of data.frame. Thus, recall what we did in the lecture about data.frame. We will learn how to read data file into R as data.table. how to manipulate data.table. We will use the flights dataset for the purpose of illustration. The flights dataset contains flights information for all the flights departing from New York City airports in 2013. This dataset is from from the Bureau of Transporation Statistics. 8.2 fread(): Read csv file into R as data.table Remember, in read.csv() reads a dataset into R as data.frame. In comparison, fread() from data.table package will read a csv file into R as a data.table. # type ?fread to check its help document ?fread Now, lets read flights.csv into R using fread() function in data.table package. flights=fread(&quot;data/flights.csv&quot;) The flights.csv is 31.9MB, with 336,776 observation and 19 variables. This is not a small dataset. But as you can feel, fread() is blazing fast. We can use the class() function to examine the type of the data.table (flights) we just created: class(flights) ## [1] &quot;data.table&quot; &quot;data.frame&quot; As seen, flights belongs to both data.table and data.frame. As mentioned, this is because data.table is an extension (enhancement of data.frame). Therefore, all functions works on data.frame all works on data.table. Now, lets compare the performance of data.table and data.frame in terms reading data into R. Performance benchmark between different programs is very common to determine which one is better. system.time( read.csv(&quot;data/flights.csv&quot;) ) ## user system elapsed ## 1.48 0.09 1.58 system.time( fread(&quot;data/flights.csv&quot;) ) ## user system elapsed ## 0.11 0.02 0.05 As seen, read.csv() spends 1.42 seconds to read the data; while fread() spends 0.06 seconds to read the data. fread() is 23 times faster (this will be different depending on the machine you used). The advantage will be even prominent if we are dealing with even big file. 8.2.1 Convert data.frame into data.table You can convert an existing data.frame into a data.table using data.table() function. tmp=read.csv(&quot;data/flights.csv&quot;) class(tmp) ## [1] &quot;data.frame&quot; tmp=data.table(tmp) class(tmp) ## [1] &quot;data.table&quot; &quot;data.frame&quot; 8.3 Quick summary of the data In the excel file, we can easily see the data in the tabular format. With R-stuido, you can click flights on the top right corner to see the data. But more commonly, we will examine the data through a few quick summary: Use head()/tail() to show the first/last 6 rows of the data. head(flights) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 517 515 2 830 819 ## 2: 2013 1 1 533 529 4 850 830 ## 3: 2013 1 1 542 540 2 923 850 ## 4: 2013 1 1 544 545 -1 1004 1022 ## 5: 2013 1 1 554 600 -6 812 837 ## 6: 2013 1 1 554 558 -4 740 728 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2: 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3: 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4: -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5: -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6: 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## time_hour ## 1: 2013-01-01 05:00:00 ## 2: 2013-01-01 05:00:00 ## 3: 2013-01-01 05:00:00 ## 4: 2013-01-01 05:00:00 ## 5: 2013-01-01 06:00:00 ## 6: 2013-01-01 05:00:00 tail(flights) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 9 30 NA 1842 NA NA 2019 ## 2: 2013 9 30 NA 1455 NA NA 1634 ## 3: 2013 9 30 NA 2200 NA NA 2312 ## 4: 2013 9 30 NA 1210 NA NA 1330 ## 5: 2013 9 30 NA 1159 NA NA 1344 ## 6: 2013 9 30 NA 840 NA NA 1020 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: NA EV 5274 N740EV LGA BNA NA 764 18 42 ## 2: NA 9E 3393 &lt;NA&gt; JFK DCA NA 213 14 55 ## 3: NA 9E 3525 &lt;NA&gt; LGA SYR NA 198 22 0 ## 4: NA MQ 3461 N535MQ LGA BNA NA 764 12 10 ## 5: NA MQ 3572 N511MQ LGA CLE NA 419 11 59 ## 6: NA MQ 3531 N839MQ LGA RDU NA 431 8 40 ## time_hour ## 1: 2013-09-30 18:00:00 ## 2: 2013-09-30 14:00:00 ## 3: 2013-09-30 22:00:00 ## 4: 2013-09-30 12:00:00 ## 5: 2013-09-30 11:00:00 ## 6: 2013-09-30 08:00:00 Use summary() to get summary statistics for each variable. summary(flights) ## year month day dep_time sched_dep_time ## Min. :2013 Min. : 1.000 Min. : 1.00 Min. : 1 Min. : 106 ## 1st Qu.:2013 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.: 907 1st Qu.: 906 ## Median :2013 Median : 7.000 Median :16.00 Median :1401 Median :1359 ## Mean :2013 Mean : 6.549 Mean :15.71 Mean :1349 Mean :1344 ## 3rd Qu.:2013 3rd Qu.:10.000 3rd Qu.:23.00 3rd Qu.:1744 3rd Qu.:1729 ## Max. :2013 Max. :12.000 Max. :31.00 Max. :2400 Max. :2359 ## NA&#39;s :8255 ## dep_delay arr_time sched_arr_time arr_delay ## Min. : -43.00 Min. : 1 Min. : 1 Min. : -86.000 ## 1st Qu.: -5.00 1st Qu.:1104 1st Qu.:1124 1st Qu.: -17.000 ## Median : -2.00 Median :1535 Median :1556 Median : -5.000 ## Mean : 12.64 Mean :1502 Mean :1536 Mean : 6.895 ## 3rd Qu.: 11.00 3rd Qu.:1940 3rd Qu.:1945 3rd Qu.: 14.000 ## Max. :1301.00 Max. :2400 Max. :2359 Max. :1272.000 ## NA&#39;s :8255 NA&#39;s :8713 NA&#39;s :9430 ## carrier flight tailnum origin ## Length:336776 Min. : 1 Length:336776 Length:336776 ## Class :character 1st Qu.: 553 Class :character Class :character ## Mode :character Median :1496 Mode :character Mode :character ## Mean :1972 ## 3rd Qu.:3465 ## Max. :8500 ## ## dest air_time distance hour ## Length:336776 Min. : 20.0 Min. : 17 Min. : 1.00 ## Class :character 1st Qu.: 82.0 1st Qu.: 502 1st Qu.: 9.00 ## Mode :character Median :129.0 Median : 872 Median :13.00 ## Mean :150.7 Mean :1040 Mean :13.18 ## 3rd Qu.:192.0 3rd Qu.:1389 3rd Qu.:17.00 ## Max. :695.0 Max. :4983 Max. :23.00 ## NA&#39;s :9430 ## minute time_hour ## Min. : 0.00 Min. :2013-01-01 05:00:00 ## 1st Qu.: 8.00 1st Qu.:2013-04-04 13:00:00 ## Median :29.00 Median :2013-07-03 10:00:00 ## Mean :26.23 Mean :2013-07-03 05:02:36 ## 3rd Qu.:44.00 3rd Qu.:2013-10-01 07:00:00 ## Max. :59.00 Max. :2013-12-31 23:00:00 ## str() is THE function for examining structure of a dataset. str(flights) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 336776 obs. of 19 variables: ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : int 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : int 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : int 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : int 1400 1416 1089 1576 762 719 1065 229 944 733 ... ## $ hour : int 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : int 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct, format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; 8.4 Data Manipulation with data.table In many cases, we need to inquiry and subset a dataset. Structured Query Language (SQL) is designed for that purpose. data.table syntax is actually very similar to the SQL syntax. data.table provides a simple, consistent, user-friendly syntax for data manipulation. The general form of data.table syntax is: DT[i, j, by] where DT is a data.table. by: grouped by what? j: what to do? i: on which rows? We will apply this form to manipulate data: Filtering rows, selecting columns, aggregating,  # # Filtering Rows Here we show how to filter particular rows with data.table. 8.4.1 Filtering rows through logical expression We can filter rows based on logical expression. E.g., # filtering flights with &quot;JFK&quot; as the origin in the month of June. tmp &lt;- flights[origin == &quot;JFK&quot; &amp; month == 6] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 6 1 2 2359 3 341 350 ## 2: 2013 6 1 538 545 -7 925 922 ## 3: 2013 6 1 539 540 -1 832 840 ## 4: 2013 6 1 553 600 -7 700 711 ## 5: 2013 6 1 554 600 -6 851 908 ## 6: 2013 6 1 557 600 -3 934 942 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: -9 B6 739 N618JB JFK PSE 200 1617 23 59 ## 2: 3 B6 725 N806JB JFK BQN 203 1576 5 45 ## 3: -8 AA 701 N5EAAA JFK MIA 140 1089 5 40 ## 4: -11 EV 5716 N835AS JFK IAD 42 228 6 0 ## 5: -17 UA 1159 N33132 JFK LAX 330 2475 6 0 ## 6: -8 B6 715 N766JB JFK SJU 198 1598 6 0 ## time_hour ## 1: 2013-06-01 23:00:00 ## 2: 2013-06-01 05:00:00 ## 3: 2013-06-01 05:00:00 ## 4: 2013-06-01 06:00:00 ## 5: 2013-06-01 06:00:00 ## 6: 2013-06-01 06:00:00 As see, this is much clear and easy to read compared to the syntax under data.frame; and we do not need to write flights$origin, flights$month as we did in data.frame. 8.4.2 Filtering rows by row index We can also filter rows by index. E.g., # filtering the first two rows from flights. flights[1:2] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 517 515 2 830 819 ## 2: 2013 1 1 533 529 4 850 830 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2: 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## time_hour ## 1: 2013-01-01 05:00:00 ## 2: 2013-01-01 05:00:00 In data.table, .N is a special symbol that contains the number of rows of the data.table. E.g., # Return the last row of flights flights[.N] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 9 30 NA 840 NA NA 1020 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: NA MQ 3531 N839MQ LGA RDU NA 431 8 40 ## time_hour ## 1: 2013-09-30 08:00:00 # same as flights[336776] flights[336776] ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 9 30 NA 840 NA NA 1020 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: NA MQ 3531 N839MQ LGA RDU NA 431 8 40 ## time_hour ## 1: 2013-09-30 08:00:00 8.4.3 Fitering rows through %between%, %chin%, %like% %between% allows you to search for numerical values in the closed interval [val1, val2]. The syntax is: numeric_col %between% c(val1, val2) # filtering all flights with arr_delay between 120 to 180 minutes tmp=flights[arr_delay%between% c(120, 180)] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 811 630 101 1047 830 ## 2: 2013 1 1 957 733 144 1056 853 ## 3: 2013 1 1 1114 900 134 1447 1222 ## 4: 2013 1 1 1505 1310 115 1638 1431 ## 5: 2013 1 1 1525 1340 105 1831 1626 ## 6: 2013 1 1 1549 1445 64 1912 1656 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 137 MQ 4576 N531MQ LGA CLT 118 544 6 30 ## 2: 123 UA 856 N534UA EWR BOS 37 200 7 33 ## 3: 145 UA 1086 N76502 LGA IAH 248 1416 9 0 ## 4: 127 EV 4497 N17984 EWR RIC 63 277 13 10 ## 5: 125 B6 525 N231JB EWR MCO 152 937 13 40 ## 6: 136 EV 4181 N21197 EWR MCI 234 1092 14 45 ## time_hour ## 1: 2013-01-01 06:00:00 ## 2: 2013-01-01 07:00:00 ## 3: 2013-01-01 09:00:00 ## 4: 2013-01-01 13:00:00 ## 5: 2013-01-01 13:00:00 ## 6: 2013-01-01 14:00:00 %chin% is only for character vectors, it allows you to filter rows with character values in a set: character_col %chin% c(val1, val2, val3) # Filtering flights with &quot;JFK&quot; or &quot;LGA&quot; as origin in the month of May tmp &lt;- flights[origin %chin% c(&quot;JFK&quot;,&quot;LGA&quot;) &amp; month==5] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 5 1 9 1655 434 308 2020 ## 2: 2013 5 1 537 540 -3 836 840 ## 3: 2013 5 1 544 545 -1 818 827 ## 4: 2013 5 1 548 600 -12 831 854 ## 5: 2013 5 1 549 600 -11 804 810 ## 6: 2013 5 1 553 600 -7 700 712 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 408 VX 413 N628VA JFK LAX 341 2475 16 55 ## 2: -4 AA 701 N5BYAA JFK MIA 144 1089 5 40 ## 3: -9 UA 450 N494UA LGA IAH 190 1416 5 45 ## 4: -23 B6 371 N523JB LGA FLL 140 1076 6 0 ## 5: -6 MQ 4650 N520MQ LGA ATL 113 762 6 0 ## 6: -12 EV 5747 N877AS LGA IAD 48 229 6 0 ## time_hour ## 1: 2013-05-01 16:00:00 ## 2: 2013-05-01 05:00:00 ## 3: 2013-05-01 05:00:00 ## 4: 2013-05-01 06:00:00 ## 5: 2013-05-01 06:00:00 ## 6: 2013-05-01 06:00:00 %like% allows you to search for a pattern in a character or a factor vector. It is best to illustrate what pattern means with an example. E.g., the two characters AA and AS (acronym for American Airline and Alaska Airlines) are both started with a upper case U. This is called a pattern. In R, ^A is used to denoted this pattern. We will talk more about pattern when we deal with strings. # filter carriers whose acronym starts with A. tmp &lt;- flights[carrier %like% &quot;^A&quot;] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 542 540 2 923 850 ## 2: 2013 1 1 558 600 -2 753 745 ## 3: 2013 1 1 559 600 -1 941 910 ## 4: 2013 1 1 606 610 -4 858 910 ## 5: 2013 1 1 623 610 13 920 915 ## 6: 2013 1 1 628 630 -2 1137 1140 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 2: 8 AA 301 N3ALAA LGA ORD 138 733 6 0 ## 3: 31 AA 707 N3DUAA LGA DFW 257 1389 6 0 ## 4: -12 AA 1895 N633AA EWR MIA 152 1085 6 10 ## 5: 5 AA 1837 N3EMAA LGA MIA 153 1096 6 10 ## 6: -3 AA 413 N3BAAA JFK SJU 192 1598 6 30 ## time_hour ## 1: 2013-01-01 05:00:00 ## 2: 2013-01-01 06:00:00 ## 3: 2013-01-01 06:00:00 ## 4: 2013-01-01 06:00:00 ## 5: 2013-01-01 06:00:00 ## 6: 2013-01-01 06:00:00 8.5 Select Columns The general form of data.table syntax is: DT[i, j, by] where by: grouped by what? j: what to do? i: on which rows? The second argument j is used to select (and compute on) columns. 8.5.1 Using column names to select columns Because data.table is an enhanced data.frame, we can select columns by column names in accordance with data.frame. # Select dep_delay and arr_delay column tmp &lt;- flights[, c(&quot;dep_delay&quot;,&quot;arr_delay&quot;)] # show the first 6 rows using head() function. head(tmp) ## dep_delay arr_delay ## 1: 2 11 ## 2: 4 20 ## 3: 2 33 ## 4: -1 -18 ## 5: -6 -25 ## 6: -4 12 # create a scatter plot between these two variables. plot(tmp) The plot shows that there is a strong linear relationship between dep_delay and arr_delay, which essentially means: if you deptures late by x minutes, you are very likely to arrive later by x minutes. Note that, as seen in flights[, c(dep_delay,arr_delay)], although we do not filter on rows, we need to include , to add a place holder for argument i. Otherwise, the code will generate an error msg. We can also select column based on the column index, which, however, is not strongly not recommended. flights[,c(6,9)] # dep_delay and arr_delay are at 6,9th column This is not recommended because if the order of columns changes (which is likely to happen if you are deleting or adding columns), the result will be wrong. We can also deselecting columns: -c(col1, col2) deselects the columns col1 and col2; in other words, delete col1 and col2. # disselect arr_delay and dep_delay column flights[,-c(&quot;dep_delay&quot;,&quot;arr_delay&quot;)] 8.5.2 Select columns as variables - the data.table way In data.table, each column is treated as a variable, thus, you can use a list of variables (column names) to select columns. # Select the arr_delay and dep_delay column. tmp &lt;- flights[, list(dep_delay,arr_delay)] The difference of this code and the above code is that dep_delay and arr_delay are not inside \"\". This is because dep_delay and arr_delay are treated as variable, rather column names. .() is an alias to list(), for convenience. We can use .() to replace list(). This reduces typing and allows you to focus on the variables that are selected when reading the code. tmp &lt;- flights[, .(dep_delay,arr_delay)] We can also rename the column while selecting the column. E.g., the following code select arr_delay and dep_delay and rename to arrive_delay and depart_delay tmp &lt;- flights[, .(arrive_delay = arr_delay, depart_delay = dep_delay)] head(tmp) ## arrive_delay depart_delay ## 1: 11 2 ## 2: 20 4 ## 3: 33 2 ## 4: -18 -1 ## 5: -25 -6 ## 6: 12 -4 8.5.3 Computing on columns Since columns in data.table can be referred to as variables, you can compute directly on them in j. We will find this is a quick and easy way to get summary statistics from the data. E.g., you want to know the average arr_delay and dep_delay of all the fligths? in other words, you want to compute the mean of arr_delay and dep_delay. # calcuate the mean arr_delay flights[,mean(arr_delay)] ## [1] NA flights[,mean(arr_delay, na.rm = TRUE)] ## [1] 6.895377 We can also rename the calculated columns: # calcuate the mean arr_delay and dep_delay flights[,.(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE))] ## V1 V2 ## 1: 6.895377 12.63907 You can compute and name multiple variables at the same time: # calcuate the mean arr_delay and dep_delay flights[,.(avg_arr_delay=mean(arr_delay, na.rm = TRUE), avg_dep_delay=mean(dep_delay, na.rm = TRUE))] ## avg_arr_delay avg_dep_delay ## 1: 6.895377 12.63907 8.5.4 Computing on rows and columns Combining i and j is straightforward. E.g., you want to compute the average arr_deply and dep_deply for all flights origined from JFK in the month of Jun flights[origin==&quot;JFK&quot; &amp; month==6,.(avg_arr_delay=mean(arr_delay, na.rm = TRUE), avg_dep_delay=mean(dep_delay, na.rm = TRUE))] ## avg_arr_delay avg_dep_delay ## 1: 17.59693 20.49973 Now, let break this code to understand how it works. We first subset in i to find matching row indices where origin airport equals JFK, and month equals 6; next we look at j and select the two columns and compute their mean(). Note that the code will return a data.table. 8.5.5 Special symbol .N in j Remember .N denotes the number of rows. .N can be used in j as well. It is particularly useful to get the number of rows after filtering in i. E.g., you want to compute how many flights are delayed (i.e., arr_delay + dep_delay&gt;0). flights[arr_delay + dep_delay&gt;0, .N] ## [1] 135059 In the above code, the data.table is first filtered by arr_delay + dep_delay&lt;=0, and then compute the number of selected rows. 8.5.6 Advanced Computation on columns Because the columns can be treated as variable in data.table, thus we can perform complex calculation on these variable in j. E.g., we want to compute the average speed of all flights, where speed is measured by distance/(airm_time/60) (miles per hour)? flights[, .(miles_per_hr=mean( 60*distance/air_time , na.rm = TRUE))] ## miles_per_hr ## 1: 394.2737 8.5.7 Sort Data by One/Multiple Columns The data.table has very simple syntax to sort data based on one/multiple columns. E.g., sort flights first by origin in ascending order, and then by dest in descending order # Use the order() function to sort by columns. tmp &lt;- flights[order(origin, -dest)] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 2 905 822 43 1313 1045 ## 2: 2013 1 3 848 850 -2 1149 1113 ## 3: 2013 1 4 901 850 11 1120 1113 ## 4: 2013 1 6 843 848 -5 1053 1111 ## 5: 2013 1 7 858 850 8 1105 1113 ## 6: 2013 1 8 847 850 -3 1116 1113 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: NA EV 4140 N15912 EWR XNA NA 1131 8 22 ## 2: 36 EV 4125 N21129 EWR XNA 196 1131 8 50 ## 3: 7 EV 4125 N16178 EWR XNA 168 1131 8 50 ## 4: -18 EV 4625 N12172 EWR XNA 174 1131 8 48 ## 5: -8 EV 4125 N13118 EWR XNA 163 1131 8 50 ## 6: 3 EV 4125 N14180 EWR XNA 177 1131 8 50 ## time_hour ## 1: 2013-01-02 08:00:00 ## 2: 2013-01-03 08:00:00 ## 3: 2013-01-04 08:00:00 ## 4: 2013-01-06 08:00:00 ## 5: 2013-01-07 08:00:00 ## 6: 2013-01-08 08:00:00 8.6 Calcuation by Group Weve already seen i and j from data.tables general form. Here, well see how they can be combined together with by to perform operations by group. E.g., how can we get the number of flights corresponding to each origin airport? flights[, .(count=.N), by = .(origin)] ## origin count ## 1: EWR 120835 ## 2: LGA 104662 ## 3: JFK 111279 e.g., How can we get the total number of flights and average arrival delay and std deviation of arrial delay for each origin, dest pair for carrier code AA? tmp&lt;-flights[carrier == &quot;AA&quot;, .(count=.N, avg_arr_delay=mean(arr_delay, na.rm = TRUE), sd_arr_delay=sd(arr_delay, na.rm = TRUE)), by = .(origin, dest)] head(tmp) ## origin dest count avg_arr_delay sd_arr_delay ## 1: JFK MIA 2221 -3.23122440 36.81734 ## 2: LGA ORD 5694 -1.31396831 41.07306 ## 3: LGA DFW 4836 -1.18901523 39.21697 ## 4: EWR MIA 1068 0.06332703 54.78252 ## 5: LGA MIA 3945 -1.64248971 37.45564 ## 6: JFK SJU 1099 -0.77603687 37.28137 From this summary, we can see that AA on average arrive on time. However, the high std deviation means there is great variation in terms of the arrive delay. How can we get the average arrival and departure delay for each origin, dest pair for each month for carrier code AA? tmp=flights[carrier == &quot;AA&quot;, .(avg_arr_delay=mean(arr_delay), avg_dep_delay=mean(dep_delay)), by = .(origin, dest, month)] # plot the avg_arr_delay, avg_dep_delay and use color to mark different month library(ggplot2) ggplot(tmp,aes(avg_arr_delay, avg_dep_delay, color=factor(month)))+ geom_point() ## Warning: Removed 151 rows containing missing values (geom_point). The data.table summary and the plot together provide a great way to visualize which month are more likely to have flights delay. Note that, the above code retain the original ordering of origin-dest pair. There are cases when preserving the original order is essential. However, if we want to automatically sort by the variables in our grouping, we can simply change by to keyby # sort by origin, dest, month tmp &lt;- flights[carrier == &quot;AA&quot;, .(avg_arr_delay=mean(arr_delay, na.rm = TRUE), avg_dep_delay=mean(dep_delay, na.rm = TRUE)), keyby = .(origin, dest, month)] head(tmp) ## origin dest month avg_arr_delay avg_dep_delay ## 1: EWR DFW 1 8.042945 9.587879 ## 2: EWR DFW 2 8.222222 11.855172 ## 3: EWR DFW 3 1.113095 16.035503 ## 4: EWR DFW 4 13.067485 16.713415 ## 5: EWR DFW 5 -6.158537 10.053571 ## 6: EWR DFW 6 4.148148 18.153374 Notice that, the datatable is ordered based on variables in our grouping (origin-dest-month). This is easy for reader to examine the the pattern in our data in a tabular form. 8.7 Summary R has currently 120,000 packages, which can enhance the power of base R. data.table is a enhanced verison of data.frame to handle big data. Learn how to install and load R packages. data.table syntax: DT[i, j, by]: by - grouped by what; j - what to do; i - on which rows. use data.table syntax to manipulate the flights dataset: filtering rows, selecting columns; computing columns; aggregrating by group. "],["references.html", "References", " References "],["use-data.table-package-for-big-data-ii.html", "Chapter 9 Use data.table Package for Big Data II 9.1 Use Chaining to Avoid Intermediate Result 9.2 uniqueN() 9.3 Subset of Data: .SD[] 9.4 Use := to Add/Update Columns By Reference 9.5 Binning a continuous value into category 9.6 Expressions in by 9.7 Summary", " Chapter 9 Use data.table Package for Big Data II Here we will look at some advanced use of data.table. You will be amazed by the power of data.table for data analysis. Again, we need to first load data.table package first. library(data.table) As a quick review, we have learned to use fread() to read csv file into R as data.table; or use data.table() to convert an existing data.frame into a data.table. data.table provides a simple, consistent, user-friendly syntax for data manipulation. The general form of data.table syntax is: DT[i, j, by] where DT is a data.table. by: grouped by what? j: what to do? i: on which rows? In this chapter,we will explore the advanced use of data.table, using the flights dataset as an illustration. Lets read flights.csv into R using fread() function in data.table package. flights=fread(&quot;data/flights.csv&quot;) 9.1 Use Chaining to Avoid Intermediate Result Lets reconsider the task of getting the total number of flights for each origin, destination pair for carrier AA: tmp &lt;- flights[carrier == &quot;AA&quot;, .(count=.N), by = .(origin, dest)] How can we order tmp using the columns origin in ascending order, and destination in descending order? We can store the intermediate result in a data.table, and then apply order(origin, -dest) on that data.table. It seems fairly straightforward. tmp &lt;- tmp[order(origin, -dest)] head(tmp,8) ## origin dest count ## 1: EWR MIA 1068 ## 2: EWR LAX 365 ## 3: EWR DFW 2054 ## 4: JFK TPA 311 ## 5: JFK STT 303 ## 6: JFK SJU 1099 ## 7: JFK SFO 1422 ## 8: JFK SEA 365 This requires having to create an intermediate data.table and then overwriting that data.table. The intermediate data.table has no other use. When you are working on big data, these intermediate data will soon consumes your computer memory and makes the code slow to run. We can do better and avoid this intermediate data.table altogether by chaining expressions. For the above task, we can: tmp &lt;- flights[carrier == &quot;AA&quot;, .(count=.N), by = .(origin, dest)][order(origin, -dest)] # print the first 8 rows of ans head(tmp, 8) ## origin dest count ## 1: EWR MIA 1068 ## 2: EWR LAX 365 ## 3: EWR DFW 2054 ## 4: JFK TPA 311 ## 5: JFK STT 303 ## 6: JFK SJU 1099 ## 7: JFK SFO 1422 ## 8: JFK SEA 365 Lets look at this code. [carrier == AA, .(count=.N), by = .(origin, dest)] is first executed; followed by [order(origin, -dest)]. We do not need to create an intermediate result. In general, we can tack expressions one after another, forming a chain of operations, i.e., DT[  ][  ][  ]. The chaining operation is very important in R to speedup the R code and has been implemented in many other packages, e.g., the pipe, %&gt;%, from the magrittr package. We will take about the chaining with pipe operator %&gt;% in later chapters. Lets look at the another example: Find the origin, dest pair with the longest average air_time. flights[,.(avg_air_time=mean(air_time)), by=.(origin, dest)][order(-avg_air_time)][1:3] ## origin dest avg_air_time ## 1: JFK HNL 623.0877 ## 2: EWR ANC 413.1250 ## 3: JFK JAC 275.5000 The result shows that the longest flight pair is JFK-HNL (Hawaii). Chaining allows you to answer questions like this with one simple line of code! Really cool! Now, lets spend sometime examine the code. The first expression [,.(avg_air_time=mean(air_time)), by=.(origin, dest)] is excuted first, and the new data.table is created in the memory. Since avg_air_time is created, the second expression [order(-avg_air_time)] can use this new variable avg_air_time for sorting. The third expression then filtering the top three observations. 9.2 uniqueN() uniqueN() is a handy function that returns an integer value containing the number of unique values in the input object. It accepts vectors as well as data.frames and data.tables as its argument. E.g., we want to find out how many unique id in this vector: student_id=c(1,2,3,4,1,2) student_id=c(1,2,3,4,1,2) uniqueN(student_id) ## [1] 4 In data.table, we can use uniqueN() together with by to get summary statistics such as: how many direct flights are at each origin airport? In other words, how many unique destination for each origin? flights[, uniqueN(dest), by=.(origin)] ## origin V1 ## 1: EWR 86 ## 2: LGA 68 ## 3: JFK 70 Thus, we know that EWR airport has direct flights to 86 other airports. 9.3 Subset of Data: .SD[] .SD is a special symbol which stands for Subset of Data. It contains subset of data corresponding to each group; which itself is a data.table. Why we need this? Lets look at one example: You want to the find out, for each origin, dest pair, which flight has the longest arr_delay? Intuitively, we know you should sort the data according to origin, dest, arr_delay. flights[order(origin,dest,-arr_delay)] To help you see result of the above code, lets print the first few rows and a few columns (origin, dest, month, day, arr_delay). head( flights[order(origin,dest,-arr_delay)][,.(origin,dest,month,day,arr_delay)] ) ## origin dest month day arr_delay ## 1: EWR ALB 1 25 328 ## 2: EWR ALB 12 5 300 ## 3: EWR ALB 1 31 268 ## 4: EWR ALB 3 8 263 ## 5: EWR ALB 2 26 217 ## 6: EWR ALB 3 15 190 But how to retrieve the top row for each origin, dest pair? .SD[] is designed for that purpose. As mentioned, .SD denotes the data.table which contains subset of data corresponding to each group. We can use .SD[i] to retrieve the i-th row from the data.table. # use .SD[] to retrieve the top row for each group. flights[order(origin,dest,-arr_delay)][, .SD[1], by=.(origin,dest)] We can use .SDcols to select columns contained in .SD. # use .SD[] to select top row for each group and select month, day, arr_delay flights[order(origin,dest,-arr_delay)][, .SD[1], by=.(origin,dest),.SDcols=c(&quot;month&quot;,&quot;day&quot;,&quot;arr_delay&quot;)] You can also select the top n rows for each group using .SD # use .SD[] to select top three row for each group and select month, day, arr_delay flights[order(origin,dest,-arr_delay)][, .SD[1:3], by=.(origin,dest),.SDcols=c(&quot;month&quot;,&quot;day&quot;,&quot;arr_delay&quot;)] Using is .SD[] provides an easy way to identify the outlier (or the most interesting data point) in our data for each group. This can help us to answer question such as: what is the top sales person for each region? which county has the top population for each state? 9.4 Use := to Add/Update Columns By Reference data.table defines a new operator := for adding/updating columns by reference. The syntax of := is as follow: LHS := RHS E.g., we want to create a new variable called total_delay, which equals to arr_delay+arr_delay. We can use the following code: flights[,total_delay:=arr_delay+arr_delay] str(flights) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 336776 obs. of 20 variables: ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : int 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : int 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : int 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : int 1400 1416 1089 1576 762 719 1065 229 944 733 ... ## $ hour : int 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : int 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct, format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... ## $ total_delay : int 22 40 66 -36 -50 24 38 -28 -16 16 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; ## - attr(*, &quot;index&quot;)= int(0) ## ..- attr(*, &quot;__carrier&quot;)= int [1:336776] 117 428 429 434 452 482 496 501 504 506 ... We will see the variable total_delay is created. You may be wondering what updating by reference means. Under the data.frame, when we need to update a column, R will read the whole data.frame into memory, update the whole data.frame and write the updated data.frame into a new object. This is really inefficient. Under data.table with :=, we do not need to updated the whole data.table object; Instead, we just refer the new column to the computer memory address that contains the new column. Therefore, := is much more efficient. The efficiency difference is particular evident when working with big data. You can also create multiple new columns simultaneously using := . E.g., you need to create a total_delay and speed=distance/(air_time/60). Note that speed is measured by miles per hour. flights[,c(&quot;total_delay&quot;,&quot;speed&quot;):=.(arr_delay+arr_delay, distance/air_time*60)] str(flights) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 336776 obs. of 21 variables: ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : int 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : int 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : int 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : int 1400 1416 1089 1576 762 719 1065 229 944 733 ... ## $ hour : int 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : int 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct, format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... ## $ total_delay : int 22 40 66 -36 -50 24 38 -28 -16 16 ... ## $ speed : num 370 374 408 517 394 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; ## - attr(*, &quot;index&quot;)= int(0) ## ..- attr(*, &quot;__carrier&quot;)= int [1:336776] 117 428 429 434 452 482 496 501 504 506 ... Note that, on the LHS of :=, i.e., c(total_delay,speed), we used variable name inside \"\" because these variables are not yet defined in the data.table and thus cannot be referred as variables directly. We can combine := with by group argument to create new variables by groups. E.g., we want to add a new column avg_air_time which is the average air_time for the origin, dest pair. tmp=flights[, avg_air_time:=mean(air_time, na.rm = TRUE), by = .(origin, dest)] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 517 515 2 830 819 ## 2: 2013 1 1 533 529 4 850 830 ## 3: 2013 1 1 542 540 2 923 850 ## 4: 2013 1 1 544 545 -1 1004 1022 ## 5: 2013 1 1 554 600 -6 812 837 ## 6: 2013 1 1 554 558 -4 740 728 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2: 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3: 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4: -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5: -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6: 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## time_hour total_delay speed avg_air_time ## 1: 2013-01-01 05:00:00 22 370.0441 195.8404 ## 2: 2013-01-01 05:00:00 40 374.2731 200.7932 ## 3: 2013-01-01 05:00:00 66 408.3750 152.1300 ## 4: 2013-01-01 05:00:00 -36 516.7213 194.2411 ## 5: 2013-01-01 06:00:00 -50 394.1379 113.5550 ## 6: 2013-01-01 05:00:00 24 287.6000 113.2603 Note that we created a new variable as the average air time. Note that we can also compute on j by group as we illustrate in previous chapter (shown in the code below. However, we cannot add this calculation as a new column to the original data.table as we did with :=. tmp=flights[, .(avg_air_time=mean(air_time, na.rm = TRUE)), by = .(origin, dest)] head(tmp) ## origin dest avg_air_time ## 1: EWR IAH 195.8404 ## 2: LGA IAH 200.7932 ## 3: JFK MIA 152.1300 ## 4: JFK BQN 194.2411 ## 5: LGA ATL 113.5550 ## 6: EWR ORD 113.2603 9.5 Binning a continuous value into category In many application, we want to convert a continuous value into levels. E.g., we want to classify flights into short/median/long flight based on the average distance. Specifically, if average distance&lt;=520 (1st Qu), short; else if average distance&lt;=1389 (3rd Qu.), median; else long. distance_level &lt;- function(distance) { avg_distance=mean(distance, na.rm = TRUE) if (avg_distance &lt;= 520) &quot;short&quot; else if (avg_distance &lt;= 1389) &quot;medium&quot; else &quot;long&quot; } tmp=flights[, distance_level:= .( distance_level(distance) ), by=.(origin, dest)] head(tmp) ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## 1: 2013 1 1 517 515 2 830 819 ## 2: 2013 1 1 533 529 4 850 830 ## 3: 2013 1 1 542 540 2 923 850 ## 4: 2013 1 1 544 545 -1 1004 1022 ## 5: 2013 1 1 554 600 -6 812 837 ## 6: 2013 1 1 554 558 -4 740 728 ## arr_delay carrier flight tailnum origin dest air_time distance hour minute ## 1: 11 UA 1545 N14228 EWR IAH 227 1400 5 15 ## 2: 20 UA 1714 N24211 LGA IAH 227 1416 5 29 ## 3: 33 AA 1141 N619AA JFK MIA 160 1089 5 40 ## 4: -18 B6 725 N804JB JFK BQN 183 1576 5 45 ## 5: -25 DL 461 N668DN LGA ATL 116 762 6 0 ## 6: 12 UA 1696 N39463 EWR ORD 150 719 5 58 ## time_hour total_delay speed avg_air_time distance_level ## 1: 2013-01-01 05:00:00 22 370.0441 195.8404 long ## 2: 2013-01-01 05:00:00 40 374.2731 200.7932 long ## 3: 2013-01-01 05:00:00 66 408.3750 152.1300 medium ## 4: 2013-01-01 05:00:00 -36 516.7213 194.2411 long ## 5: 2013-01-01 06:00:00 -50 394.1379 113.5550 medium ## 6: 2013-01-01 05:00:00 24 287.6000 113.2603 medium 9.6 Expressions in by Recall the data.table syntax DT[i, j, by]. Like argument j, argument by also accepts expressions? As an example, if we would like to find out how many flights departed late but arrived early, departed and arrived late, departed early and arrived early, departed early but arrived late: flights[, .(count=.N), by=.(dep_late=dep_delay&gt;0, arr_late=arr_delay&gt;0)] ## dep_late arr_late count ## 1: TRUE TRUE 92303 ## 2: FALSE FALSE 158900 ## 3: FALSE TRUE 40701 ## 4: TRUE FALSE 35442 ## 5: FALSE NA 488 ## 6: TRUE NA 687 ## 7: NA NA 8255 We can further show calcuate count as percentage of total. flights[, .(count=.N), by=.(dep_late=dep_delay&gt;0, arr_late=arr_delay&gt;0)][, percent:=count/sum(count)][] ## dep_late arr_late count percent ## 1: TRUE TRUE 92303 0.274078319 ## 2: FALSE FALSE 158900 0.471826971 ## 3: FALSE TRUE 40701 0.120854812 ## 4: TRUE FALSE 35442 0.105239091 ## 5: FALSE NA 488 0.001449034 ## 6: TRUE NA 687 0.002039932 ## 7: NA NA 8255 0.024511842 As seen, 27.4% of the flights does not depart nor arrive late. About half of flights depart and arrive on time. This is a very useful and quick way to slice data into different groups and compute summary statistics accordingly. 9.7 Summary The basic data.table syntax DT[ i,j, by] is like a building block and can be chained DT[i, j, by][][] to avoid intermediate results. Use uniqueN() to count the unqiue value in a group Use .SD[ ] to select the top n rows in a group Use := to add new columns in a data.table Convert a continuous value into category Use expression in by "],["join-mutliple-datasets.html", "Chapter 10 Join Mutliple Datasets 10.1 Merge dataset using data.table syntax 10.2 Chaining Join and Data Manipulation 10.3 Exercise 10.4 Summary", " Chapter 10 Join Mutliple Datasets Its rare that data scientists work with only a single table of data during a project. Typically you have many tables of data, and you must combine them to answer the questions that youre interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. As an illustration, in addition to the flights dataset, we will use planes.csv, which contains the information about the airplane for each flights in the flights.csv dataset and weather.csv dataset, which contains the weather information for the time period when flights.csv is collected. First thing first, we need to load the data.table into R. library(data.table) Lets read these three data file into R using fread() and examine these datasets. flights=fread(&quot;data/flights.csv&quot;) str(flights) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 336776 obs. of 19 variables: ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : int 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : int 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : int 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : int 1400 1416 1089 1576 762 719 1065 229 944 733 ... ## $ hour : int 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : int 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct, format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; weather=fread(&quot;data/weather.csv&quot;) str(weather) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 26115 obs. of 15 variables: ## $ origin : chr &quot;EWR&quot; &quot;EWR&quot; &quot;EWR&quot; &quot;EWR&quot; ... ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int 1 1 1 1 1 1 1 1 1 1 ... ## $ hour : int 1 2 3 4 5 6 7 8 9 10 ... ## $ temp : num 39 39 39 39.9 39 ... ## $ dewp : num 26.1 27 28 28 28 ... ## $ humid : num 59.4 61.6 64.4 62.2 64.4 ... ## $ wind_dir : int 270 250 240 250 260 240 240 250 260 260 ... ## $ wind_speed: num 10.36 8.06 11.51 12.66 12.66 ... ## $ wind_gust : num NA NA NA NA NA NA NA NA NA NA ... ## $ precip : num 0 0 0 0 0 0 0 0 0 0 ... ## $ pressure : num 1012 1012 1012 1012 1012 ... ## $ visib : num 10 10 10 10 10 10 10 10 10 10 ... ## $ time_hour : POSIXct, format: &quot;2013-01-01 01:00:00&quot; &quot;2013-01-01 02:00:00&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; Weather is uniquely identify by origin, year, month, day, hour; it records the weather at particular origin airport at particular hour of a day. Generally, each dataset should have a set of variables (also called key) to uniquely identify an observation. planes=fread(&quot;data/planes.csv&quot;) str(planes) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 3322 obs. of 9 variables: ## $ tailnum : chr &quot;N10156&quot; &quot;N102UW&quot; &quot;N103US&quot; &quot;N104UW&quot; ... ## $ year : int 2004 1998 1999 1999 2002 1999 1999 1999 1999 1999 ... ## $ type : chr &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; &quot;Fixed wing multi engine&quot; ... ## $ manufacturer: chr &quot;EMBRAER&quot; &quot;AIRBUS INDUSTRIE&quot; &quot;AIRBUS INDUSTRIE&quot; &quot;AIRBUS INDUSTRIE&quot; ... ## $ model : chr &quot;EMB-145XR&quot; &quot;A320-214&quot; &quot;A320-214&quot; &quot;A320-214&quot; ... ## $ engines : int 2 2 2 2 2 2 2 2 2 2 ... ## $ seats : int 55 182 182 182 55 182 182 182 182 182 ... ## $ speed : int NA NA NA NA NA NA NA NA NA NA ... ## $ engine : chr &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; &quot;Turbo-fan&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; The key of planes dataset is tailnum, the unique ID of a plane. 10.0.1 Join files using merge() The merge() function also exists in data.frame to merge data.frames. data.table also implement this function so that we can use the merge() function to join two data.table. Between we talk about the syntax of merge(), we need to understand the 4 different types of join. inner join full join left join right join Dont be overwhelmed! They are actually very similar. The general syntax of merge() is: merge(x = dt1, y = dt2, by.x = name, by.y = name, all.x=FALSE, all.y=FALSE) The 4 cases of all.x and all.y corresponds to 4 different types of join. Note that the default value of all.x and all.y are FALSE. 10.0.2 inner join We will start with an example. tmp=merge(flights, planes, by.x=&quot;tailnum&quot;, by.y=&quot;tailnum&quot;) head(tmp) ## tailnum year.x month day dep_time sched_dep_time dep_delay arr_time ## 1: N10156 2013 1 10 626 630 -4 802 ## 2: N10156 2013 1 10 1120 1032 48 1320 ## 3: N10156 2013 1 10 1619 1540 39 1831 ## 4: N10156 2013 1 11 632 634 -2 810 ## 5: N10156 2013 1 11 1116 1120 -4 1328 ## 6: N10156 2013 1 11 1845 1819 26 1959 ## sched_arr_time arr_delay carrier flight origin dest air_time distance hour ## 1: 800 2 EV 4560 EWR PIT 60 319 6 ## 2: 1240 40 EV 4269 EWR CHS 99 628 10 ## 3: 1744 47 EV 4667 EWR MSP 175 1008 15 ## 4: 822 -12 EV 4334 EWR CMH 81 463 6 ## 5: 1336 -8 EV 4298 EWR MCI 171 1092 11 ## 6: 1932 27 EV 4520 EWR PWM 49 284 18 ## minute time_hour year.y type manufacturer ## 1: 30 2013-01-10 06:00:00 2004 Fixed wing multi engine EMBRAER ## 2: 32 2013-01-10 10:00:00 2004 Fixed wing multi engine EMBRAER ## 3: 40 2013-01-10 15:00:00 2004 Fixed wing multi engine EMBRAER ## 4: 34 2013-01-11 06:00:00 2004 Fixed wing multi engine EMBRAER ## 5: 20 2013-01-11 11:00:00 2004 Fixed wing multi engine EMBRAER ## 6: 19 2013-01-11 18:00:00 2004 Fixed wing multi engine EMBRAER ## model engines seats speed engine ## 1: EMB-145XR 2 55 NA Turbo-fan ## 2: EMB-145XR 2 55 NA Turbo-fan ## 3: EMB-145XR 2 55 NA Turbo-fan ## 4: EMB-145XR 2 55 NA Turbo-fan ## 5: EMB-145XR 2 55 NA Turbo-fan ## 6: EMB-145XR 2 55 NA Turbo-fan This is the inner join: only observations with tailnum in both data.table are kept. The unmatched observation is not kept. You can check the dimension of the merged data.table. You will find that the number of observation in the merged data is smaller than both flights and planes. This is because unmatched rows will be removed. dim(tmp) ## [1] 284170 27 Since tailnum has the same column name in both data.table, we can simplify the code as tmp=merge(flights, planes, by=&quot;tailnum&quot;) If you examine tmp carefully, you will find the new variable year.x and year.y. This is because year appears in both flights and planes dataset; when merge these two datasets, we need to differentiate which dataset the year variable is from. Based on argument position, flight is at x and plane is at y, thus R will automatically rename the variables of same name to mark their sources. 10.0.3 full join Full join keeps all observations that are in either data.table. The code is exactly the same except letting all.x=TRUE and all.y=TRUE, meaning the merge will keep all observation from x and all observation from y. tmp=merge(flights, planes, by=&quot;tailnum&quot;, all.x = TRUE, all.y = TRUE) 10.0.4 left join/right join Left join adds information from the right data.table to the left data.table. E.g., we want to add the plane information to the flights dataset. tmp=merge(flights, planes, by=&quot;tailnum&quot;, all.x = TRUE) Since we want to add information to flights, therefore, we will keep all flights information; Thus, in the above code, we set all.x=TRUE. Right join is essentially the same with left join; we can always switch the position of x and y data.table to achieve the same goal. left/right join is the most common join in data analysis, because we constantly need to add information to the main dataset. 10.1 Merge dataset using data.table syntax The data.table syntax for joining is as below: DT2[DT1, on=.(name)] The code will add information from DT2 to DT1, and the variable to match DT1 and DT2 is name. The default join in data.table is right join. If the variable name are different in DT1 and DT2, we can modify the code as: DT2[DT1, on=c(name2=name1)] Lets look at an example: add the planes information to the flights dataset. tmp=planes[flights, on=.(tailnum)] dim(tmp) ## [1] 336776 27 head(tmp) ## tailnum year type manufacturer model engines seats ## 1: N14228 1999 Fixed wing multi engine BOEING 737-824 2 149 ## 2: N24211 1998 Fixed wing multi engine BOEING 737-824 2 149 ## 3: N619AA 1990 Fixed wing multi engine BOEING 757-223 2 178 ## 4: N804JB 2012 Fixed wing multi engine AIRBUS A320-232 2 200 ## 5: N668DN 1991 Fixed wing multi engine BOEING 757-232 2 178 ## 6: N39463 2012 Fixed wing multi engine BOEING 737-924ER 2 191 ## speed engine i.year month day dep_time sched_dep_time dep_delay arr_time ## 1: NA Turbo-fan 2013 1 1 517 515 2 830 ## 2: NA Turbo-fan 2013 1 1 533 529 4 850 ## 3: NA Turbo-fan 2013 1 1 542 540 2 923 ## 4: NA Turbo-fan 2013 1 1 544 545 -1 1004 ## 5: NA Turbo-fan 2013 1 1 554 600 -6 812 ## 6: NA Turbo-fan 2013 1 1 554 558 -4 740 ## sched_arr_time arr_delay carrier flight origin dest air_time distance hour ## 1: 819 11 UA 1545 EWR IAH 227 1400 5 ## 2: 830 20 UA 1714 LGA IAH 227 1416 5 ## 3: 850 33 AA 1141 JFK MIA 160 1089 5 ## 4: 1022 -18 B6 725 JFK BQN 183 1576 5 ## 5: 837 -25 DL 461 LGA ATL 116 762 6 ## 6: 728 12 UA 1696 EWR ORD 150 719 5 ## minute time_hour ## 1: 15 2013-01-01 05:00:00 ## 2: 29 2013-01-01 05:00:00 ## 3: 40 2013-01-01 05:00:00 ## 4: 45 2013-01-01 05:00:00 ## 5: 0 2013-01-01 06:00:00 ## 6: 58 2013-01-01 05:00:00 The dimension of the merged dataset indicates that all rows of flights dataset are kept. This is because we are adding plane information to the flights dataset. Be really careful about the where to put the main dataset (should be inside the [] because the default is right join). Examing the merged dataset, you will find a new variable i.year, which indicates the year from flights dataset (because flights is in the i argument position). For inner join, i.e., removing unmatched rows, we can set nomatch=0, as shown below: tmp=planes[flights, on=.(tailnum), nomatch=0] dim(tmp) ## [1] 284170 27 Full join is not possible with the data.table syntax, use the merge() function instead. You must be wondering why we need another way of joining data.table. This is because, with the data.table syntax, we can chain the inquiry like this DT[][][]. 10.2 Chaining Join and Data Manipulation data.table expressions can be chained in sequence: DT[][][]. This enables us to join multiple data.table and then manipulate the merged data.table for insights. E.g., we want to compute the market share of airplane manufacturers in terms of the number flights using the airplanes made by the manufacturers. We need to first include the manufacturer information to the flights dataset and then compute the count statistics. planes[flights, on=.(tailnum)][,.(count=.N), by=manufacturer] ## manufacturer count ## 1: BOEING 82912 ## 2: AIRBUS 47302 ## 3: AIRBUS INDUSTRIE 40891 ## 4: CANADAIR 1594 ## 5: &lt;NA&gt; 52606 ## 6: MCDONNELL DOUGLAS AIRCRAFT CO 8932 ## 7: EMBRAER 66068 ## 8: MCDONNELL DOUGLAS CORPORATION 1259 ## 9: BOMBARDIER INC 28272 ## 10: MCDONNELL DOUGLAS 3998 ## 11: GULFSTREAM AEROSPACE 499 ## 12: CESSNA 658 ## 13: HURLEY JAMES LARRY 17 ## 14: CIRRUS DESIGN CORP 291 ## 15: PIPER 162 ## 16: FRIEDEMANN JON 63 ## 17: ROBINSON HELICOPTER CO 286 ## 18: PAIR MIKE E 25 ## 19: BARKER JACK L 252 ## 20: LAMBERT RICHARD 54 ## 21: MARZ BARRY 44 ## 22: CANADAIR LTD 103 ## 23: BEECH 47 ## 24: AMERICAN AIRCRAFT INC 42 ## 25: LEBLANC GLENN T 40 ## 26: STEWART MACO 55 ## 27: DEHAVILLAND 63 ## 28: AVIAT AIRCRAFT INC 18 ## 29: AGUSTA SPA 32 ## 30: KILDALL GARY 51 ## 31: BELL 65 ## 32: LEARJET INC 19 ## 33: DOUGLAS 22 ## 34: SIKORSKY 27 ## 35: AVIONS MARCEL DASSAULT 4 ## 36: JOHN G HESS 3 ## manufacturer count E.g., we want to know the average seat avialable between each origin,dest pair, which shows how well the two nodes are connected. The seats information is in the planes dataset, thus we need to first add planes to the flights dataset. tmp=planes[flights, on=.(tailnum)][,.(avg_seat=mean(seats,na.rm = TRUE)), by=.(origin, dest)] head(tmp) ## origin dest avg_seat ## 1: EWR IAH 190.9485 ## 2: LGA IAH 170.4513 ## 3: JFK MIA 180.9665 ## 4: JFK BQN 196.6559 ## 5: LGA ATL 148.1784 ## 6: EWR ORD 176.6021 10.3 Exercise Suppose you are developing a system that can check the weather condition at each flight. One important task is to add the weather information to the flights dataset. Note that the key of the weather dataset is: year, month, day, hour, origin; in other words, this set of variable uniquely identify an observation in weather dataset. You can do that using merge() function: tmp=merge(flights, weather, by=c(&quot;year&quot;,&quot;month&quot;,&quot;day&quot;,&quot;hour&quot;,&quot;origin&quot;), all.x = TRUE) head(tmp) ## year month day hour origin dep_time sched_dep_time dep_delay arr_time ## 1: 2013 1 1 5 EWR 517 515 2 830 ## 2: 2013 1 1 5 EWR 554 558 -4 740 ## 3: 2013 1 1 5 JFK 542 540 2 923 ## 4: 2013 1 1 5 JFK 544 545 -1 1004 ## 5: 2013 1 1 5 JFK 559 559 0 702 ## 6: 2013 1 1 5 LGA 533 529 4 850 ## sched_arr_time arr_delay carrier flight tailnum dest air_time distance ## 1: 819 11 UA 1545 N14228 IAH 227 1400 ## 2: 728 12 UA 1696 N39463 ORD 150 719 ## 3: 850 33 AA 1141 N619AA MIA 160 1089 ## 4: 1022 -18 B6 725 N804JB BQN 183 1576 ## 5: 706 -4 B6 1806 N708JB BOS 44 187 ## 6: 830 20 UA 1714 N24211 IAH 227 1416 ## minute time_hour.x temp dewp humid wind_dir wind_speed wind_gust ## 1: 15 2013-01-01 05:00:00 39.02 28.04 64.43 260 12.65858 NA ## 2: 58 2013-01-01 05:00:00 39.02 28.04 64.43 260 12.65858 NA ## 3: 40 2013-01-01 05:00:00 39.02 26.96 61.63 260 14.96014 NA ## 4: 45 2013-01-01 05:00:00 39.02 26.96 61.63 260 14.96014 NA ## 5: 59 2013-01-01 05:00:00 39.02 26.96 61.63 260 14.96014 NA ## 6: 29 2013-01-01 05:00:00 39.92 24.98 54.81 250 14.96014 21.86482 ## precip pressure visib time_hour.y ## 1: 0 1011.9 10 2013-01-01 05:00:00 ## 2: 0 1011.9 10 2013-01-01 05:00:00 ## 3: 0 1012.1 10 2013-01-01 05:00:00 ## 4: 0 1012.1 10 2013-01-01 05:00:00 ## 5: 0 1012.1 10 2013-01-01 05:00:00 ## 6: 0 1011.4 10 2013-01-01 05:00:00 We can plot the dep_delay against the weather condition (e.g., humid) at the origin airport to see whether there is a relationship between these two # load the ggplot2 package into R library(ggplot2) # scatter plot with humid on x, dep_delay on y ggplot(tmp, aes(humid,dep_delay, col=origin))+ geom_point(position = &quot;jitter&quot;, alpha=0.3) ## Warning: Removed 9800 rows containing missing values (geom_point). What is your observation of the chart? 1) it seems to suggest a higher humidity is associated with a longer departure delay; 2) however, there also exist many departure delay in a non-humid weather, which will be attributed to other factors. Or you can do that using the data.table syntax: # note that the main data.table is inside [] tmp=weather[flights, on=.(year,month, day, hour, origin)] dim(tmp) ## [1] 336776 29 Now, suppose a customer is inquiring the scheduled departure time and the weather condition for flights UA1714 for the date 2013-01-01. You will inquire the flight and return the temp, wind_speed, humid, and visib. Use the data.table syntax to chain the inquiry. weather[flights, on=.(year,month, day, hour, origin)][ carrier==&quot;UA&quot; &amp; flight==1714 &amp; month==1 &amp; day==1,.(year,month, day, hour, origin, carrier, flight, temp, wind_speed, humid, visib)] ## year month day hour origin carrier flight temp wind_speed humid visib ## 1: 2013 1 1 5 LGA UA 1714 39.92 14.96014 54.81 10 10.4 Summary We reviewed the 4 types of join: inner join, full join, left/right join. The most commonly used is th left/right join because we need to add information from one dataset to the main dataset We learn the merge() to join datasets: the syntax for adding dt2 to dt1 is: merge(dt1, dt2, by.x=name1, by.y=name2, all.x=TRUE) We learn the data.table syntax for joining data.table: dt2[dt1, on=.(name)]. We learn to chain the join and computation with data.table syntax DT[][][] So far, we have learned how to manipulate big data with data.table package. What we have learned so far should cover you for most of the common data manipulation in a real job. I hope you are amazed and empowered by the data.table packages. "],["put-it-together-explore-big-data-with-data.html", "Chapter 11 Put it together: explore big data with data.table 11.1 Import the COVID-19 case data 11.2 Selecting Rows and Columns 11.3 Selecting Rows and Columns 11.4 Computing on j 11.5 Computing on j 11.6 Computing on j 11.7 Computing on j 11.8 Special symbol .N 11.9 Fitering rows through %between% 11.10 Fitering rows through %chin% 11.11 uniqueN() 11.12 Subset of Data: .SD[ ] 11.13 Subset of Data: .SD[ ] 11.14 Use := to Add/Update Columns By Reference 11.15 Join datasets 11.16 Additional Exercises: 11.17 Summary", " Chapter 11 Put it together: explore big data with data.table The general form of data.table syntax is DT[i, j, by] where DT is a data.table. by: grouped by what? j: what to do? i: on which rows? In general, we can tack expressions one after another, forming a chain of operations, i.e., DT[  ][  ][  ]. The learning objectives is to Practice the data.table syntax to explore datasets on COVID-19: Two datasets will be used: COVID-19 new/cumulative cases on county level population of US counties in 2019 library(data.table) library(curl) ## Warning: package &#39;curl&#39; was built under R version 4.0.5 ## Using libcurl 7.64.1 with Schannel library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 4.0.5 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(magrittr) ## Warning: package &#39;magrittr&#39; was built under R version 4.0.5 library(ggplot2) 11.1 Import the COVID-19 case data The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak. The data is published on Github. We can download the csv data directly using fread(). # the data record the total confirmed cases and deaths for each county at each date. covid_county=fread(&quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&quot;) head(covid_county) ## date county state fips cases deaths ## 1: 2020-01-21 Snohomish Washington 53061 1 0 ## 2: 2020-01-22 Snohomish Washington 53061 1 0 ## 3: 2020-01-23 Snohomish Washington 53061 1 0 ## 4: 2020-01-24 Cook Illinois 17031 1 0 ## 5: 2020-01-24 Snohomish Washington 53061 1 0 ## 6: 2020-01-25 Orange California 6059 1 0 str(covid_county) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 1693157 obs. of 6 variables: ## $ date : IDate, format: &quot;2020-01-21&quot; &quot;2020-01-22&quot; ... ## $ county: chr &quot;Snohomish&quot; &quot;Snohomish&quot; &quot;Snohomish&quot; &quot;Cook&quot; ... ## $ state : chr &quot;Washington&quot; &quot;Washington&quot; &quot;Washington&quot; &quot;Illinois&quot; ... ## $ fips : int 53061 53061 53061 17031 53061 6059 17031 53061 4013 6037 ... ## $ cases : int 1 1 1 1 1 1 1 1 1 1 ... ## $ deaths: int 0 0 0 0 0 0 0 0 0 0 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; # add a column to define abbr for each state covid_county[,state_abbr:=state.abb[match(state,state.name)]] # Set order of column setcolorder(covid_county, c(&quot;date&quot;, &quot;state&quot;, &quot;state_abbr&quot;,&quot;county&quot;,&quot;fips&quot;,&quot;cases&quot;,&quot;deaths&quot;)) # change state name to lower case for consistency covid_county[,state:=tolower(state)] # illustrate the total cases and total death covid_county[county==&quot;Lancaster&quot; &amp; state_abbr==&quot;NE&quot;, .(date,cases)] %&gt;% ggplot()+ geom_line(aes(date,cases)) covid_county[county==&quot;Lancaster&quot; &amp; state_abbr==&quot;NE&quot;, .(date,deaths)] %&gt;% ggplot()+ geom_line(aes(date,deaths)) # compute the daily new cases and new deaths covid_county=covid_county[order(state,county,date)] covid_county[ , confirmed_count := cases - shift(cases), by = .(state,county)] covid_county[ , death_count := deaths - shift(deaths), by = .(state,county)] covid_county[is.na(confirmed_count),confirmed_count:=cases][is.na(death_count),death_count:=deaths] # save the data to local computer fwrite(covid_county,&quot;data/covid_county.csv&quot;) 11.2 Selecting Rows and Columns Q1: What is the new confirmed cases in the Lancaster county, NE, on 2020-07-16? covid_county[county==&quot;Lancaster&quot; &amp; state_abbr==&quot;NE&quot; &amp; date==&quot;2020-07-16&quot;, .(confirmed_count)] ## confirmed_count ## 1: 50 11.3 Selecting Rows and Columns Q2: What is the new confirmed cases and death in the Lancaster county, NE, on 2020-07-16? covid_county[county==&quot;Lancaster&quot; &amp; state_abbr==&quot;NE&quot; &amp; date==&quot;2020-07-16&quot;, .(confirmed_count, death_count)] ## confirmed_count death_count ## 1: 50 0 11.4 Computing on j Q3: What is the new confirmed cases in US on 2020-07-16? covid_county[date==&quot;2020-07-16&quot;, .(confirmed_count=sum(confirmed_count))] ## confirmed_count ## 1: 75701 11.5 Computing on j Q4: What is the new confirmed cases for each state on 2020-07-16? Order the result in the descending order of cases count. tmp=covid_county[date==&quot;2020-07-16&quot;, .(confirmed_count=sum(confirmed_count)), by=.(state_abbr)][order(-confirmed_count)] head(tmp) ## state_abbr confirmed_count ## 1: TX 15038 ## 2: FL 13965 ## 3: CA 9264 ## 4: AZ 3319 ## 5: GA 2701 ## 6: TN 2312 11.6 Computing on j Q5: What is the new confirmed cases and death for each state, on 2020-07-16? Order the result in the descending order of cases count. tmp=covid_county[date==&quot;2020-07-16&quot;, .(confirmed_count=sum(confirmed_count), death_count=sum(death_count)), by=.(state_abbr)][order(-confirmed_count)] head(tmp) ## state_abbr confirmed_count death_count ## 1: TX 15038 154 ## 2: FL 13965 156 ## 3: CA 9264 122 ## 4: AZ 3319 64 ## 5: GA 2701 13 ## 6: TN 2312 14 11.7 Computing on j Q6: What is the new confirmed cases each day for the state of Nebraska? tmp=covid_county[state_abbr==&quot;NE&quot;, .(confirmed_count=sum(confirmed_count)), by=.(state_abbr,date)] head(tmp) ## state_abbr date confirmed_count ## 1: NE 2020-03-18 7 ## 2: NE 2020-03-19 4 ## 3: NE 2020-03-20 6 ## 4: NE 2020-03-21 8 ## 5: NE 2020-03-22 1 ## 6: NE 2020-03-23 11 11.8 Special symbol .N Q7: How many counties has new confirmed cases that is above 5000? covid_county[confirmed_count&gt;5000, .N] ## [1] 270 Q8: What are these counties which has new confirmed cases that is above 5000 in a single day? covid_county[confirmed_count&gt;5000, .(state,county, date, confirmed_count)] %&gt;% head() ## state county date confirmed_count ## 1: arizona Maricopa 2020-12-01 8330 ## 2: arizona Maricopa 2020-12-08 9701 ## 3: arizona Maricopa 2020-12-14 6975 ## 4: arizona Maricopa 2020-12-22 6088 ## 5: arizona Maricopa 2020-12-28 6878 ## 6: arizona Maricopa 2020-12-31 5181 11.9 Fitering rows through %between% Q9: How many county has daily new confirmed case between 1000 and 5000 for each day? tmp=covid_county[confirmed_count%between%c(1000,5000), .(count=.N), by=.(date)] tail(tmp) ## date count ## 1: 2021-06-03 1 ## 2: 2020-10-03 3 ## 3: 2020-08-30 1 ## 4: 2020-09-14 1 ## 5: 2020-09-15 1 ## 6: 2020-09-29 1 11.10 Fitering rows through %chin% Q10: How many new confirmed cases in each day for the top10 states together? # find the top10 states in terms of total cases covid_county[date==&quot;2021-08-27&quot;,.(cases=sum(cases)), by=c(&quot;state&quot;,&quot;state_abbr&quot;)][order(-cases)][1:10] ## state state_abbr cases ## 1: california CA 4396238 ## 2: texas TX 3548371 ## 3: florida FL 3179714 ## 4: new york NY 2258143 ## 5: illinois IL 1512387 ## 6: georgia GA 1339537 ## 7: pennsylvania PA 1288041 ## 8: ohio OH 1202728 ## 9: north carolina NC 1191456 ## 10: new jersey NJ 1084546 top10_states=covid_county[date==&quot;2021-08-27&quot;,.(cases=sum(cases)), by=c(&quot;state&quot;,&quot;state_abbr&quot;)][order(-cases)][1:10]$state_abbr top10_states=c(&quot;NY&quot;, &quot;CA&quot;, &quot;FL&quot;, &quot;TX&quot;, &quot;NJ&quot;, &quot;IL&quot;, &quot;MA&quot;, &quot;AZ&quot;, &quot;GA&quot;, &quot;PA&quot;) # find how many confirmed cases each date for the top 10 state tmp=covid_county[state_abbr%chin%top10_states, .(confirmed=sum(cases)), by=.(date)] tail(tmp) ## date confirmed ## 1: 2020-03-10 572 ## 2: 2020-03-11 677 ## 3: 2020-03-12 896 ## 4: 2020-03-13 1179 ## 5: 2020-01-25 2 ## 6: 2020-01-24 1 11.11 uniqueN() Q11: How many counties has COVID-19 cases for each state? tmp=covid_county[confirmed_count&gt;0, .(county_cnt=uniqueN(county)), by=.(state_abbr)] head(tmp) ## state_abbr county_cnt ## 1: AL 67 ## 2: AK 28 ## 3: AZ 16 ## 4: AR 76 ## 5: CA 59 ## 6: CO 65 11.12 Subset of Data: .SD[ ] Q12: for each day, find the top 3 counties in terms of the new confirmed cases. tmp=covid_county[order(date,-confirmed_count)][ , .SD[1:3], by=.(date), .SDcols=c(&quot;state_abbr&quot;,&quot;county&quot;, &quot;confirmed_count&quot;)] tail(tmp) ## date state_abbr county confirmed_count ## 1: 2021-09-05 NY New York City 3632 ## 2: 2021-09-05 AZ Maricopa 2352 ## 3: 2021-09-05 CA Los Angeles 2111 ## 4: 2021-09-06 SC Greenville 1780 ## 5: 2021-09-06 AZ Maricopa 1636 ## 6: 2021-09-06 CA Los Angeles 1530 11.13 Subset of Data: .SD[ ] Q13: find the top 3 counties for each state in terms of the total confirmed cases. tmp=covid_county[,.(confirmed=sum(confirmed_count)), by=.(county,state_abbr)][ order(state_abbr,-confirmed)][, .SD[1:3], by=.(state_abbr)] head(tmp) ## state_abbr county confirmed ## 1: AK Anchorage 38593 ## 2: AK Matanuska-Susitna Borough 13904 ## 3: AK Fairbanks North Star Borough 9298 ## 4: AL Jefferson 104567 ## 5: AL Mobile 65549 ## 6: AL Madison 45548 Q13_extension: find the bottom 3 counties for each state in terms of the total confirmed cases. 11.14 Use := to Add/Update Columns By Reference Q14: Define a new variable death_rate, which equals to cumulative death divided by cumulative confirmed cases for each state tmp=covid_county[, deat_rate:=deaths/cases][ order(-deat_rate)] head(tmp) ## date state state_abbr county fips cases deaths confirmed_count ## 1: 2020-03-24 arizona AZ Unknown NA 0 3 0 ## 2: 2020-03-25 arizona AZ Unknown NA 0 3 0 ## 3: 2020-03-26 arizona AZ Unknown NA 0 2 0 ## 4: 2020-03-27 arizona AZ Unknown NA 0 4 0 ## 5: 2020-03-28 arizona AZ Unknown NA 0 3 0 ## 6: 2020-03-29 arizona AZ Unknown NA 0 5 0 ## death_count deat_rate ## 1: 3 Inf ## 2: 0 Inf ## 3: -1 Inf ## 4: 2 Inf ## 5: -1 Inf ## 6: 2 Inf 11.15 Join datasets Q15: Compute the infection_rate (percent of population infected at each county). We need to first compute the total cases for each county and add population information to it for computing the infection rate. population2019=fread(&quot;data/population2019.csv&quot;) tmp=covid_county[, .(cum_case=sum(confirmed_count)), by=.(state_abbr, state, county)] tmp1=merge(tmp, population2019, by.x=c(&quot;state&quot;,&quot;county&quot;), by.y = c(&quot;state_name&quot;,&quot;county_name&quot;), all.x = TRUE) tmp2=tmp1[,infection_rate:=cum_case/population19][,-c(&quot;state&quot;)][ order(-infection_rate)] head(tmp2) ## county state_abbr cum_case population19 infection_rate ## 1: Chattahoochee GA 5109 10907 0.4684148 ## 2: Crowley CO 2236 6061 0.3689160 ## 3: Dimmit TX 3567 10124 0.3523311 ## 4: Bent CO 1566 5577 0.2807961 ## 5: Lincoln AR 3572 13024 0.2742629 ## 6: Dewey SD 1591 5892 0.2700272 11.16 Additional Exercises: Q16: what is the total death in each state? Q17: Which county in Nebraska has the highest total confirmed cases? 11.17 Summary Congratulation! You have mastered the skills to manipulate big data for insights using data.table. It is very likely you will forget the codes for manipulating data. Do not try to memorize the code; memorize the scenarios in which the code is used; develop a code library for these scenarios. We will later learn to combine data manipulation and visualization. "],["data-cleaning-i.html", "Chapter 12 Data Cleaning I 12.1 Exploring the raw data 12.2 Tidying data 12.3 Preparing data for analysis 12.4 Missing and special values in R 12.5 Outliers, obvious error and missing value 12.6 Summary", " Chapter 12 Data Cleaning I Guess what, data scientists spend 80% of their time cleaning data! When you are in a real job, the data you get to work with can be very messy. However, we do not typically teach how to clean data in class but provide students with clean data that is ready for analysis. I am compelled to teach you how to clean data, because, again, 80% of the time, we will be cleaning data. You will find the skills of cleaning data will be of great help later in your work. Thus, data cleaning, although may be tedious, will pay off in the near future. This chapter provides a very basic introduction to cleaning data in R using the data.table, lubridate, and stringr packages. After taking the course youll be able to go from raw data to awesome insights as quickly and painlessly as possible! There are two main characteristics of a clean data: 1). each row represents an observation and each column represents a variable/attribute associated with the observation. 2). The columns are in the right variable mode (e.g., numbers are numeric, date are date not character, categorical variables are factors). Although this sounds really intuitive and simple, most raw datasets are not like that, potentially due to the way the data are collected. There are three basic steps to clean a messy data: exploring raw data to diagnose the places to be cleaned tidying data to make rows as observations, columns as variables. preparing data for analysis (converting columns into the right variable type) We will go over each of these steps in this chapter. Run the code to install the packages (only need to run once): install.packages(&quot;lubridate&quot;) # dealing date variable install.packages(&quot;stringr&quot;) # dealing strings/character install.packages(&quot;tidyr&quot;) # tidying data install.packages(&quot;data.table&quot;) Run the code to load the packages into R: library(lubridate) library(stringr) library(tidyr) ## Warning: package &#39;tidyr&#39; was built under R version 4.0.5 ## ## Attaching package: &#39;tidyr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## extract library(data.table) For the purpose of illustration, we will use a messy, real-world dataset containing an entire years worth of weather data from Boston (i.e., weather_boston.csv). Among other things, youll be presented with variables that contain column names, column names that should be values, numbers coded as character strings, and values that are missing, extreme, and downright erroneous! First thing first, lets read the weather_boston.csv data into R. # use fread() to read csv into R as data.table weather_boston&lt;-fread(&quot;data/weather_boston.csv&quot;) 12.1 Exploring the raw data It is critical to explore the raw data, understand its structure and diagnose why the data is messy. We can explore the raw data through the following means. understand the structure of the data look at the data visualize the data 12.1.1 Understand the structure of the data class(weather_boston) # view its class ## [1] &quot;data.table&quot; &quot;data.frame&quot; dim(weather_boston) # view its dimensions ## [1] 286 34 names(weather_boston) # check the column names (variable names) ## [1] &quot;year&quot; &quot;month&quot; &quot;measure&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; ## [8] &quot;X5&quot; &quot;X6&quot; &quot;X7&quot; &quot;X8&quot; &quot;X9&quot; &quot;X10&quot; &quot;X11&quot; ## [15] &quot;X12&quot; &quot;X13&quot; &quot;X14&quot; &quot;X15&quot; &quot;X16&quot; &quot;X17&quot; &quot;X18&quot; ## [22] &quot;X19&quot; &quot;X20&quot; &quot;X21&quot; &quot;X22&quot; &quot;X23&quot; &quot;X24&quot; &quot;X25&quot; ## [29] &quot;X26&quot; &quot;X27&quot; &quot;X28&quot; &quot;X29&quot; &quot;X30&quot; &quot;X31&quot; str(weather_boston) # examine the structure of the data ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 286 obs. of 34 variables: ## $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... ## $ month : int 12 12 12 12 12 12 12 12 12 12 ... ## $ measure: chr &quot;Max.TemperatureF&quot; &quot;Mean.TemperatureF&quot; &quot;Min.TemperatureF&quot; &quot;Max.Dew.PointF&quot; ... ## $ X1 : chr &quot;64&quot; &quot;52&quot; &quot;39&quot; &quot;46&quot; ... ## $ X2 : chr &quot;42&quot; &quot;38&quot; &quot;33&quot; &quot;40&quot; ... ## $ X3 : chr &quot;51&quot; &quot;44&quot; &quot;37&quot; &quot;49&quot; ... ## $ X4 : chr &quot;43&quot; &quot;37&quot; &quot;30&quot; &quot;24&quot; ... ## $ X5 : chr &quot;42&quot; &quot;34&quot; &quot;26&quot; &quot;37&quot; ... ## $ X6 : chr &quot;45&quot; &quot;42&quot; &quot;38&quot; &quot;45&quot; ... ## $ X7 : chr &quot;38&quot; &quot;30&quot; &quot;21&quot; &quot;36&quot; ... ## $ X8 : chr &quot;29&quot; &quot;24&quot; &quot;18&quot; &quot;28&quot; ... ## $ X9 : chr &quot;49&quot; &quot;39&quot; &quot;29&quot; &quot;49&quot; ... ## $ X10 : chr &quot;48&quot; &quot;43&quot; &quot;38&quot; &quot;45&quot; ... ## $ X11 : chr &quot;39&quot; &quot;36&quot; &quot;32&quot; &quot;37&quot; ... ## $ X12 : chr &quot;39&quot; &quot;35&quot; &quot;31&quot; &quot;28&quot; ... ## $ X13 : chr &quot;42&quot; &quot;37&quot; &quot;32&quot; &quot;28&quot; ... ## $ X14 : chr &quot;45&quot; &quot;39&quot; &quot;33&quot; &quot;29&quot; ... ## $ X15 : chr &quot;42&quot; &quot;37&quot; &quot;32&quot; &quot;33&quot; ... ## $ X16 : chr &quot;44&quot; &quot;40&quot; &quot;35&quot; &quot;42&quot; ... ## $ X17 : chr &quot;49&quot; &quot;45&quot; &quot;41&quot; &quot;46&quot; ... ## $ X18 : chr &quot;44&quot; &quot;40&quot; &quot;36&quot; &quot;34&quot; ... ## $ X19 : chr &quot;37&quot; &quot;33&quot; &quot;29&quot; &quot;25&quot; ... ## $ X20 : chr &quot;36&quot; &quot;32&quot; &quot;27&quot; &quot;30&quot; ... ## $ X21 : chr &quot;36&quot; &quot;33&quot; &quot;30&quot; &quot;30&quot; ... ## $ X22 : chr &quot;44&quot; &quot;39&quot; &quot;33&quot; &quot;39&quot; ... ## $ X23 : chr &quot;47&quot; &quot;45&quot; &quot;42&quot; &quot;45&quot; ... ## $ X24 : chr &quot;46&quot; &quot;44&quot; &quot;41&quot; &quot;46&quot; ... ## $ X25 : chr &quot;59&quot; &quot;52&quot; &quot;44&quot; &quot;58&quot; ... ## $ X26 : chr &quot;50&quot; &quot;44&quot; &quot;37&quot; &quot;31&quot; ... ## $ X27 : chr &quot;52&quot; &quot;45&quot; &quot;38&quot; &quot;34&quot; ... ## $ X28 : chr &quot;52&quot; &quot;46&quot; &quot;40&quot; &quot;42&quot; ... ## $ X29 : chr &quot;41&quot; &quot;36&quot; &quot;30&quot; &quot;26&quot; ... ## $ X30 : chr &quot;30&quot; &quot;26&quot; &quot;22&quot; &quot;10&quot; ... ## $ X31 : chr &quot;30&quot; &quot;25&quot; &quot;20&quot; &quot;8&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; summary(weather_boston) # examine potential outliers and missing values ## year month measure X1 ## Min. :2014 Min. : 1.000 Length:286 Length:286 ## 1st Qu.:2015 1st Qu.: 4.000 Class :character Class :character ## Median :2015 Median : 7.000 Mode :character Mode :character ## Mean :2015 Mean : 6.923 ## 3rd Qu.:2015 3rd Qu.:10.000 ## Max. :2015 Max. :12.000 ## X2 X3 X4 X5 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X6 X7 X8 X9 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X10 X11 X12 X13 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X14 X15 X16 X17 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X18 X19 X20 X21 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X22 X23 X24 X25 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X26 X27 X28 X29 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X30 X31 ## Length:286 Length:286 ## Class :character Class :character ## Mode :character Mode :character ## ## ## 12.1.2 Visulize the data in Tabular form There is no substitute to actually see the data. First, lets see the data in a tabular form. head(weather_boston) # view the first 6 rows of the data ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 ## 1: 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 45 ## 2: 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 39 ## 3: 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 33 ## 4: 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 29 ## 5: 2014 12 MeanDew.PointF 40 27 42 21 25 40 20 16 41 39 31 27 26 27 ## 6: 2014 12 Min.DewpointF 26 17 24 13 12 36 -3 3 28 37 27 25 24 25 ## X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: 42 44 49 44 37 36 36 44 47 46 59 50 52 52 41 30 30 ## 2: 37 40 45 40 33 32 33 39 45 44 52 44 45 46 36 26 25 ## 3: 32 35 41 36 29 27 30 33 42 41 44 37 38 40 30 22 20 ## 4: 33 42 46 34 25 30 30 39 45 46 58 31 34 42 26 10 8 ## 5: 29 36 41 30 22 24 27 34 42 44 43 29 31 35 20 4 5 ## 6: 27 30 32 26 20 20 25 25 37 41 29 28 29 27 10 -6 1 head(weather_boston, 10) # view the first 10 rows of the data ## year month measure X1 X2 X3 X4 X5 X6 ## 1: 2014 12 Max.TemperatureF 64 42 51 43 42 45 ## 2: 2014 12 Mean.TemperatureF 52 38 44 37 34 42 ## 3: 2014 12 Min.TemperatureF 39 33 37 30 26 38 ## 4: 2014 12 Max.Dew.PointF 46 40 49 24 37 45 ## 5: 2014 12 MeanDew.PointF 40 27 42 21 25 40 ## 6: 2014 12 Min.DewpointF 26 17 24 13 12 36 ## 7: 2014 12 Max.Humidity 74 92 100 69 85 100 ## 8: 2014 12 Mean.Humidity 63 72 79 54 66 93 ## 9: 2014 12 Min.Humidity 52 51 57 39 47 85 ## 10: 2014 12 Max.Sea.Level.PressureIn 30.45 30.71 30.4 30.56 30.68 30.42 ## X7 X8 X9 X10 X11 X12 X13 X14 X15 X16 X17 X18 ## 1: 38 29 49 48 39 39 42 45 42 44 49 44 ## 2: 30 24 39 43 36 35 37 39 37 40 45 40 ## 3: 21 18 29 38 32 31 32 33 32 35 41 36 ## 4: 36 28 49 45 37 28 28 29 33 42 46 34 ## 5: 20 16 41 39 31 27 26 27 29 36 41 30 ## 6: -3 3 28 37 27 25 24 25 27 30 32 26 ## 7: 92 92 100 100 92 85 75 82 89 96 100 89 ## 8: 61 70 93 95 87 75 65 68 75 85 85 73 ## 9: 29 47 86 89 82 64 55 53 60 73 70 57 ## 10: 30.69 30.77 30.51 29.58 29.81 29.88 29.86 29.91 30.15 30.17 29.91 29.87 ## X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 ## 1: 37 36 36 44 47 46 59 50 52 52 41 30 ## 2: 33 32 33 39 45 44 52 44 45 46 36 26 ## 3: 29 27 30 33 42 41 44 37 38 40 30 22 ## 4: 25 30 30 39 45 46 58 31 34 42 26 10 ## 5: 22 24 27 34 42 44 43 29 31 35 20 4 ## 6: 20 20 25 25 37 41 29 28 29 27 10 -6 ## 7: 69 89 85 89 100 100 100 70 70 76 64 50 ## 8: 63 79 77 79 91 98 75 60 60 65 51 38 ## 9: 56 69 69 69 82 96 49 49 50 53 37 26 ## 10: 30.15 30.31 30.37 30.4 30.31 30.13 29.96 30.16 30.22 29.99 30.22 30.36 ## X31 ## 1: 30 ## 2: 25 ## 3: 20 ## 4: 8 ## 5: 5 ## 6: 1 ## 7: 57 ## 8: 44 ## 9: 31 ## 10: 30.32 tail(weather_boston) # view the last 6 rows of the data ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 ## 1: 2015 12 Mean.Wind.SpeedMPH 6 ## 2: 2015 12 Max.Gust.SpeedMPH 17 ## 3: 2015 12 PrecipitationIn 0.14 ## 4: 2015 12 CloudCover 7 ## 5: 2015 12 Events Rain ## 6: 2015 12 WindDirDegrees 109 ## X14 X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: tail(weather_boston, 10) # view the last 10 rows of the data ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 ## 1: 2015 12 Max.VisibilityMiles 10 ## 2: 2015 12 Mean.VisibilityMiles 8 ## 3: 2015 12 Min.VisibilityMiles 1 ## 4: 2015 12 Max.Wind.SpeedMPH 15 ## 5: 2015 12 Mean.Wind.SpeedMPH 6 ## 6: 2015 12 Max.Gust.SpeedMPH 17 ## 7: 2015 12 PrecipitationIn 0.14 ## 8: 2015 12 CloudCover 7 ## 9: 2015 12 Events Rain ## 10: 2015 12 WindDirDegrees 109 ## X13 X14 X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## 8: ## 9: ## 10: Among other things, we see that the weather_boston dataset suffers from one of the most common symptoms of messy data: column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. As a result, each row is not an individual observation, but a combined observation of 31 days. Also, variables are not orginaized by columns. The measure column indicates that the multiple measure are stacked by rows. We will tidy this data into the correct form later. 12.1.3 Visualize the raw data through charts Histgoram is a great tool to see the range and distribution of the data. It provides a great way to identify potential outliers in your data. hist(as.numeric(weather_boston$X1)) ## Warning in hist(as.numeric(weather_boston$X1)): NAs introduced by coercion We can also look at the scatter plot between two variable to check their relationship. plot(as.numeric(weather_boston$X1), as.numeric(weather_boston$X2)) ## Warning in plot(as.numeric(weather_boston$X1), as.numeric(weather_boston$X2)): ## NAs introduced by coercion ## Warning in xy.coords(x, y, xlabel, ylabel, log): NAs introduced by coercion We can also look at the boxplot boxplot(as.numeric(weather_boston$X1)) ## Warning in boxplot(as.numeric(weather_boston$X1)): NAs introduced by coercion 12.2 Tidying data What is tidy data? For any statistical analysis or visualization, the data needs to be in the right form: each row represents an observation and each column represents a variable/attribute associated with the observation. Data analyst spend huge amount of time to get data ready in this format. Nevertheless, the tidy data concept is proposed and formalized by Hadley Wickham in the Journal of Statistical Software recently. Here is the link to the paper: https://vita.had.co.nz/papers/tidy-data.pdf Here is an example of tidy data. country&lt;-rep(c(&quot;United Stat&quot;, &quot;Japan&quot;, &quot;China&quot;), times=3) abbr&lt;-rep(c(&quot;US&quot;, &quot;JP&quot;, &quot;CH&quot;), times=3) year&lt;-rep(c(2017:2019),each=3) GDP&lt;-c(400,200,290,420,210,310,450,230,320) unemployrate&lt;-c(&quot;4%&quot;,&quot;3%&quot;,&quot;2%&quot;,&quot;4.1%&quot;,&quot;3.3%&quot;,&quot;2.5%&quot;,&quot;3.9%&quot;,&quot;3.2%&quot;,&quot;2.9%&quot;) GDP_tidy&lt;-data.table(country,abbr,year,GDP,unemployrate) GDP_tidy ## country abbr year GDP unemployrate ## 1: United Stat US 2017 400 4% ## 2: Japan JP 2017 200 3% ## 3: China CH 2017 290 2% ## 4: United Stat US 2018 420 4.1% ## 5: Japan JP 2018 210 3.3% ## 6: China CH 2018 310 2.5% ## 7: United Stat US 2019 450 3.9% ## 8: Japan JP 2019 230 3.2% ## 9: China CH 2019 320 2.9% The GDP_tidy is an example is a simple tidy data. By name, we know this data is about GDP of country. Each row represents an observation: we observe the GDP and unemployment of a particular country in a particular year. Thus, each column represents one attribute of the observation: which country is observed; in which year it is observed, and what is the observed GDP amount. Here is an example of non-tidy data. country&lt;-c(&quot;United Stat/US&quot;, &quot;Japan/JP&quot;, &quot;China/CH&quot;) GDP_2017&lt;-c(400,200,290) GDP_2018&lt;-c(420,210,310) GDP_2019&lt;-c(450,230,320) unemployrate_2017=c(&quot;4%&quot;, &quot;3%&quot;,&quot;2%&quot;) unemployrate_2018=c(&quot;4.1%&quot;, &quot;3.3%&quot;,&quot;2.5%&quot;) unemployrate_2019=c(&quot;3.9%&quot;, &quot;3.2%&quot;,&quot;2.9%&quot;) GDP_messy&lt;-data.table(country,GDP_2017,GDP_2018,GDP_2019,unemployrate_2017,unemployrate_2018,unemployrate_2019) GDP_messy ## country GDP_2017 GDP_2018 GDP_2019 unemployrate_2017 ## 1: United Stat/US 400 420 450 4% ## 2: Japan/JP 200 210 230 3% ## 3: China/CH 290 310 320 2% ## unemployrate_2018 unemployrate_2019 ## 1: 4.1% 3.9% ## 2: 3.3% 3.2% ## 3: 2.5% 2.9% This is a non-tidy data because the attribute of an observation (i.e., the year when the observation is observed) is represented in the column. Next, we will use the melt() and dcast() function in data.table package to tidy data. There are some other packages (e.g., tidyr) for this purpose. We choose to use the data.table function for consistence. Also, the data.table is very fast and memory efficient, making it well-suited to handling large data sets. 12.2.1 melt() A common problem is a dataset where some of the column names are not names of variables, but values of a variable. country&lt;-c(&quot;United Stat/US&quot;, &quot;Japan/JP&quot;, &quot;China/CH&quot;) abbr&lt;-c(&quot;US&quot;, &quot;JP&quot;, &quot;CH&quot;) GDP_2017&lt;-c(400,200,290) GDP_2018&lt;-c(420,210,310) GDP_2019&lt;-c(450,230,320) GDP_messy&lt;-data.table(country,abbr, GDP_2017,GDP_2018,GDP_2019) GDP_messy ## country abbr GDP_2017 GDP_2018 GDP_2019 ## 1: United Stat/US US 400 420 450 ## 2: Japan/JP JP 200 210 230 ## 3: China/CH CH 290 310 320 The dataset GDP_messy from the following code as an example. The column GDP_2017, GDP_2018 and GDP_2019 represent the value of the year when it is observed. melt() is design to collect the information in those columns name as a new variable. GDP1&lt;-melt(GDP_messy, id =&quot;country&quot;, measure =c(&quot;GDP_2017&quot;,&quot;GDP_2018&quot;,&quot;GDP_2019&quot;), variable.name = &quot;year&quot;, value.name = &quot;GDP&quot;) GDP1 ## country year GDP ## 1: United Stat/US GDP_2017 400 ## 2: Japan/JP GDP_2017 200 ## 3: China/CH GDP_2017 290 ## 4: United Stat/US GDP_2018 420 ## 5: Japan/JP GDP_2018 210 ## 6: China/CH GDP_2018 310 ## 7: United Stat/US GDP_2019 450 ## 8: Japan/JP GDP_2019 230 ## 9: China/CH GDP_2019 320 The code above collect the colunme name GDP_2017, GDP_2018, GDP_2019 into a new variable (and renamed as year); and put the corresponding value into the value variable (and renamed as GDP). The general format for melt() is as follow: melt( data.table, id= c(\"), # id variable for origin data, will be kept in the new data measure = c(), # columns to be melt variable.name =year, # name of new variable to place original column name value.name =GDP\", # name of new variable to place original column value ) Note that: 1) the melt() function takes only data.table not data.frame as argument; 2) melt() function will return a data.table as a result; 3) the variables not included the id.vars will not be kept in the new data.table. We can also use pattern argument: GDP_tidy = melt(GDP_messy, id=&quot;country&quot;, measure = patterns(&#39;GDP_&#39;),variable.name = &quot;year&quot;, value.name = &quot;GDP&quot;) GDP_tidy ## country year GDP ## 1: United Stat/US GDP_2017 400 ## 2: Japan/JP GDP_2017 200 ## 3: China/CH GDP_2017 290 ## 4: United Stat/US GDP_2018 420 ## 5: Japan/JP GDP_2018 210 ## 6: China/CH GDP_2018 310 ## 7: United Stat/US GDP_2019 450 ## 8: Japan/JP GDP_2019 230 ## 9: China/CH GDP_2019 320 In the above code, instead of specifying the measure =c(GDP_2017,GDP_2018,GDP_2019), which share the same pattern of GDP_, we can use measure=patterns(GDP_) to specify these columns. To clean the data, we need extract the 4-digit year from the year column. This can be done from the following code: GDP_tidy[,year2:=str_sub(year,5,9)] GDP_tidy ## country year GDP year2 ## 1: United Stat/US GDP_2017 400 2017 ## 2: Japan/JP GDP_2017 200 2017 ## 3: China/CH GDP_2017 290 2017 ## 4: United Stat/US GDP_2018 420 2018 ## 5: Japan/JP GDP_2018 210 2018 ## 6: China/CH GDP_2018 310 2018 ## 7: United Stat/US GDP_2019 450 2019 ## 8: Japan/JP GDP_2019 230 2019 ## 9: China/CH GDP_2019 320 2019 str_sub(year,5,9) extract the substring from the character vector year, where the position of the substring is from 5th char to the 9th char. 12.2.2 dcast() The opposite of melt() is dcast(), which takes key-values pairs and spreads them across multiple columns. This is useful when values in a column should actually be column names (i.e. variables). Lets look at one example: country&lt;-rep(c(&quot;United Stat/US&quot;, &quot;Japan/JP&quot;, &quot;China/CH&quot;), times=6) measure&lt;-rep(c(&quot;GDP&quot;,&quot;unemployrate&quot;), each=3*3) year&lt;-rep(c(2017,2018,2019), each=3,times=2) amount&lt;-c(&quot;400&quot;,&quot;200&quot;,&quot;290&quot;,&quot;420&quot;,&quot;210&quot;,&quot;310&quot;,&quot;450&quot;,&quot;230&quot;,&quot;320&quot;,&quot;4%&quot;, &quot;3%&quot;, &quot;2%&quot;,&quot;4.1%&quot;, &quot;3.3%&quot;, &quot;2.5%&quot;, &quot;3.9%&quot;, &quot;3.2%&quot; ,&quot;2.9%&quot;) GPD_messy=data.table(country,year,measure, amount) GPD_messy ## country year measure amount ## 1: United Stat/US 2017 GDP 400 ## 2: Japan/JP 2017 GDP 200 ## 3: China/CH 2017 GDP 290 ## 4: United Stat/US 2018 GDP 420 ## 5: Japan/JP 2018 GDP 210 ## 6: China/CH 2018 GDP 310 ## 7: United Stat/US 2019 GDP 450 ## 8: Japan/JP 2019 GDP 230 ## 9: China/CH 2019 GDP 320 ## 10: United Stat/US 2017 unemployrate 4% ## 11: Japan/JP 2017 unemployrate 3% ## 12: China/CH 2017 unemployrate 2% ## 13: United Stat/US 2018 unemployrate 4.1% ## 14: Japan/JP 2018 unemployrate 3.3% ## 15: China/CH 2018 unemployrate 2.5% ## 16: United Stat/US 2019 unemployrate 3.9% ## 17: Japan/JP 2019 unemployrate 3.2% ## 18: China/CH 2019 unemployrate 2.9% As see, in GPD_messy, the column measure should be variable name: (GDP nd unemployrate are stacked in one column). We need to allocate this column into two columns. The folowing code with dcast() achieves this purpose: GDP_tidy&lt;-dcast(GPD_messy, country+year~measure, value.var=&quot;amount&quot;) GDP_tidy ## country year GDP unemployrate ## 1: China/CH 2017 290 2% ## 2: China/CH 2018 310 2.5% ## 3: China/CH 2019 320 2.9% ## 4: Japan/JP 2017 200 3% ## 5: Japan/JP 2018 210 3.3% ## 6: Japan/JP 2019 230 3.2% ## 7: United Stat/US 2017 400 4% ## 8: United Stat/US 2018 420 4.1% ## 9: United Stat/US 2019 450 3.9% In the above code, country+year~measure is called formula. Here it means that each country+year pair will consists a row in the new data.table, the variables in the original measure column will be the new columns. value.var=amount indicates that the value of the new columns is populated by the value in the orignal amount column. The general syntax for dcast() function is as below: dcast(data, formula, value.var). 12.3 Preparing data for analysis Now that we have tidy the data, we need to futher clean it so that it is ready for statistical analysis or visualization. The most important task in this step is to make sure each column is in the right format (i.e., numbers are numeric, date is represented in date, categorical variables are represented in factors, characters are characters, logical are logical) 12.3.1 Type conversions It is often necessary to change, or coerce, the way that variables in a dataset are stored. This could be because of the way they were read into R (with read.csv(), for example) or perhaps the function you are using to analyze the data requires variables to be coded a certain way. The common type conversions in R include: as.character(2016) ## [1] &quot;2016&quot; as.numeric(TRUE) ## [1] 1 as.factor(c(&quot;level A&quot;, &quot;level B&quot;, &quot;leval A&quot;, &quot;leval A&quot;)) ## [1] level A level B leval A leval A ## Levels: leval A level A level B as.logical(0) ## [1] FALSE Only certain coercions are allowed, but the rules for what works are generally pretty intuitive. For example, trying to convert a character string to a number gives a missing value NA: as.numeric(&quot;some text&quot;) ## Warning: NAs introduced by coercion ## [1] NA There are a few less intuitive results. For example, under the hood, the logical values TRUE and FALSE are coded as 1 and 0, respectively. Therefore, as.logical(1) returns TRUE and as.numeric(TRUE) returns 1. as.logical(1) ## [1] TRUE as.numeric(TRUE) ## [1] 1 12.3.2 Working with dates Dates can be a challenge to work with in any programming language, but thanks to the lubridate package, working with dates in R isnt so bad. Since this module is about cleaning data, we only cover the most basic functions from lubridate to help us standardize the format of dates and times in our data. But this will cover most of the situations you face in real world. These functions combine the letters y, m, d, h, m, s, which stand for year, month, day, hour, minute, and second, respectively. The order of the letters in the function should match the order of the date/time you are attempting to read in, although not all combinations are valid. Notice that the functions are smart in that they are capable of parsing multiple formats. # Experiment with basic lubridate functions ymd(&quot;2015-08-25&quot;) ## [1] &quot;2015-08-25&quot; class(ymd(&quot;2015-08-25&quot;)) ## [1] &quot;Date&quot; ymd(&quot;2015 August 25&quot;) ## [1] &quot;2015-08-25&quot; mdy(&quot;August 25, 2015&quot;) ## [1] &quot;2015-08-25&quot; hms(&quot;13:33:09&quot;) ## [1] &quot;13H 33M 9S&quot; ymd_hms(&quot;2015/08/25 13.33.09&quot;) ## [1] &quot;2015-08-25 13:33:09 UTC&quot; As see, the these functions from lubridate package is quite smart to understand the general date in various form and return the standardized date. 12.3.3 String (character) manipulation Here we introduce you to string manipulation in R. In many situation we have to deal with textual data, e.g., customer review, address, tweets, . We will use the stringr package for string manipulation, which is written Hadley Wickham. Base R contains many functions to work with strings but well avoid them because they can be inconsistent, which makes them hard to remember. Instead well use functions from stringr. These have more intuitive names, and all start with str_. Examine the length of a string: str_length(c(&quot;a&quot;, &quot;data science&quot;, NA)) ## [1] 1 12 NA Combine two or more strings (we have used paste() in base R for this): str_c(&quot;Leon&quot;, &quot;Xu&quot;, sep=&quot; &quot;) ## [1] &quot;Leon Xu&quot; str_c(2019, &quot;Mar&quot;, 24, sep=&quot;-&quot;) ## [1] &quot;2019-Mar-24&quot; str_c(&quot;https://www.zillow.com/&quot;, c(&quot;page1&quot;,&quot;page2&quot;,&quot;page3&quot;), sep=&quot;&quot;) ## [1] &quot;https://www.zillow.com/page1&quot; &quot;https://www.zillow.com/page2&quot; ## [3] &quot;https://www.zillow.com/page3&quot; subsetting strings: you can extract parts of a string use str_sub(). Subsetting a string is the most used operations to deal with strings. E.g., to extract zip from address. It takes a general form like this: str_sub(string, start, end) address=&quot;7313 Sherman Street, Licoln, NE 68506&quot; len=str_length(address) str_sub(address, len-4, len) ## [1] &quot;68506&quot; str_replace() and str_replace_all() allow you to replace matches with new strings. The general syntax is: str_replace(string, pattern, replacement) x&lt;-c(&quot;apple?&quot;,&quot;pear4&quot;,&quot;banana&quot;) str_replace(x,&quot;a&quot;,&quot;A&quot;) # replace the first match ## [1] &quot;Apple?&quot; &quot;peAr4&quot; &quot;bAnana&quot; str_replace_all(x,&quot;a&quot;,&quot;A&quot;) # replace all match ## [1] &quot;Apple?&quot; &quot;peAr4&quot; &quot;bAnAnA&quot; Trim all leading and trailing whitespace str_trim(c(&quot; Filip &quot;, &quot;Nick &quot;, &quot; Jonathan&quot;)) ## [1] &quot;Filip&quot; &quot;Nick&quot; &quot;Jonathan&quot; Pad these strings with leading zeros str_pad(c(&quot;23485W&quot;, &quot;8823453Q&quot;, &quot;994Z&quot;), width=7, side=&#39;left&#39;, pad=&quot;0&quot;) ## [1] &quot;023485W&quot; &quot;8823453Q&quot; &quot;000994Z&quot; tolower() turn all letter in string into lower case. tolower(&quot;University of Nebraska&quot;) ## [1] &quot;university of nebraska&quot; toupper() turn all letter in string into upper case. toupper(&quot;University of Nebraska&quot;) ## [1] &quot;UNIVERSITY OF NEBRASKA&quot; 12.4 Missing and special values in R Missing values in R should be represented by NA. If missing values are properly coded as NA, the is.na() function will help you find them. Unfortunately you will not always be so lucky. Before you can deal with missing values, you have to find them in the data. Otherwise, if your dataset is too big to just look at the whole thing, you may need to try searching for some of the usual suspects like \",#N/A\", etc to identify the missing value. Lets examine the example to see the missing values. x&lt;-data.table(x1=c(2,5,NA,9), x2=c(NA, 23,9,NA)) x ## x1 x2 ## 1: 2 NA ## 2: 5 23 ## 3: NA 9 ## 4: 9 NA is.na() return for each element in the data.table, it returns TRUE/FALSE to indicate whether the data is missing. is.na(x) ## x1 x2 ## [1,] FALSE TRUE ## [2,] FALSE FALSE ## [3,] TRUE FALSE ## [4,] FALSE TRUE # since TRUE is stored as 1 and FALSE is stored as 0, we can use sum() to count to total number of missing value sum(is.na(x)) ## [1] 3 # use which() returns the index of missing elements in each column which(is.na(x$x1)) ## [1] 3 which(is.na(x$x2)) ## [1] 1 4 # Use complete.cases() to see which rows have no missing values complete.cases(x) ## [1] FALSE TRUE FALSE FALSE For each row in x, complete.cases(x) returns TRUE/FALSE if the corresponding row hast/has missing value. # Use na.omit() to remove all rows with any missing values na.omit(x) ## x1 x2 ## 1: 5 23 na.omit() remove all rows that contain missing value. The resulting data will have no missing value. 12.5 Outliers, obvious error and missing value When dealing with strange values in your data, you often must decide whether they are just extreme or actually error. Extreme values show up all over the place, but you, the data analyst, must figure out when they are plausible and when they are not. One quick way to see the outlier is to visulize your data. The summary() and hist() and boxplot() are useful for this purpose. # Look at a summary() of a variable summary(as.numeric(weather_boston$X1)) ## Warning in summary(as.numeric(weather_boston$X1)): NAs introduced by coercion ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -6.00 10.00 29.99 38.70 50.00 332.00 17 # View a histogram of a variable hist(as.numeric(weather_boston$X1), breaks=30) ## Warning in hist(as.numeric(weather_boston$X1), breaks = 30): NAs introduced by ## coercion Another useful way of looking at strange values is with boxplots. Simply put, boxplots draw a box around the middle 50% of values for a given variable, with a bolded horizontal line drawn at the median. Values that fall far from the bulk of the data points (i.e. outliers) are denoted by open circles. # view a boxplot boxplot(as.numeric(weather_boston$X1)) ## Warning in boxplot(as.numeric(weather_boston$X1)): NAs introduced by coercion 12.6 Summary 80% of the data scientists time is to clean data. Three steps to clean data: 1) exploring raw data to for diagnosis; 2) tidy the data using melt() and dcast() 3) convert columns into the right variable type. A tidy data means each row represents an observation, and each column represents a variable (attribute) about an observation. When columns names contains variable information, we should use melt(); when multiple variables are stacked into rows, we should use dcast(). Convert each column into the correct variable types using as.character(), as.numeric(), as.factor(), as.logical(). Use lubridate and stringr package to work with date and string variables. "],["data-cleaning-ii.html", "Chapter 13 Data Cleaning II 13.1 Explore the raw data 13.2 Summarize the data 13.3 Tidy the data 13.4 prepare for analysis 13.5 Summary", " Chapter 13 Data Cleaning II In this chapter, we will continue the discussion on how to clean data through an example of cleaning the messy dataset: weather_boston. Run the code to install the packages (only need to run once): install.packages(&quot;lubridate&quot;) # dealing date variable install.packages(&quot;stringr&quot;) # dealing strings/character install.packages(&quot;tidyr&quot;) # tidying data install.packages(&quot;data.table&quot;) Run the code to load the packages into R: library(lubridate) library(stringr) library(tidyr) library(data.table) Just a recap: the three basic steps to clean a messy data include: exploring raw data to diagnose the places to be cleaned tidying data to make rows as observations, columns as variables. preparing data for analysis (converting columns into the right variable type) First thing first, lets read the weather_boston.csv data into R. # use fread() to read csv into R as data.table weather_boston&lt;-fread(&quot;data/weather_boston.csv&quot;) 13.1 Explore the raw data Before diving into our data cleaning routine, we must first understand the basic structure of the data. This involves looking at things like the class() of the data object to make sure its what we expect (generally a data.table) in addition to checking its dimensions with dim() and the column names with names() # Verify that weather_boston is a data.frame class(weather_boston) ## [1] &quot;data.table&quot; &quot;data.frame&quot; # Check the dimensions dim(weather_boston) ## [1] 286 34 # View the column names names(weather_boston) ## [1] &quot;year&quot; &quot;month&quot; &quot;measure&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; ## [8] &quot;X5&quot; &quot;X6&quot; &quot;X7&quot; &quot;X8&quot; &quot;X9&quot; &quot;X10&quot; &quot;X11&quot; ## [15] &quot;X12&quot; &quot;X13&quot; &quot;X14&quot; &quot;X15&quot; &quot;X16&quot; &quot;X17&quot; &quot;X18&quot; ## [22] &quot;X19&quot; &quot;X20&quot; &quot;X21&quot; &quot;X22&quot; &quot;X23&quot; &quot;X24&quot; &quot;X25&quot; ## [29] &quot;X26&quot; &quot;X27&quot; &quot;X28&quot; &quot;X29&quot; &quot;X30&quot; &quot;X31&quot; 13.2 Summarize the data Next up is to look at some summaries of the data. This is where functions like str(), glimpse() from dplyr, and summary() come in handy. # View the structure of the data str(weather_boston) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 286 obs. of 34 variables: ## $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... ## $ month : int 12 12 12 12 12 12 12 12 12 12 ... ## $ measure: chr &quot;Max.TemperatureF&quot; &quot;Mean.TemperatureF&quot; &quot;Min.TemperatureF&quot; &quot;Max.Dew.PointF&quot; ... ## $ X1 : chr &quot;64&quot; &quot;52&quot; &quot;39&quot; &quot;46&quot; ... ## $ X2 : chr &quot;42&quot; &quot;38&quot; &quot;33&quot; &quot;40&quot; ... ## $ X3 : chr &quot;51&quot; &quot;44&quot; &quot;37&quot; &quot;49&quot; ... ## $ X4 : chr &quot;43&quot; &quot;37&quot; &quot;30&quot; &quot;24&quot; ... ## $ X5 : chr &quot;42&quot; &quot;34&quot; &quot;26&quot; &quot;37&quot; ... ## $ X6 : chr &quot;45&quot; &quot;42&quot; &quot;38&quot; &quot;45&quot; ... ## $ X7 : chr &quot;38&quot; &quot;30&quot; &quot;21&quot; &quot;36&quot; ... ## $ X8 : chr &quot;29&quot; &quot;24&quot; &quot;18&quot; &quot;28&quot; ... ## $ X9 : chr &quot;49&quot; &quot;39&quot; &quot;29&quot; &quot;49&quot; ... ## $ X10 : chr &quot;48&quot; &quot;43&quot; &quot;38&quot; &quot;45&quot; ... ## $ X11 : chr &quot;39&quot; &quot;36&quot; &quot;32&quot; &quot;37&quot; ... ## $ X12 : chr &quot;39&quot; &quot;35&quot; &quot;31&quot; &quot;28&quot; ... ## $ X13 : chr &quot;42&quot; &quot;37&quot; &quot;32&quot; &quot;28&quot; ... ## $ X14 : chr &quot;45&quot; &quot;39&quot; &quot;33&quot; &quot;29&quot; ... ## $ X15 : chr &quot;42&quot; &quot;37&quot; &quot;32&quot; &quot;33&quot; ... ## $ X16 : chr &quot;44&quot; &quot;40&quot; &quot;35&quot; &quot;42&quot; ... ## $ X17 : chr &quot;49&quot; &quot;45&quot; &quot;41&quot; &quot;46&quot; ... ## $ X18 : chr &quot;44&quot; &quot;40&quot; &quot;36&quot; &quot;34&quot; ... ## $ X19 : chr &quot;37&quot; &quot;33&quot; &quot;29&quot; &quot;25&quot; ... ## $ X20 : chr &quot;36&quot; &quot;32&quot; &quot;27&quot; &quot;30&quot; ... ## $ X21 : chr &quot;36&quot; &quot;33&quot; &quot;30&quot; &quot;30&quot; ... ## $ X22 : chr &quot;44&quot; &quot;39&quot; &quot;33&quot; &quot;39&quot; ... ## $ X23 : chr &quot;47&quot; &quot;45&quot; &quot;42&quot; &quot;45&quot; ... ## $ X24 : chr &quot;46&quot; &quot;44&quot; &quot;41&quot; &quot;46&quot; ... ## $ X25 : chr &quot;59&quot; &quot;52&quot; &quot;44&quot; &quot;58&quot; ... ## $ X26 : chr &quot;50&quot; &quot;44&quot; &quot;37&quot; &quot;31&quot; ... ## $ X27 : chr &quot;52&quot; &quot;45&quot; &quot;38&quot; &quot;34&quot; ... ## $ X28 : chr &quot;52&quot; &quot;46&quot; &quot;40&quot; &quot;42&quot; ... ## $ X29 : chr &quot;41&quot; &quot;36&quot; &quot;30&quot; &quot;26&quot; ... ## $ X30 : chr &quot;30&quot; &quot;26&quot; &quot;22&quot; &quot;10&quot; ... ## $ X31 : chr &quot;30&quot; &quot;25&quot; &quot;20&quot; &quot;8&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; # View a summary of the data summary(weather_boston) ## year month measure X1 ## Min. :2014 Min. : 1.000 Length:286 Length:286 ## 1st Qu.:2015 1st Qu.: 4.000 Class :character Class :character ## Median :2015 Median : 7.000 Mode :character Mode :character ## Mean :2015 Mean : 6.923 ## 3rd Qu.:2015 3rd Qu.:10.000 ## Max. :2015 Max. :12.000 ## X2 X3 X4 X5 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X6 X7 X8 X9 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X10 X11 X12 X13 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X14 X15 X16 X17 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X18 X19 X20 X21 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X22 X23 X24 X25 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X26 X27 X28 X29 ## Length:286 Length:286 Length:286 Length:286 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## X30 X31 ## Length:286 Length:286 ## Class :character Class :character ## Mode :character Mode :character ## ## ## 13.2.1 Take a closer look After understanding the structure of the data and looking at some brief summaries, it often helps to preview the actual data. The functions head() and tail() allow you to view the top and bottom 6 rows of the data, respectively. You can show more rows by assigning a second argument. # View first 6 rows head(weather_boston) ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 ## 1: 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 45 ## 2: 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 39 ## 3: 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 33 ## 4: 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 29 ## 5: 2014 12 MeanDew.PointF 40 27 42 21 25 40 20 16 41 39 31 27 26 27 ## 6: 2014 12 Min.DewpointF 26 17 24 13 12 36 -3 3 28 37 27 25 24 25 ## X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: 42 44 49 44 37 36 36 44 47 46 59 50 52 52 41 30 30 ## 2: 37 40 45 40 33 32 33 39 45 44 52 44 45 46 36 26 25 ## 3: 32 35 41 36 29 27 30 33 42 41 44 37 38 40 30 22 20 ## 4: 33 42 46 34 25 30 30 39 45 46 58 31 34 42 26 10 8 ## 5: 29 36 41 30 22 24 27 34 42 44 43 29 31 35 20 4 5 ## 6: 27 30 32 26 20 20 25 25 37 41 29 28 29 27 10 -6 1 # View first 8 rows head(weather_boston, 8) ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 ## 1: 2014 12 Max.TemperatureF 64 42 51 43 42 45 38 29 49 48 39 39 42 ## 2: 2014 12 Mean.TemperatureF 52 38 44 37 34 42 30 24 39 43 36 35 37 ## 3: 2014 12 Min.TemperatureF 39 33 37 30 26 38 21 18 29 38 32 31 32 ## 4: 2014 12 Max.Dew.PointF 46 40 49 24 37 45 36 28 49 45 37 28 28 ## 5: 2014 12 MeanDew.PointF 40 27 42 21 25 40 20 16 41 39 31 27 26 ## 6: 2014 12 Min.DewpointF 26 17 24 13 12 36 -3 3 28 37 27 25 24 ## 7: 2014 12 Max.Humidity 74 92 100 69 85 100 92 92 100 100 92 85 75 ## 8: 2014 12 Mean.Humidity 63 72 79 54 66 93 61 70 93 95 87 75 65 ## X14 X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: 45 42 44 49 44 37 36 36 44 47 46 59 50 52 52 41 30 30 ## 2: 39 37 40 45 40 33 32 33 39 45 44 52 44 45 46 36 26 25 ## 3: 33 32 35 41 36 29 27 30 33 42 41 44 37 38 40 30 22 20 ## 4: 29 33 42 46 34 25 30 30 39 45 46 58 31 34 42 26 10 8 ## 5: 27 29 36 41 30 22 24 27 34 42 44 43 29 31 35 20 4 5 ## 6: 25 27 30 32 26 20 20 25 25 37 41 29 28 29 27 10 -6 1 ## 7: 82 89 96 100 89 69 89 85 89 100 100 100 70 70 76 64 50 57 ## 8: 68 75 85 85 73 63 79 77 79 91 98 75 60 60 65 51 38 44 # View the last 6 rows tail(weather_boston) ## year month measure X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 ## 1: 2015 12 Mean.Wind.SpeedMPH 6 ## 2: 2015 12 Max.Gust.SpeedMPH 17 ## 3: 2015 12 PrecipitationIn 0.14 ## 4: 2015 12 CloudCover 7 ## 5: 2015 12 Events Rain ## 6: 2015 12 WindDirDegrees 109 ## X14 X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 X30 X31 ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: 13.3 Tidy the data The first symptoms of the data is that column names are values. In particular, the column names X1-X31 represent days of the month, which should really be values of a new variable called day. We can use melt() function exactly this scenario. weather_boston2 = melt(weather_boston, id=c(&quot;year&quot;,&quot;month&quot;,&quot;measure&quot;), measure=patterns(&quot;X&quot;), variable.name = &quot;day&quot;, value.name= &quot;value&quot;) # View the head head(weather_boston2) ## year month measure day value ## 1: 2014 12 Max.TemperatureF X1 64 ## 2: 2014 12 Mean.TemperatureF X1 52 ## 3: 2014 12 Min.TemperatureF X1 39 ## 4: 2014 12 Max.Dew.PointF X1 46 ## 5: 2014 12 MeanDew.PointF X1 40 ## 6: 2014 12 Min.DewpointF X1 26 The resulting data now suffers from another common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. The dcast() function is designed to help with this. weather_boston3 &lt;- dcast(weather_boston2, year+month+day~measure, value.var = &quot;value&quot;) head(weather_boston3) ## year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH ## 1: 2014 12 X1 6 Rain 46 29 ## 2: 2014 12 X2 7 Rain-Snow 40 29 ## 3: 2014 12 X3 8 Rain 49 38 ## 4: 2014 12 X4 3 24 33 ## 5: 2014 12 X5 5 Rain 37 26 ## 6: 2014 12 X6 8 Rain 45 25 ## Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## 1: 74 30.45 64 10 ## 2: 92 30.71 42 10 ## 3: 100 30.4 51 10 ## 4: 69 30.56 43 10 ## 5: 85 30.68 42 10 ## 6: 100 30.42 45 10 ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## 1: 22 63 30.13 52 ## 2: 24 72 30.59 38 ## 3: 29 79 30.07 44 ## 4: 25 54 30.33 37 ## 5: 22 66 30.59 34 ## 6: 22 93 30.24 42 ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## 1: 10 13 40 26 ## 2: 8 15 27 17 ## 3: 5 12 42 24 ## 4: 10 12 21 13 ## 5: 10 10 25 12 ## 6: 4 8 40 36 ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## 1: 52 30.01 39 10 ## 2: 51 30.4 33 2 ## 3: 57 29.87 37 1 ## 4: 39 30.09 30 10 ## 5: 47 30.45 26 5 ## 6: 85 30.16 38 0 ## PrecipitationIn WindDirDegrees ## 1: 0.01 268 ## 2: 0.10 62 ## 3: 0.44 254 ## 4: 0.00 292 ## 5: 0.11 61 ## 6: 1.09 313 Now, the data is tidy: each row is an observation, each column is a variable. 13.4 prepare for analysis 13.4.1 Clean up dates Now that the weather_boston dataset adheres to tidy data principles, the next step is to prepare it for analysis. Well start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of base R, stringr, and lubridate to accomplish this task. # Remove X&#39;s from day column weather_boston3[,day:=str_replace(day, &quot;X&quot;, &quot;&quot;)] head(weather_boston3) ## year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH ## 1: 2014 12 1 6 Rain 46 29 ## 2: 2014 12 2 7 Rain-Snow 40 29 ## 3: 2014 12 3 8 Rain 49 38 ## 4: 2014 12 4 3 24 33 ## 5: 2014 12 5 5 Rain 37 26 ## 6: 2014 12 6 8 Rain 45 25 ## Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## 1: 74 30.45 64 10 ## 2: 92 30.71 42 10 ## 3: 100 30.4 51 10 ## 4: 69 30.56 43 10 ## 5: 85 30.68 42 10 ## 6: 100 30.42 45 10 ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## 1: 22 63 30.13 52 ## 2: 24 72 30.59 38 ## 3: 29 79 30.07 44 ## 4: 25 54 30.33 37 ## 5: 22 66 30.59 34 ## 6: 22 93 30.24 42 ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## 1: 10 13 40 26 ## 2: 8 15 27 17 ## 3: 5 12 42 24 ## 4: 10 12 21 13 ## 5: 10 10 25 12 ## 6: 4 8 40 36 ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## 1: 52 30.01 39 10 ## 2: 51 30.4 33 2 ## 3: 57 29.87 37 1 ## 4: 39 30.09 30 10 ## 5: 47 30.45 26 5 ## 6: 85 30.16 38 0 ## PrecipitationIn WindDirDegrees ## 1: 0.01 268 ## 2: 0.10 62 ## 3: 0.44 254 ## 4: 0.00 292 ## 5: 0.11 61 ## 6: 1.09 313 Convert date column to proper date format using lubridatess ymd() # combine year, month day to be a date variable weather_boston3[,date:=ymd(str_c(year,month,day, sep=&quot;-&quot;))] ## Warning: 7 failed to parse. # examine the rows with missing date weather_boston3[is.na(date)] ## year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH ## 1: 2015 2 29 ## 2: 2015 2 30 ## 3: 2015 2 31 ## 4: 2015 4 31 ## 5: 2015 6 31 ## 6: 2015 9 31 ## 7: 2015 11 31 ## Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## 1: ## 2: ## 3: ## 4: ## 5: ## 6: ## 7: ## PrecipitationIn WindDirDegrees date ## 1: &lt;NA&gt; ## 2: &lt;NA&gt; ## 3: &lt;NA&gt; ## 4: &lt;NA&gt; ## 5: &lt;NA&gt; ## 6: &lt;NA&gt; ## 7: &lt;NA&gt; # remove rows with missing date weather_boston4=weather_boston3[!is.na(date)] 13.4.2 A closer look at column types Its important for analysis that variables are coded appropriately. This is not yet the case with our weather_boston data. Recall that functions such as as.numeric() and as.character() can be used to coerce variables into different types. Its important to keep in mind that coercion are not always successful, particularly if theres some data in a column that you dont expect. For example, the following will cause problems: as.numeric(c(4, 6.44, &quot;some string&quot;, 222)) ## Warning: NAs introduced by coercion ## [1] 4.00 6.44 NA 222.00 If you run the code above in the console, youll get a warning message saying that R introduced an NA in the process of coercing to numeric. This is because it doesnt know how to make a number out of a string (some string). Watch out for this in our weather_boston data! str(weather_boston4) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 396 obs. of 26 variables: ## $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... ## $ month : int 12 12 12 12 12 12 12 12 12 12 ... ## $ day : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ CloudCover : chr &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;3&quot; ... ## $ Events : chr &quot;Rain&quot; &quot;Rain-Snow&quot; &quot;Rain&quot; &quot;&quot; ... ## $ Max.Dew.PointF : chr &quot;46&quot; &quot;40&quot; &quot;49&quot; &quot;24&quot; ... ## $ Max.Gust.SpeedMPH : chr &quot;29&quot; &quot;29&quot; &quot;38&quot; &quot;33&quot; ... ## $ Max.Humidity : chr &quot;74&quot; &quot;92&quot; &quot;100&quot; &quot;69&quot; ... ## $ Max.Sea.Level.PressureIn : chr &quot;30.45&quot; &quot;30.71&quot; &quot;30.4&quot; &quot;30.56&quot; ... ## $ Max.TemperatureF : chr &quot;64&quot; &quot;42&quot; &quot;51&quot; &quot;43&quot; ... ## $ Max.VisibilityMiles : chr &quot;10&quot; &quot;10&quot; &quot;10&quot; &quot;10&quot; ... ## $ Max.Wind.SpeedMPH : chr &quot;22&quot; &quot;24&quot; &quot;29&quot; &quot;25&quot; ... ## $ Mean.Humidity : chr &quot;63&quot; &quot;72&quot; &quot;79&quot; &quot;54&quot; ... ## $ Mean.Sea.Level.PressureIn: chr &quot;30.13&quot; &quot;30.59&quot; &quot;30.07&quot; &quot;30.33&quot; ... ## $ Mean.TemperatureF : chr &quot;52&quot; &quot;38&quot; &quot;44&quot; &quot;37&quot; ... ## $ Mean.VisibilityMiles : chr &quot;10&quot; &quot;8&quot; &quot;5&quot; &quot;10&quot; ... ## $ Mean.Wind.SpeedMPH : chr &quot;13&quot; &quot;15&quot; &quot;12&quot; &quot;12&quot; ... ## $ MeanDew.PointF : chr &quot;40&quot; &quot;27&quot; &quot;42&quot; &quot;21&quot; ... ## $ Min.DewpointF : chr &quot;26&quot; &quot;17&quot; &quot;24&quot; &quot;13&quot; ... ## $ Min.Humidity : chr &quot;52&quot; &quot;51&quot; &quot;57&quot; &quot;39&quot; ... ## $ Min.Sea.Level.PressureIn : chr &quot;30.01&quot; &quot;30.4&quot; &quot;29.87&quot; &quot;30.09&quot; ... ## $ Min.TemperatureF : chr &quot;39&quot; &quot;33&quot; &quot;37&quot; &quot;30&quot; ... ## $ Min.VisibilityMiles : chr &quot;10&quot; &quot;2&quot; &quot;1&quot; &quot;10&quot; ... ## $ PrecipitationIn : chr &quot;0.01&quot; &quot;0.10&quot; &quot;0.44&quot; &quot;0.00&quot; ... ## $ WindDirDegrees : chr &quot;268&quot; &quot;62&quot; &quot;254&quot; &quot;292&quot; ... ## $ date : Date, format: &quot;2014-12-01&quot; &quot;2014-12-02&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; ## - attr(*, &quot;sorted&quot;)= chr [1:2] &quot;year&quot; &quot;month&quot; Thus, all of these character variables (except for events) need to be convert to numeric. Now, see what happens if we try to convert PrecipitationIn to numeric as.numeric(weather_boston4$PrecipitationIn) ## Warning: NAs introduced by coercion ## [1] 0.01 0.10 0.44 0.00 0.11 1.09 0.13 0.03 2.90 0.28 0.02 NA NA 0.00 0.00 ## [16] NA 0.43 0.01 0.00 NA NA 0.05 0.25 0.56 0.14 0.00 0.00 0.01 0.00 0.00 ## [31] 0.00 0.00 0.00 0.62 0.57 0.00 0.02 NA 0.00 0.01 0.00 0.00 0.20 0.00 NA ## [46] 0.12 0.00 0.00 0.15 0.00 0.00 0.00 NA 0.00 0.71 0.00 0.10 0.95 0.01 NA ## [61] 0.06 0.05 0.00 0.78 0.00 0.00 0.09 NA 0.07 0.37 0.88 0.05 0.01 0.03 0.00 ## [76] 0.23 0.39 0.00 0.02 0.01 0.06 0.00 0.17 0.11 0.00 NA 0.07 0.02 0.00 0.00 ## [91] 0.17 0.01 0.26 0.02 NA 0.00 0.00 NA 0.00 0.06 0.01 0.00 0.00 0.80 0.27 ## [106] 0.00 0.14 0.00 0.00 0.05 0.09 0.00 0.00 0.00 0.04 0.80 0.21 0.12 0.00 NA ## [121] 0.00 0.00 0.00 0.03 0.39 0.00 0.00 0.03 0.26 0.09 0.09 0.00 0.00 0.00 0.01 ## [136] 0.00 0.00 0.06 0.00 0.00 0.61 0.54 NA 0.00 NA 0.00 0.00 0.10 0.07 0.00 ## [151] 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 ## [166] 0.00 NA 0.00 0.00 0.27 0.00 0.00 NA 0.00 0.00 NA 0.00 0.00 NA 0.00 ## [181] 0.00 0.91 0.38 0.74 0.00 0.00 NA 0.09 0.00 NA NA 0.00 0.00 0.00 NA ## [196] 0.00 0.40 NA 0.00 0.00 0.00 0.04 1.72 0.00 0.01 0.00 0.00 NA 0.20 1.43 ## [211] NA 0.00 0.50 0.00 0.00 NA 0.00 0.00 0.02 NA 0.15 1.12 0.00 0.00 0.00 ## [226] 0.03 NA 0.00 NA 0.14 NA NA NA 0.00 0.00 0.01 0.00 NA 0.06 0.00 ## [241] 0.00 0.02 0.00 NA 0.00 0.00 0.49 0.00 0.00 0.00 0.00 0.00 0.00 0.83 0.00 ## [256] 0.00 0.00 0.08 0.00 0.00 0.14 0.00 0.00 0.63 NA 0.02 NA 0.00 NA 0.00 ## [271] 0.00 0.00 0.00 0.00 0.00 0.00 0.01 NA 0.00 0.00 0.00 0.20 0.00 0.17 0.66 ## [286] 0.01 0.38 0.00 0.00 0.00 0.00 0.00 0.00 NA 0.00 0.00 0.00 0.00 0.00 0.00 ## [301] 0.00 0.00 0.04 2.46 NA 0.08 0.01 0.00 0.00 0.00 0.00 0.00 0.34 0.00 0.00 ## [316] 0.00 0.12 0.00 0.00 NA NA NA 0.00 NA 0.07 NA 0.00 0.00 0.03 0.00 ## [331] 0.00 0.36 0.73 0.00 0.00 NA 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.07 ## [346] 0.54 0.04 0.01 0.00 0.00 0.00 0.00 0.00 NA 0.86 0.00 0.30 0.04 0.00 0.00 ## [361] 0.00 0.00 0.21 0.00 0.00 0.14 NA NA NA NA NA NA NA NA NA ## [376] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [391] NA NA NA NA NA NA Note that some NA is created. This is because there are strings in the PrecipitationIn column. To find that out, we can use table() to get the frequency count of each value in PrecipitationIn. table(weather_boston4$PrecipitationIn) ## ## 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 ## 30 190 16 10 6 5 4 5 5 2 5 3 2 3 1 5 ## 0.15 0.17 0.20 0.21 0.23 0.25 0.26 0.27 0.28 0.30 0.34 0.36 0.37 0.38 0.39 0.40 ## 2 3 3 2 1 1 2 2 1 1 1 1 1 2 2 1 ## 0.43 0.44 0.49 0.50 0.54 0.56 0.57 0.61 0.62 0.63 0.66 0.71 0.73 0.74 0.78 0.80 ## 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 ## 0.83 0.86 0.88 0.91 0.95 1.09 1.12 1.43 1.72 2.46 2.90 T ## 1 1 1 1 1 1 1 1 1 1 1 49 Here, T was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the PrecipitationIn column. In order to coerce this column to numeric, youll need to deal with this somehow. To keep things simple, we will just replace T with zero, as a string (0). # replace &quot;T&quot; with &quot;0&quot; in the PrecipitationIn column weather_boston4$PrecipitationIn&lt;-str_replace(weather_boston4$PrecipitationIn,&quot;T&quot;,&quot;0&quot;) # convert string to numeric using := weather_boston4[,weather_boston4:=as.numeric(PrecipitationIn) ] If we want to convert multiple column to numeric simultaneously: # obtain the column names which you want to convert to numeric col_names=names(weather_boston4)[c(3,4,6:25)] # convert the to numeric weather_boston4[,(col_names):=lapply(.SD, as.numeric), .SDcols=col_names] str(weather_boston4) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 396 obs. of 27 variables: ## $ year : int 2014 2014 2014 2014 2014 2014 2014 2014 2014 2014 ... ## $ month : int 12 12 12 12 12 12 12 12 12 12 ... ## $ day : num 1 2 3 4 5 6 7 8 9 10 ... ## $ CloudCover : num 6 7 8 3 5 8 6 8 8 8 ... ## $ Events : chr &quot;Rain&quot; &quot;Rain-Snow&quot; &quot;Rain&quot; &quot;&quot; ... ## $ Max.Dew.PointF : num 46 40 49 24 37 45 36 28 49 45 ... ## $ Max.Gust.SpeedMPH : num 29 29 38 33 26 25 32 28 52 29 ... ## $ Max.Humidity : num 74 92 100 69 85 100 92 92 100 100 ... ## $ Max.Sea.Level.PressureIn : num 30.4 30.7 30.4 30.6 30.7 ... ## $ Max.TemperatureF : num 64 42 51 43 42 45 38 29 49 48 ... ## $ Max.VisibilityMiles : num 10 10 10 10 10 10 10 10 10 10 ... ## $ Max.Wind.SpeedMPH : num 22 24 29 25 22 22 25 21 38 23 ... ## $ Mean.Humidity : num 63 72 79 54 66 93 61 70 93 95 ... ## $ Mean.Sea.Level.PressureIn: num 30.1 30.6 30.1 30.3 30.6 ... ## $ Mean.TemperatureF : num 52 38 44 37 34 42 30 24 39 43 ... ## $ Mean.VisibilityMiles : num 10 8 5 10 10 4 10 8 2 3 ... ## $ Mean.Wind.SpeedMPH : num 13 15 12 12 10 8 15 13 20 13 ... ## $ MeanDew.PointF : num 40 27 42 21 25 40 20 16 41 39 ... ## $ Min.DewpointF : num 26 17 24 13 12 36 -3 3 28 37 ... ## $ Min.Humidity : num 52 51 57 39 47 85 29 47 86 89 ... ## $ Min.Sea.Level.PressureIn : num 30 30.4 29.9 30.1 30.4 ... ## $ Min.TemperatureF : num 39 33 37 30 26 38 21 18 29 38 ... ## $ Min.VisibilityMiles : num 10 2 1 10 5 0 5 2 1 1 ... ## $ PrecipitationIn : num 0.01 0.1 0.44 0 0.11 1.09 0.13 0.03 2.9 0.28 ... ## $ WindDirDegrees : num 268 62 254 292 61 313 350 354 38 357 ... ## $ date : Date, format: &quot;2014-12-01&quot; &quot;2014-12-02&quot; ... ## $ weather_boston4 : num 0.01 0.1 0.44 0 0.11 1.09 0.13 0.03 2.9 0.28 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; ## - attr(*, &quot;sorted&quot;)= chr [1:2] &quot;year&quot; &quot;month&quot; In the above code, LHS:=RHS is the data.table syntax for updating columns. On the LHS, we specify the columns to be updated using the col_names (note, do not forget to the ()). The RHS side is more comples: First, .SD is the special character in data.table to represent the subset of data and .SDcols=col_names specifies which variables should be in the subset of data by column names. The syntax for lapply() is as follow: lapply(X, FUN) Arguments: -X: A vector or an object -FUN: Function applied to each element of x 13.4.3 Missing, extreme and unexpected values Return whether a data frame element is missing: is.na() # Count missing values sum(is.na(weather_boston4)) ## [1] 666 # Find missing values summary(is.na(weather_boston4)) ## year month day CloudCover ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:396 FALSE:396 FALSE:396 FALSE:366 ## TRUE :30 ## Events Max.Dew.PointF Max.Gust.SpeedMPH Max.Humidity ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:396 FALSE:366 FALSE:360 FALSE:366 ## TRUE :30 TRUE :36 TRUE :30 ## Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## Mode :logical Mode :logical Mode :logical ## FALSE:366 FALSE:366 FALSE:366 ## TRUE :30 TRUE :30 TRUE :30 ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:366 FALSE:366 FALSE:366 FALSE:366 ## TRUE :30 TRUE :30 TRUE :30 TRUE :30 ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:366 FALSE:366 FALSE:366 FALSE:366 ## TRUE :30 TRUE :30 TRUE :30 TRUE :30 ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:366 FALSE:366 FALSE:366 FALSE:366 ## TRUE :30 TRUE :30 TRUE :30 TRUE :30 ## PrecipitationIn WindDirDegrees date weather_boston4 ## Mode :logical Mode :logical Mode :logical Mode :logical ## FALSE:366 FALSE:366 FALSE:396 FALSE:366 ## TRUE :30 TRUE :30 TRUE :30 # Find indices of NAs in Max.Gust.SpeedMPH ( ind&lt;-which(is.na(weather_boston4$Max.Gust.SpeedMPH )) ) ## [1] 169 185 251 275 316 338 367 368 369 370 371 372 373 374 375 376 377 378 379 ## [20] 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 # Look at the full rows for records missing Max.Gust.SpeedMPH weather_boston4[ind,] ## year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH ## 1: 2015 5 18 6 Fog 52 NA ## 2: 2015 6 3 7 48 NA ## 3: 2015 8 8 4 61 NA ## 4: 2015 9 1 1 63 NA ## 5: 2015 10 12 0 56 NA ## 6: 2015 11 3 1 44 NA ## 7: 2015 12 2 NA NA NA ## 8: 2015 12 3 NA NA NA ## 9: 2015 12 4 NA NA NA ## 10: 2015 12 5 NA NA NA ## 11: 2015 12 6 NA NA NA ## 12: 2015 12 7 NA NA NA ## 13: 2015 12 8 NA NA NA ## 14: 2015 12 9 NA NA NA ## 15: 2015 12 10 NA NA NA ## 16: 2015 12 11 NA NA NA ## 17: 2015 12 12 NA NA NA ## 18: 2015 12 13 NA NA NA ## 19: 2015 12 14 NA NA NA ## 20: 2015 12 15 NA NA NA ## 21: 2015 12 16 NA NA NA ## 22: 2015 12 17 NA NA NA ## 23: 2015 12 18 NA NA NA ## 24: 2015 12 19 NA NA NA ## 25: 2015 12 20 NA NA NA ## 26: 2015 12 21 NA NA NA ## 27: 2015 12 22 NA NA NA ## 28: 2015 12 23 NA NA NA ## 29: 2015 12 24 NA NA NA ## 30: 2015 12 25 NA NA NA ## 31: 2015 12 26 NA NA NA ## 32: 2015 12 27 NA NA NA ## 33: 2015 12 28 NA NA NA ## 34: 2015 12 29 NA NA NA ## 35: 2015 12 30 NA NA NA ## 36: 2015 12 31 NA NA NA ## year month day CloudCover Events Max.Dew.PointF Max.Gust.SpeedMPH ## Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## 1: 100 30.30 58 10 ## 2: 93 30.31 56 10 ## 3: 87 30.02 76 10 ## 4: 78 30.06 79 10 ## 5: 89 29.86 76 10 ## 6: 82 30.25 73 10 ## 7: NA NA NA NA ## 8: NA NA NA NA ## 9: NA NA NA NA ## 10: NA NA NA NA ## 11: NA NA NA NA ## 12: NA NA NA NA ## 13: NA NA NA NA ## 14: NA NA NA NA ## 15: NA NA NA NA ## 16: NA NA NA NA ## 17: NA NA NA NA ## 18: NA NA NA NA ## 19: NA NA NA NA ## 20: NA NA NA NA ## 21: NA NA NA NA ## 22: NA NA NA NA ## 23: NA NA NA NA ## 24: NA NA NA NA ## 25: NA NA NA NA ## 26: NA NA NA NA ## 27: NA NA NA NA ## 28: NA NA NA NA ## 29: NA NA NA NA ## 30: NA NA NA NA ## 31: NA NA NA NA ## 32: NA NA NA NA ## 33: NA NA NA NA ## 34: NA NA NA NA ## 35: NA NA NA NA ## 36: NA NA NA NA ## Max.Humidity Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## 1: 16 79 30.23 54 ## 2: 14 82 30.24 52 ## 3: 14 68 29.99 69 ## 4: 15 65 30.02 74 ## 5: 15 65 29.80 64 ## 6: 16 57 30.13 60 ## 7: NA NA NA NA ## 8: NA NA NA NA ## 9: NA NA NA NA ## 10: NA NA NA NA ## 11: NA NA NA NA ## 12: NA NA NA NA ## 13: NA NA NA NA ## 14: NA NA NA NA ## 15: NA NA NA NA ## 16: NA NA NA NA ## 17: NA NA NA NA ## 18: NA NA NA NA ## 19: NA NA NA NA ## 20: NA NA NA NA ## 21: NA NA NA NA ## 22: NA NA NA NA ## 23: NA NA NA NA ## 24: NA NA NA NA ## 25: NA NA NA NA ## 26: NA NA NA NA ## 27: NA NA NA NA ## 28: NA NA NA NA ## 29: NA NA NA NA ## 30: NA NA NA NA ## 31: NA NA NA NA ## 32: NA NA NA NA ## 33: NA NA NA NA ## 34: NA NA NA NA ## 35: NA NA NA NA ## 36: NA NA NA NA ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## 1: 8 10 48 43 ## 2: 10 7 45 43 ## 3: 10 6 57 54 ## 4: 10 9 62 59 ## 5: 10 8 51 48 ## 6: 10 8 42 40 ## 7: NA NA NA NA ## 8: NA NA NA NA ## 9: NA NA NA NA ## 10: NA NA NA NA ## 11: NA NA NA NA ## 12: NA NA NA NA ## 13: NA NA NA NA ## 14: NA NA NA NA ## 15: NA NA NA NA ## 16: NA NA NA NA ## 17: NA NA NA NA ## 18: NA NA NA NA ## 19: NA NA NA NA ## 20: NA NA NA NA ## 21: NA NA NA NA ## 22: NA NA NA NA ## 23: NA NA NA NA ## 24: NA NA NA NA ## 25: NA NA NA NA ## 26: NA NA NA NA ## 27: NA NA NA NA ## 28: NA NA NA NA ## 29: NA NA NA NA ## 30: NA NA NA NA ## 31: NA NA NA NA ## 32: NA NA NA NA ## 33: NA NA NA NA ## 34: NA NA NA NA ## 35: NA NA NA NA ## 36: NA NA NA NA ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## 1: 57 30.12 49 0 ## 2: 71 30.19 47 10 ## 3: 49 29.95 61 10 ## 4: 52 29.96 69 10 ## 5: 41 29.74 51 10 ## 6: 31 30.06 47 10 ## 7: NA NA NA NA ## 8: NA NA NA NA ## 9: NA NA NA NA ## 10: NA NA NA NA ## 11: NA NA NA NA ## 12: NA NA NA NA ## 13: NA NA NA NA ## 14: NA NA NA NA ## 15: NA NA NA NA ## 16: NA NA NA NA ## 17: NA NA NA NA ## 18: NA NA NA NA ## 19: NA NA NA NA ## 20: NA NA NA NA ## 21: NA NA NA NA ## 22: NA NA NA NA ## 23: NA NA NA NA ## 24: NA NA NA NA ## 25: NA NA NA NA ## 26: NA NA NA NA ## 27: NA NA NA NA ## 28: NA NA NA NA ## 29: NA NA NA NA ## 30: NA NA NA NA ## 31: NA NA NA NA ## 32: NA NA NA NA ## 33: NA NA NA NA ## 34: NA NA NA NA ## 35: NA NA NA NA ## 36: NA NA NA NA ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## PrecipitationIn WindDirDegrees date weather_boston4 ## 1: 0 72 2015-05-18 0 ## 2: 0 90 2015-06-03 0 ## 3: 0 45 2015-08-08 0 ## 4: 0 54 2015-09-01 0 ## 5: 0 199 2015-10-12 0 ## 6: 0 281 2015-11-03 0 ## 7: NA NA 2015-12-02 NA ## 8: NA NA 2015-12-03 NA ## 9: NA NA 2015-12-04 NA ## 10: NA NA 2015-12-05 NA ## 11: NA NA 2015-12-06 NA ## 12: NA NA 2015-12-07 NA ## 13: NA NA 2015-12-08 NA ## 14: NA NA 2015-12-09 NA ## 15: NA NA 2015-12-10 NA ## 16: NA NA 2015-12-11 NA ## 17: NA NA 2015-12-12 NA ## 18: NA NA 2015-12-13 NA ## 19: NA NA 2015-12-14 NA ## 20: NA NA 2015-12-15 NA ## 21: NA NA 2015-12-16 NA ## 22: NA NA 2015-12-17 NA ## 23: NA NA 2015-12-18 NA ## 24: NA NA 2015-12-19 NA ## 25: NA NA 2015-12-20 NA ## 26: NA NA 2015-12-21 NA ## 27: NA NA 2015-12-22 NA ## 28: NA NA 2015-12-23 NA ## 29: NA NA 2015-12-24 NA ## 30: NA NA 2015-12-25 NA ## 31: NA NA 2015-12-26 NA ## 32: NA NA 2015-12-27 NA ## 33: NA NA 2015-12-28 NA ## 34: NA NA 2015-12-29 NA ## 35: NA NA 2015-12-30 NA ## 36: NA NA 2015-12-31 NA ## PrecipitationIn WindDirDegrees date weather_boston4 Now, we will look at one obvious error summary(weather_boston4) ## year month day CloudCover ## Min. :2014 Min. : 1.000 Min. : 1.00 Min. :0.000 ## 1st Qu.:2015 1st Qu.: 4.000 1st Qu.: 8.00 1st Qu.:3.000 ## Median :2015 Median : 7.000 Median :16.00 Median :5.000 ## Mean :2015 Mean : 6.955 Mean :15.74 Mean :4.708 ## 3rd Qu.:2015 3rd Qu.:10.000 3rd Qu.:23.00 3rd Qu.:7.000 ## Max. :2015 Max. :12.000 Max. :31.00 Max. :8.000 ## NA&#39;s :30 ## Events Max.Dew.PointF Max.Gust.SpeedMPH Max.Humidity ## Length:396 Min. :-6.00 Min. : 0.00 Min. : 39.00 ## Class :character 1st Qu.:32.00 1st Qu.:21.00 1st Qu.: 73.25 ## Mode :character Median :47.50 Median :25.50 Median : 86.00 ## Mean :45.48 Mean :26.99 Mean : 85.69 ## 3rd Qu.:61.00 3rd Qu.:31.25 3rd Qu.: 93.00 ## Max. :75.00 Max. :94.00 Max. :1000.00 ## NA&#39;s :30 NA&#39;s :36 NA&#39;s :30 ## Max.Sea.Level.PressureIn Max.TemperatureF Max.VisibilityMiles ## Min. :29.58 Min. :18.00 Min. : 2.000 ## 1st Qu.:30.00 1st Qu.:42.00 1st Qu.:10.000 ## Median :30.14 Median :60.00 Median :10.000 ## Mean :30.16 Mean :58.93 Mean : 9.907 ## 3rd Qu.:30.31 3rd Qu.:76.00 3rd Qu.:10.000 ## Max. :30.88 Max. :96.00 Max. :10.000 ## NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 ## Max.Wind.SpeedMPH Mean.Humidity Mean.Sea.Level.PressureIn Mean.TemperatureF ## Min. : 8.00 Min. :28.00 Min. :29.49 Min. : 8.00 ## 1st Qu.:16.00 1st Qu.:56.00 1st Qu.:29.87 1st Qu.:36.25 ## Median :20.00 Median :66.00 Median :30.03 Median :53.50 ## Mean :20.62 Mean :66.02 Mean :30.04 Mean :51.40 ## 3rd Qu.:24.00 3rd Qu.:76.75 3rd Qu.:30.19 3rd Qu.:68.00 ## Max. :38.00 Max. :98.00 Max. :30.77 Max. :84.00 ## NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 ## Mean.VisibilityMiles Mean.Wind.SpeedMPH MeanDew.PointF Min.DewpointF ## Min. :-1.000 Min. : 4.00 Min. :-11.00 Min. :-18.00 ## 1st Qu.: 8.000 1st Qu.: 8.00 1st Qu.: 24.00 1st Qu.: 16.25 ## Median :10.000 Median :10.00 Median : 41.00 Median : 35.00 ## Mean : 8.861 Mean :10.68 Mean : 38.96 Mean : 32.25 ## 3rd Qu.:10.000 3rd Qu.:13.00 3rd Qu.: 56.00 3rd Qu.: 51.00 ## Max. :10.000 Max. :22.00 Max. : 71.00 Max. : 68.00 ## NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 ## Min.Humidity Min.Sea.Level.PressureIn Min.TemperatureF Min.VisibilityMiles ## Min. :16.00 Min. :29.16 Min. :-3.00 Min. : 0.000 ## 1st Qu.:35.00 1st Qu.:29.76 1st Qu.:30.00 1st Qu.: 2.000 ## Median :46.00 Median :29.94 Median :46.00 Median :10.000 ## Mean :48.31 Mean :29.93 Mean :43.33 Mean : 6.716 ## 3rd Qu.:60.00 3rd Qu.:30.09 3rd Qu.:60.00 3rd Qu.:10.000 ## Max. :96.00 Max. :30.64 Max. :74.00 Max. :10.000 ## NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 ## PrecipitationIn WindDirDegrees date weather_boston4 ## Min. :0.0000 Min. : 1.0 Min. :2014-12-01 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:113.0 1st Qu.:2015-03-09 1st Qu.:0.0000 ## Median :0.0000 Median :222.0 Median :2015-06-16 Median :0.0000 ## Mean :0.1016 Mean :200.1 Mean :2015-06-16 Mean :0.1016 ## 3rd Qu.:0.0400 3rd Qu.:275.0 3rd Qu.:2015-09-23 3rd Qu.:0.0400 ## Max. :2.9000 Max. :360.0 Max. :2015-12-31 Max. :2.9000 ## NA&#39;s :30 NA&#39;s :30 NA&#39;s :30 you may notice the max humidity is 1000, which is an obvious error since humidity is measured in percentage from 1-100. In fact, this is a input error, and the value should be 100. We can make a histogram for Max.Humidity. hist(weather_boston4$Max.Humidity,breaks=100) We can also make a boxplot for Max.Humidity. boxplot(weather_boston4$Max.Humidity) Change the Max.Humidity from 1000 to 100. weather_boston4[Max.Humidity==1000,Max.Humidity:=100] hist(weather_boston4$Max.Humidity) In the above code, Max.Humidity==1000 selects the rows which Max.Humidity==1000, and Max.Humidity:=100 updates the Max.Humidity for the selected rows. Another obvious error: Youve discovered and repaired one obvious error in the data, but it appears that theres another. Sometimes you get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day summary(weather_boston4$Mean.VisibilityMiles) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -1.000 8.000 10.000 8.861 10.000 10.000 30 We cannot have negative visibility miles. This is likely to be an input error and it should 1. Lets change -1 to 1. weather_boston4[Mean.VisibilityMiles==-1, Mean.VisibilityMiles:=1] hist(weather_boston4$Mean.VisibilityMiles) Check other possible extreme values # Look at histogram for MeanDew.PointF hist(weather_boston4$MeanDew.PointF) # Look at histogram for Min.TemperatureF hist(weather_boston4$Min.TemperatureF) # Compare to histogram for Mean.TemperatureF hist(weather_boston4$Mean.TemperatureF) 13.4.4 Finishing touches Before officially calling our weather_boston data clean, we want to put a couple of finishing touches on the data. These are a bit more subjective and may not be necessary for analysis, but they will make the data easier for others to interpret, which is generally a good thing. There are a number of stylistic conventions in the R language. Depending on who you ask, these conventions may vary. Because the period (.) has special meaning in certain situations, we generally recommend using underscores (_) to separate words in variable names. We also prefer all lowercase letters so that no one has to remember which letters are uppercase or lowercase. Finally, the events column (renamed to be all lowercase in the first instruction) contains an empty string (\"\") for any day on which there was no significant weather event such as rain, fog, a thunderstorm, etc. However, if its the first time youre seeing these data, it may not be obvious that this is the case, so its best for us to be explicit and replace the empty strings with something more meaningful. # Clean up column names names(weather_boston4) = tolower(names(weather_boston4)) names(weather_boston4)=str_replace_all(names(weather_boston4),&quot;\\\\.&quot;,&quot;_&quot;) Note that str_replace_all(names(weather_boston4),.,\") will not replace \".\" with \" becuase . is a special operator in R, we need to use \\ in front of . such that the code know we are not refer . as the special operator, but literal . # Replace empty cells in events column weather_boston4[events == &quot;&quot;, events:= &quot;none&quot;] table(weather_boston4$events) ## ## Fog Fog-Rain ## 6 8 ## Fog-Rain-Hail-Thunderstorm Fog-Rain-Snow ## 1 4 ## Fog-Rain-Thunderstorm Fog-Snow ## 3 7 ## none Rain ## 231 90 ## Rain-Snow Rain-Thunderstorm ## 10 4 ## Snow Thunderstorm ## 31 1 Print the first 6 rows of weather_boston4. Enjoy examining your great accomplishment of cleaning a messy data! head(weather_boston4[order(year,month,day)]) ## year month day cloudcover events max_dew_pointf max_gust_speedmph ## 1: 2014 12 1 6 Rain 46 29 ## 2: 2014 12 2 7 Rain-Snow 40 29 ## 3: 2014 12 3 8 Rain 49 38 ## 4: 2014 12 4 3 none 24 33 ## 5: 2014 12 5 5 Rain 37 26 ## 6: 2014 12 6 8 Rain 45 25 ## max_humidity max_sea_level_pressurein max_temperaturef max_visibilitymiles ## 1: 74 30.45 64 10 ## 2: 92 30.71 42 10 ## 3: 100 30.40 51 10 ## 4: 69 30.56 43 10 ## 5: 85 30.68 42 10 ## 6: 100 30.42 45 10 ## max_wind_speedmph mean_humidity mean_sea_level_pressurein mean_temperaturef ## 1: 22 63 30.13 52 ## 2: 24 72 30.59 38 ## 3: 29 79 30.07 44 ## 4: 25 54 30.33 37 ## 5: 22 66 30.59 34 ## 6: 22 93 30.24 42 ## mean_visibilitymiles mean_wind_speedmph meandew_pointf min_dewpointf ## 1: 10 13 40 26 ## 2: 8 15 27 17 ## 3: 5 12 42 24 ## 4: 10 12 21 13 ## 5: 10 10 25 12 ## 6: 4 8 40 36 ## min_humidity min_sea_level_pressurein min_temperaturef min_visibilitymiles ## 1: 52 30.01 39 10 ## 2: 51 30.40 33 2 ## 3: 57 29.87 37 1 ## 4: 39 30.09 30 10 ## 5: 47 30.45 26 5 ## 6: 85 30.16 38 0 ## precipitationin winddirdegrees date weather_boston4 ## 1: 0.01 268 2014-12-01 0.01 ## 2: 0.10 62 2014-12-02 0.10 ## 3: 0.44 254 2014-12-03 0.44 ## 4: 0.00 292 2014-12-04 0.00 ## 5: 0.11 61 2014-12-05 0.11 ## 6: 1.09 313 2014-12-06 1.09 Remove intermediate datasets to save memory space. This is particular important when working with big data. remove(weather_boston2, weather_boston3) 13.5 Summary We go through the three steps to clean data with an example of weather_boston: 1) exploring raw data to for diagnosis; 2) tidy the data using melt() and dcast() 3) convert columns into the right variable type. We use melt() and dcast() to tidy the data. Convert variables into the right variable type using teh data.table syntax. In real work, we may encounter different messy data. We should use the learned knowledge to describe the messy data, and clean the data. "],["visualization-with-ggplot2-i.html", "Chapter 14 Visualization with ggplot2 I 14.1 Scatter plot 14.2 line-fitting 14.3 Overplotting 14.4 Bar chart and statistical transformation 14.5 stacked par 14.6 Histogram 14.7 boxplot 14.8 line chart 14.9 use theme to customize the appearance of your chart. 14.10 Summary", " Chapter 14 Visualization with ggplot2 I Base R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. With ggplot2, you can do more faster by learning one system and applying it in many places. ggplot2 is probably the one of the most downloaded and well-known R-package. Install the ggplot2 package. install.packages(&quot;ggplot2&quot;) Load the package into R: library(ggplot2) library(data.table) For the purpose of illustration, we will use the mpg dataset bundled in ggplot2 packaged: mpg: This dataset contains Auto models which was released between 1999 and 2008. It shows their fuel economy data. Lets take a quick look at the mpg datasets: mpg=fread(&quot;data/mpg.csv&quot;) head(mpg) ## manufacturer model displ year cyl trans drv cty hwy fl class ## 1: audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2: audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3: audi a4 2.0 2008 4 manual(m6) f 20 31 p compact ## 4: audi a4 2.0 2008 4 auto(av) f 21 30 p compact ## 5: audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6: audi a4 2.8 1999 6 manual(m5) f 18 26 p compact str(mpg) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 234 obs. of 11 variables: ## $ manufacturer: chr &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; You may notice that mpg is a tibble (another extended version of data.frame). This is a data structure implement by Hadley Wickham as part of the tidyverse package. But as mentioned, we will stick to the data.table format. 14.1 Scatter plot Lets use our first graph to answer a question: do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? In the mpg dateset, displ measures a cars engine size, in cu.in.. hwy measures its highway fuel efficiency, in miles per gallon. We will make a scatter plot between hwy and displ. The scatter plot is the graph to show relationship between two variables. Lets see how to make scatter plot using base R. plot(mpg$displ,mpg$hwy) In base R, plot(x,y) plot a scatter plot with x on x-axis and y-axis; and displays the plot on the screen. However, ggplot2 creates a plot object that is very flexible to modify. Now, lets see the how to make scatter plot with ggplot2. ggplot(data = mpg, aes(x=displ, y=hwy)) + geom_point() As we can see, there is a negative linear relationship between hwy and displ. In other words, cars with big engines use more fuel. Here is what each part of the code does: ggplot(mpg, aes(x=displ, y=hwy)) will draw a blank canvas, using data.frame mpg. aes(displ, hwy) is the mapping argument, which maps data to the visual elements in the plot. In this case, it maps displ on x-axis and hwy on the y-axis. is telling R that the code is not finished and the remaining code is in next line. geom_point() plots scatter plot (i.e., points) on top the canvas, with x and y are inherited in ggplot(). The geom_point() is the function for creating scatter plot. Like geom_point(), there are other geom function to create bar charts, box charts, which we will talk later. The geom function like geom_point() takes the mapping argument, which maps the data (e.g., displ, hwy) to particular aesthetic elements of a chart (e.g., x-axis, y-axis). In the above code, geom_point() inherited the aes() from the ggplot(). Now, lets break the code see what happens: ggplot(data = mpg, aes(x=displ, y=hwy)) As mentioned, ggplot() creates an empty plot canvas, on which you can add plots layer by layer. Unlike base R, ggplot2 creates the plot as an object and we can easily modify that object. Now, we will save the ggplot() result and then modify it by adding layer. # create a plot object and save it to variable g g&lt;-ggplot(data = mpg, aes(x=displ, y=hwy)) # modify variable g by adding the scatter plot layer g + geom_point() As see, plots can be saved as variables, which can be added two later on using the + operator. This is really useful if you want to make multiple related plots from a common base. A general template for making graphs with ggplot2 is as follow. To make a graph, replace the bracketed sections in the following code with a dataset, a geom function, and a collection of aesthetic mappings: ggplot(data = , mapping = aes()) + (data = , mapping = aes()) Notice that each GEOM_FUNCTION can also have its own data and aesthetic mapping to customize its corresponding layer. This provides a very flexible way of customize your charts. The rest of this chapter will show you how to complete and extend this template to make different types of graphs. Scatterplot is great to visualize relationship between two variables. But sometime we want to show three or even more variable at the same time. For example, we want to see how auto class (class) affects the relationship between hwy and displ. You can convey this information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. ggplot(data = mpg, aes(x=displ, y= hwy, color=class)) + geom_point() This chart is quite revealing because we see that 2seater seems to be the outlier of the linear relationship between hwy/displ. This is mostly because 2seater is like roadster with big engines. These are the aesthetics you can consider within aes() in this chapter: x, y, color, fill, size, alpha, and shape. One common convention is that you dont name the x and y arguments to aes(), since they almost always come first, but you do name other arguments. If we instead assign color to cyl variable (# of cylinder), we see the legend is on a continuous scale from 4-8. But we know cylinder only takes discrete value. We need to convert cyl to factor (the data type to represent categorical variable in R) and the resulting chart is more informative. ggplot(data = mpg, aes(displ, hwy, color=cyl)) + geom_point() ggplot(data = mpg, aes(displ, hwy, color=as.factor(cyl))) + geom_point() Notice that how different data type would affect the appearance of the chart. It is very critical to prepare the data in the right format before any analysis. We have talked about cleaning data in the previous chapter. Next, we want to customize the x/y-axis and legend to make it more informative ggplot(data = mpg, aes(displ, hwy, color=class)) + geom_point()+ labs(x=&quot;displacement&quot;, y=&quot;highway miles per gallon&quot;, color=&quot;type of vehicle&quot;) We can also use the shape of point to indicate the class. ggplot(data = mpg, aes( displ, hwy, shape=class)) + geom_point()+ labs(x=&quot;displacement&quot;, y=&quot;miles per gallon&quot;, shape=&quot;type of vehicle&quot;) ## Warning: The shape palette can deal with a maximum of 6 discrete values because ## more than 6 becomes difficult to discriminate; you have 7. Consider ## specifying shapes manually if you must have them. ## Warning: Removed 62 rows containing missing values (geom_point). Note: that shape can deal with a maximum of 6 discrete values; try to use color if you have more than 6 discrete value for a categorical variable. If we want to include a continuous variable into the chart, i.e., including cty (miles per gallon in city) of a car, we can use the size aes() element. ggplot(data = mpg, aes( displ, hwy, size=cty)) + geom_point()+ labs(x=&quot;displacement&quot;, y=&quot;miles per gallon&quot;) + theme(legend.position = &quot;none&quot;) # remove the legend In this plot, the size of the circle represents the cty of the car. One way to add additional variables is with aesthetics (e.g., color, fill, shape, size). Another way, particularly useful for categorical variables, is to split your plot into facets (i.e., subplots) each of which displays one subset of the data. It also reduces the complexity of a chart and allow readers to focus on one category at a time. ggplot(data = mpg, aes(displ, hwy, color=class)) + geom_point()+ labs(x=&quot;displacement&quot;, y=&quot;miles per gallon&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ class, nrow = 3) This chart clearly shows that compact has great advantage in fuel economy. To facet your plot on the combination of two discrete variables, add facet_grid() to your plot call. The first argument of facet_grid() is a formula. This time the formula should contain two variable names separated by a ~: ggplot(data = mpg) + geom_point(mapping = aes(displ, hwy)) + facet_grid(class ~ trans) # class ~ trans: indicates to break the chart into a grid where y-axis is class and x-axis is trans (4, 5, 6, 8) 14.2 line-fitting Next, we will add a fitted line to the scatter plot. The fitted line is a great way to visualize the potential linear/nonlinear relationship between two variables. To add a fitted line, we simply include the geom_smooth() on top of the chart. ggplot(data = mpg, aes(displ, hwy)) + geom_point()+ geom_smooth() # fit scatter plot using the default method LOESS (locally estimated scatterplot smoothing) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The band round the fitted line is the 95% prediction interval (i.e., with 95% probability, the hwy for an Auto with given displ should fall in such interval). To remove the interval, we can set se=FALSE as below: ggplot(data = mpg, aes(displ, hwy)) + geom_point()+ geom_smooth(se=FALSE) # fit scatter plot using the default method LOESS (locally estimated scatterplot smoothing) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The fitted curve is bended upwards, due to the few outlier on the up-right corner (i.e., the 2seaters auto class). This creates a FALSE visual impression that hwy actually is improved when the engine is sufficiently large. To correct such FALSE impression, we can remove the outliers when adding the geom_smooth() layer. ggplot(data = mpg, aes(displ, hwy)) + geom_point()+ geom_smooth(data = mpg[mpg$class!=&quot;2seater&quot;,], aes(x=displ, y=hwy)) # remove the 2seater class as outlier when fitting the scatter plot ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Once the outliers are removed, the fitted line is no longer bended upwords. In the code above, geom_smooth() override the data and aesthetic inherited from ggplot(), and use its own data and aesthetic. This shows the great flexibility of ggplot2. In many case, we want to fit the scatter plot with linear line due to is clear interpretation. We can easily do that by changing the fitting method to lm. ggplot(data = mpg, aes(displ, hwy)) + geom_point()+ geom_smooth(se=FALSE, method=&quot;lm&quot;)+ # fit the scatter plot with linear line geom_smooth(data = mpg[mpg$class!=&quot;2seater&quot;,], aes(x=displ, y=hwy),method=&quot;lm&quot;, se=FALSE, color=&quot;red&quot;) # fit the scatter plot with linear line without outliers ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; If you have multiple geoms, then mapping an aesthetic to data variable inside the call to ggplot() will change all the geoms. It is also possible to make changes to individual geoms by passing arguments to the geom_*() functions. 14.3 Overplotting Did you notice that the plot displays only 126 points, even though there are 234 observations in the dataset? Also, no a single point overlay each other in the above chart. The values of hwy and displ are rounded so the points appear on a grid and many points overlap each other. This problem is known as overplotting. You can avoid this gridding by setting the position adjustment to jitter. position = jitter adds a small amount of random noise to each point. This spreads the points out because no two points are likely to receive the same amount of random noise: ggplot(mpg, aes(displ, hwy)) + geom_point(position=&quot;jitter&quot;, alpha=0.4) # alpha sets the transparency of the point geom_point() has an alpha argument that controls the opacity of the points. A value of 1 (the default) means that the points are totally opaque; a value of 0 means the points are totally transparent (and therefore invisible). Values in between specify transparency. we must always consider overplotting, particularly in the following four situations: Large datasets Aligned values on a single axis Low-precision data Integer data For example, the diamonds datast (also bundled with ggplot2 package) contains over 50,000 diamonds in terms their price, carat, clarity .. You can plot the scatter plot for between carat and price for the diamonds dataset. ggplot(diamonds,aes(x=carat, y=price))+ geom_point() This is a very large dataset, it is a best practice to set position=jitter to aviod over ploting. ggplot(diamonds,aes(x=carat, y=price))+ geom_point(position=&quot;jitter&quot;, alpha=0.2) As seen in above chart, the jitter plot gives a strong visual clue of where most data is clustered. We can also customize the random disturbance in the point position using position_fitter(width), where width indicates how much random disturbance in the points. ggplot(diamonds,aes(x=carat, y=price))+ geom_point(position = position_jitter(0.2), alpha=0.1) # setting the width to 0.2 14.4 Bar chart and statistical transformation Now, suppose we want to plot the number of Auto produced by each manufacturer using a bar chart. This can be done directly with geom_bar() function. ggplot(data=mpg)+ geom_bar(aes(x=manufacturer)) On the x-axis, the chart displays manufacturer, a variable from mpg dataset. On the y-axis, it displays count, but count is not a variable in mpg! Where does count come from? Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot: Bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin. geom_smooth fit a model to your data and then plot predictions from the model. Boxplots compute a robust summary of the distribution and display a specially formatted box. The algorithm used to calculate new values for a graph is called a stat, short for statistical transformation. Lets look at one example. The sample data shows the teaching evaluation of the three SCMA courses. myData=data.frame(course=c(&quot;SCMA350&quot;, &quot;SCMA450&quot;,&quot;SCMA451&quot;), evaluation=c(3.8,4.1,4.1)) myData ## course evaluation ## 1 SCMA350 3.8 ## 2 SCMA450 4.1 ## 3 SCMA451 4.1 We want to plot the evaluation score of each course on a bar chart. # this will generate a count for each category and plot count on y-axis. ggplot(data=myData)+ geom_bar(mapping=aes(x=course)) The y-axis is count; in other words, the code above counts the occurance of each course and plot that on the y-axis. To plot the evaluation data as it is, we need to set the y-axis as evaluation and set stat function as identity. ggplot(data=myData)+ geom_bar(mapping=aes(x=course, y=evaluation), stat = &quot;identity&quot;) stat = identity tells the program to plot the data as it is, rather than generating the counts. Now, plot a bar chart to show the average hwy for each manufacturer. In this case, we need to use the stat function for computing average. # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;) We can switch the x-axis and y-axis of a vertical bar chart so that the label for each manufacturer does not overlay each other. # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;) + labs(x=&quot;manufacturer&quot;, y=&quot;highway miles per gallon&quot;)+ # add x/y labels coord_flip() Or we can switch the x-y axis in the aes() mapping: # Plot a horizontal bar chart ggplot(mpg, aes(x = hwy, y = manufacturer)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;) + labs(x=&quot;highway miles per gallon&quot;, y=&quot;manufacturer&quot;) # add x/y labels We can add more detail information to the chart by showing each points through geom_point() # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;, alpha=0.3)+ # add points to show the hwy of each Auto under the same manufacturer geom_point(position=position_jitter(width = 0.1), alpha=0.5)+ labs(x=&quot;manufacturer&quot;, y=&quot;highway miles per gallon&quot;)+ # add x/y labels coord_flip() 14.5 stacked par In some cases, we want to show the composition of each bar. Stacked bar chart is for this purpose. ggplot(mpg, aes(y = manufacturer, fill = class))+ geom_bar() The above charts shows how many types of car each manufacturer has. We can also stack bar side-by-side. ggplot(mpg, aes(x =hwy, y = manufacturer, fill=class)) + stat_summary(fun = mean, geom = &quot;bar&quot;, position = position_dodge2(width = 4, preserve = &quot;single&quot;)) Again, we can facet the plot into subplot to show each sub-category. ggplot(mpg, aes(x =hwy, y = manufacturer, fill=class))+ stat_summary(fun = mean, geom = &quot;bar&quot;)+ facet_wrap(~class, nrow=7) ggsave(&quot;data/Mydata.png&quot;, width =10, height = 27, units=&#39;cm&#39;) 14.6 Histogram Histogram is a great to see the distribution of a variable. ggplot(mpg, mapping=aes(x=hwy)) + geom_histogram(binwidth=1, alpha=0.8, color=&quot;black&quot;, fill=&quot;gray&quot;) Histogram by group using facet_wrap() function ggplot(mpg, mapping=aes(x=hwy)) + geom_histogram(binwidth=1, color=&quot;black&quot;, fill=&quot;gray&quot;)+ facet_wrap(~ drv, nrow = 3) + coord_fixed(ratio=0.3) 14.7 boxplot Boxplot is a great way to show the median as well as its spread of a variable. ggplot(mpg, mapping=aes(x=class, y=hwy)) + geom_boxplot() 14.8 line chart Line chart is usually used to show how a variable change over time. We will use the economics dataset which contains the US economic time series. head(economics) ## # A tibble: 6 x 6 ## date pce pop psavert uempmed unemploy ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967-07-01 507. 198712 12.6 4.5 2944 ## 2 1967-08-01 510. 198911 12.6 4.7 2945 ## 3 1967-09-01 516. 199113 11.9 4.6 2958 ## 4 1967-10-01 512. 199311 12.9 4.9 3143 ## 5 1967-11-01 517. 199498 12.8 4.7 3066 ## 6 1967-12-01 525. 199657 11.8 4.8 3018 Use a line chart to show how uempmed(unemployment rate) evolve over time: ggplot(economics, aes(date, uempmed)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;unemployment rate&quot;) Use a line chart to show how pce(personal consumption expenditures, in billions $) evolve over time: ggplot(economics, aes(date, pce)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;personal consumption expenditures&quot;) 14.9 use theme to customize the appearance of your chart. Many plot elements have multiple properties that can be set. For example, line elements in the plot such as axes and gridlines have a color, a thickness (size), and a line type (solid line, dashed, or dotted). To set the style of a line, you use element_line(). For example, to make the axis lines into red, dashed lines, you would use the following. g=ggplot(economics, aes(date, pce)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;personal consumption expenditures&quot;) g + theme(axis.line = element_line(color = &quot;red&quot;, linetype = &quot;solid&quot;)) Built-in themes: In addition to making your own themes, there are several out-of-the-box solutions that may save you lots of time. theme_gray() is the default. theme_bw() is useful when you use transparency. theme_classic() is more traditional. theme_void() removes everything but the data. The theme_classic() is the my go-to-theme. g+theme_classic() When the x/y axis goes too high, we may want to compress the axis through log-transformation. g+ theme_minimal() + scale_y_continuous(trans=&#39;log10&#39;) # this code log transform y axis 14.10 Summary The general syntax for ggplot() is: ggplot(data = DATA, mapping = aes(MAPPINGS)) + GEOM_FUNCTION(data = DATA, mapping = aes(MAPPINGS)) To make a graph, replace the bold sections in the above code with a dataset, a geom function, and a collection of aesthetic mappings. We use geom_point() function to make scatter plot: Scatter plot can plot two variables on the x/y axis. To include more information on scatter plot, we can use aesthetic (e.g., color, fill, shape, size). It is the best practice to map discrete variables to color, fill, shape, while continuous variable to size. We can also use facet to create sub-plot for each sub-category. Use geom_smooth() to add fitted line to the scatter plot. Set position=jitter to add random disturbance to avoid overplotting. "],["visualization-with-ggplot2-ii.html", "Chapter 15 Visualization with ggplot2 II 15.1 Bar chart and statistical transformation 15.2 stacked par 15.3 Histogram 15.4 boxplot 15.5 line chart 15.6 use theme to customize the appearance of your chart. 15.7 Summary", " Chapter 15 Visualization with ggplot2 II In previous chapter, we learn the general syntax for ggplot() as below: ggplot(data = DATA, mapping = aes(MAPPINGS)) + GEOM_FUNCTION(data = DATA, mapping = aes(MAPPINGS)) To make a graph, replace the bold sections in the above code with a dataset, a geom function, and a collection of aesthetic mappings. We have learned the geom_functions, geom_point() and geom_smooth() for making scatter plot and adding fitted line on scatter plot. Today, we will introduce geom_bar(), geom_line, geom_box for making bar/line/box plot. We will introduce how to use theme() function to beautify the plots. 15.1 Bar chart and statistical transformation Now, suppose we want to plot the number of Auto produced by each manufacturer using a bar chart. This can be done directly with geom_bar() function. ggplot(data=mpg)+ geom_bar(aes(x=manufacturer)) On the x-axis, the chart displays manufacturer, a variable from mpg dataset. On the y-axis, it displays count, but count is not a variable in mpg! Where does count come from? Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot: Bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin. geom_smooth fit a model to your data and then plot predictions from the model. Boxplots compute a robust summary of the distribution and display a specially formatted box. The algorithm used to calculate new values for a graph is called a stat, short for statistical transformation. Lets look at one example. The sample data shows the teaching evaluation of the three SCMA courses. myData=data.frame(course=c(&quot;SCMA350&quot;, &quot;SCMA450&quot;,&quot;SCMA451&quot;), evaluation=c(3.8,4.1,4.1)) myData ## course evaluation ## 1 SCMA350 3.8 ## 2 SCMA450 4.1 ## 3 SCMA451 4.1 We want to plot the evaluation score of each course on a bar chart. # this will generate a count for each category and plot count on y-axis. ggplot(data=myData)+ geom_bar(mapping=aes(x=course)) The y-axis is count; in other words, the code above counts the occurance of each course and plot that on the y-axis. To plot the evaluation data as it is, we need to set the y-axis as evaluation and set stat function as identity. ggplot(data=myData)+ geom_bar(mapping=aes(x=course, y=evaluation), stat = &quot;identity&quot;) stat = identity tells the program to plot the data as it is, rather than generating the counts. Now, plot a bar chart to show the average hwy for each manufacturer. In this case, we need to use the stat function for computing average. # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;) We can switch the x-axis and y-axis of a vertical bar chart so that the label for each manufacturer does not overlay each other. # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;) + labs(x=&quot;manufacturer&quot;, y=&quot;highway miles per gallon&quot;)+ # add x/y labels coord_flip() Or we can switch the x-y axis in the aes() mapping: # Plot a horizontal bar chart ggplot(mpg, aes(x = hwy, y = manufacturer)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;) + labs(x=&quot;highway miles per gallon&quot;, y=&quot;manufacturer&quot;) # add x/y labels We can add more detail information to the chart by showing each points through geom_point() # Plot a horizontal bar chart ggplot(mpg, aes(x = manufacturer , y = hwy)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;, color=&quot;skyblue&quot;, alpha=0.3)+ # add points to show the hwy of each Auto under the same manufacturer geom_point(position=position_jitter(width = 0.1), alpha=0.5)+ labs(x=&quot;manufacturer&quot;, y=&quot;highway miles per gallon&quot;)+ # add x/y labels coord_flip() 15.2 stacked par In some cases, we want to show the composition of each bar. Stacked bar chart is for this purpose. ggplot(mpg, aes(y = manufacturer, fill = class))+ geom_bar() The above charts shows how many types of car each manufacturer has. We can also stack bar side-by-side. ggplot(mpg, aes(x =hwy, y = manufacturer, fill=class)) + stat_summary(fun = mean, geom = &quot;bar&quot;, position = position_dodge2(width = 4, preserve = &quot;single&quot;)) Again, we can facet the plot into subplot to show each sub-category. ggplot(mpg, aes(x =hwy, y = manufacturer, fill=class))+ stat_summary(fun = mean, geom = &quot;bar&quot;)+ facet_wrap(~class, nrow=7) ggsave(&quot;data/Mydata.png&quot;, width =10, height = 27, units=&#39;cm&#39;) 15.3 Histogram Histogram is a great to see the distribution of a variable. ggplot(mpg, mapping=aes(x=hwy)) + geom_histogram(binwidth=1, alpha=0.8, color=&quot;black&quot;, fill=&quot;gray&quot;) Histogram by group using facet_wrap() function ggplot(mpg, mapping=aes(x=hwy)) + geom_histogram(binwidth=1, color=&quot;black&quot;, fill=&quot;gray&quot;)+ facet_wrap(~ drv, nrow = 3) + coord_fixed(ratio=0.3) 15.4 boxplot Boxplot is a great way to show the median as well as its spread of a variable. ggplot(mpg, mapping=aes(x=class, y=hwy)) + geom_boxplot() 15.5 line chart Line chart is usually used to show how a variable change over time. We will use the economics dataset which contains the US economic time series. head(economics) ## # A tibble: 6 x 6 ## date pce pop psavert uempmed unemploy ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967-07-01 507. 198712 12.6 4.5 2944 ## 2 1967-08-01 510. 198911 12.6 4.7 2945 ## 3 1967-09-01 516. 199113 11.9 4.6 2958 ## 4 1967-10-01 512. 199311 12.9 4.9 3143 ## 5 1967-11-01 517. 199498 12.8 4.7 3066 ## 6 1967-12-01 525. 199657 11.8 4.8 3018 Use a line chart to show how uempmed(unemployment rate) evolve over time: ggplot(economics, aes(date, uempmed)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;unemployment rate&quot;) Use a line chart to show how pce(personal consumption expenditures, in billions $) evolve over time: ggplot(economics, aes(date, pce)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;personal consumption expenditures&quot;) 15.6 use theme to customize the appearance of your chart. Many plot elements have multiple properties that can be set. For example, line elements in the plot such as axes and gridlines have a color, a thickness (size), and a line type (solid line, dashed, or dotted). To set the style of a line, you use element_line(). For example, to make the axis lines into red, dashed lines, you would use the following. g=ggplot(economics, aes(date, pce)) + geom_line(alpha = 0.7) + labs(x = &quot;date&quot;, y = &quot;personal consumption expenditures&quot;) g + theme(axis.line = element_line(color = &quot;red&quot;, linetype = &quot;solid&quot;)) Built-in themes: In addition to making your own themes, there are several out-of-the-box solutions that may save you lots of time. theme_gray() is the default. theme_bw() is useful when you use transparency. theme_classic() is more traditional. theme_void() removes everything but the data. The g+theme_minimal() is the my go-to-theme. g+theme_minimal() When the x/y axis goes too high, we may want to compress the axis through log-transformation. g+ theme_minimal() + scale_y_continuous(trans=&#39;log10&#39;) # this code log transform y axis 15.7 Summary we show how to make other types of charts, including, bar, boxplot. "],["visualization-with-ggplot2-iii.html", "Chapter 16 Visualization with ggplot2 III 16.1 Draw world map 16.2 Draw US map 16.3 draw US county map 16.4 Summary", " Chapter 16 Visualization with ggplot2 III In many cases, we work with location data. The best way to visualize the location data is map. To draw maps in R, we first need to install the map-related packages. install.packages(&quot;ggplot2&quot;) install.packages(&quot;maps&quot;) # maps packages contains world maps data install.packages(&quot;mapproj&quot;) # Converts latitude/longitude into projected coordinates library(ggplot2) library(maps) ## Warning: package &#39;maps&#39; was built under R version 4.0.5 library(mapproj) ## Warning: package &#39;mapproj&#39; was built under R version 4.0.5 We will also be using data.table and lubridate packages to clean data; and curl package for download data directly from website. library(lubridate) library(data.table) library(curl) library(ggplot2) 16.1 Draw world map First, we get the world map data from the map package. # map_data() is a function in maps package to load the map data world_map=as.data.table(map_data(&quot;world&quot;)) head(world_map) ## long lat group order region subregion ## 1: -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2: -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3: -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4: -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5: -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6: -70.05088 12.59707 1 6 Aruba &lt;NA&gt; Second, we can use ggplot2 to plot the world map: ggplot() + geom_polygon(data=world_map, aes(long, lat, group=group), fill=&quot;gray&quot;,color=&quot;black&quot;)+ geom_path()+ coord_map(xlim=c(-180,180)) Next, we want to add information of each country on the map. For example, we want to show the number of confirmed COVID-19 cases. The csv format data can be access through from the following website: covid_world=fread(&quot;https://covid.ourworldindata.org/data/owid-covid-data.csv&quot;) Lets first take a look at the data: covid_world=covid_world[,.(iso_code,continent,location,date,total_cases,new_cases,total_deaths,new_deaths,total_tests_per_thousand,people_vaccinated_per_hundred)] head(covid_world) ## iso_code continent location date total_cases new_cases total_deaths ## 1: AFG Asia Afghanistan 2020-02-24 1 1 NA ## 2: AFG Asia Afghanistan 2020-02-25 1 0 NA ## 3: AFG Asia Afghanistan 2020-02-26 1 0 NA ## 4: AFG Asia Afghanistan 2020-02-27 1 0 NA ## 5: AFG Asia Afghanistan 2020-02-28 1 0 NA ## 6: AFG Asia Afghanistan 2020-02-29 1 0 NA ## new_deaths total_tests_per_thousand people_vaccinated_per_hundred ## 1: NA NA NA ## 2: NA NA NA ## 3: NA NA NA ## 4: NA NA NA ## 5: NA NA NA ## 6: NA NA NA str(covid_world) ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 114995 obs. of 10 variables: ## $ iso_code : chr &quot;AFG&quot; &quot;AFG&quot; &quot;AFG&quot; &quot;AFG&quot; ... ## $ continent : chr &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; &quot;Asia&quot; ... ## $ location : chr &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; ... ## $ date : IDate, format: &quot;2020-02-24&quot; &quot;2020-02-25&quot; ... ## $ total_cases : num 1 1 1 1 1 1 1 1 2 4 ... ## $ new_cases : num 1 0 0 0 0 0 0 0 1 2 ... ## $ total_deaths : num NA NA NA NA NA NA NA NA NA NA ... ## $ new_deaths : num NA NA NA NA NA NA NA NA NA NA ... ## $ total_tests_per_thousand : num NA NA NA NA NA NA NA NA NA NA ... ## $ people_vaccinated_per_hundred: num NA NA NA NA NA NA NA NA NA NA ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; summary(covid_world) ## iso_code continent location date ## Length:114995 Length:114995 Length:114995 Min. :2020-01-01 ## Class :character Class :character Class :character 1st Qu.:2020-07-25 ## Mode :character Mode :character Mode :character Median :2020-12-18 ## Mean :2020-12-11 ## 3rd Qu.:2021-05-02 ## Max. :2021-09-06 ## ## total_cases new_cases total_deaths new_deaths ## Min. : 1 Min. :-74347 Min. : 1 Min. :-1918 ## 1st Qu.: 1776 1st Qu.: 2 1st Qu.: 63 1st Qu.: 0 ## Median : 17999 Median : 86 Median : 542 Median : 2 ## Mean : 1323061 Mean : 6389 Mean : 34173 Mean : 146 ## 3rd Qu.: 192856 3rd Qu.: 892 3rd Qu.: 4713 3rd Qu.: 18 ## Max. :221211222 Max. :905932 Max. :4576372 Max. :17977 ## NA&#39;s :5536 NA&#39;s :5539 NA&#39;s :16134 NA&#39;s :15979 ## total_tests_per_thousand people_vaccinated_per_hundred ## Min. : 0.00 Min. : 0.00 ## 1st Qu.: 18.03 1st Qu.: 3.30 ## Median : 91.11 Median : 15.11 ## Mean : 401.32 Mean : 23.45 ## 3rd Qu.: 375.29 3rd Qu.: 40.29 ## Max. :13230.12 Max. :117.71 ## NA&#39;s :65196 NA&#39;s :91556 We need to clean the date into the correct format before any analysis: Examine extreme values covid_world[new_cases&lt;0 | new_cases&gt;100000] ## iso_code continent location date total_cases ## 1: ATG North America Antigua and Barbuda 2020-07-03 68 ## 2: ATG North America Antigua and Barbuda 2021-05-09 1231 ## 3: OWID_ASI Asia 2020-08-26 6604021 ## 4: OWID_ASI Asia 2020-08-27 6707611 ## 5: OWID_ASI Asia 2020-08-28 6811518 ## --- ## 1655: OWID_WRL World 2021-09-04 220348745 ## 1656: OWID_WRL World 2021-09-05 220775393 ## 1657: OWID_WRL World 2021-09-06 221211222 ## 1658: YEM Asia Yemen 2020-08-11 1831 ## 1659: ZWE Africa Zimbabwe 2020-05-02 34 ## new_cases total_deaths new_deaths total_tests_per_thousand ## 1: -1 3 0 NA ## 2: -1 32 0 NA ## 3: 112974 135017 1682 NA ## 4: 103590 136658 1641 NA ## 5: 103907 138253 1595 NA ## --- ## 1655: 480942 4561996 7736 NA ## 1656: 426648 4568456 6460 NA ## 1657: 435829 4576372 7916 NA ## 1658: -1 523 5 NA ## 1659: -6 4 0 NA ## people_vaccinated_per_hundred ## 1: NA ## 2: NA ## 3: NA ## 4: NA ## 5: NA ## --- ## 1655: 40.33 ## 1656: 40.47 ## 1657: 40.61 ## 1658: NA ## 1659: NA Clean up the data: covid_world[,date:=ymd(date)] # in the world map data, United States is named USA covid_world[location==&quot;United States&quot;,location:=&quot;USA&quot;] # clean up the negative new cases as missing value covid_world[new_cases&lt;0,new_cases:=NA] Use a line chart to show the number of new cases over time: ggplot(covid_world)+ geom_line(aes(date, new_cases, color=location),show.legend = FALSE)+ theme_classic() ## Warning: Removed 5538 row(s) containing missing values (geom_path). Use a line chart to show the number of new cases over time: ggplot(covid_world[location%in%c(&quot;United Kingdom&quot;,&quot;USA&quot;,&quot;Italy&quot;,&quot;Brazil&quot;,&quot;Korean&quot;,&quot;China&quot;)])+ geom_line(aes(date, new_cases, color=location))+ theme_classic() ## Warning: Removed 2 row(s) containing missing values (geom_path). To plot the map of new cases in each day, we need to first prepare the map data to include the COVID-19 cases informatin. # define any date you want to inquiry today=ymd(&quot;2020-06-19&quot;) # subset the covid dataset to select the above date and relevant columns covid_today=covid_world[date==today,.(location,continent,date,total_cases,new_cases, total_deaths, total_tests_per_thousand)] # combine the map data file my_map = merge(world_map,covid_today, by.x=&quot;region&quot;, by.y =&quot;location&quot;, all.x = TRUE) head(my_map) ## region long lat group order subregion continent date ## 1: Afghanistan 74.89131 37.23164 2 12 &lt;NA&gt; Asia 2020-06-19 ## 2: Afghanistan 74.84023 37.22505 2 13 &lt;NA&gt; Asia 2020-06-19 ## 3: Afghanistan 74.76738 37.24917 2 14 &lt;NA&gt; Asia 2020-06-19 ## 4: Afghanistan 74.73896 37.28564 2 15 &lt;NA&gt; Asia 2020-06-19 ## 5: Afghanistan 74.72666 37.29072 2 16 &lt;NA&gt; Asia 2020-06-19 ## 6: Afghanistan 74.66895 37.26670 2 17 &lt;NA&gt; Asia 2020-06-19 ## total_cases new_cases total_deaths total_tests_per_thousand ## 1: 27882 346 552 NA ## 2: 27882 346 552 NA ## 3: 27882 346 552 NA ## 4: 27882 346 552 NA ## 5: 27882 346 552 NA ## 6: 27882 346 552 NA # It is vital that the map data is ordered by the order column. Otherwise, the map will be messed up. my_map[order(order)] ## region long lat group order subregion continent date ## 1: Aruba -69.89912 12.45200 1 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2: Aruba -69.89571 12.42300 1 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3: Aruba -69.94219 12.43853 1 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4: Aruba -70.00415 12.50049 1 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5: Aruba -70.06612 12.54697 1 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## --- ## 99334: Vatican 12.43838 41.90620 1627 100960 enclave Europe 2020-06-19 ## 99335: Vatican 12.43057 41.90547 1627 100961 enclave Europe 2020-06-19 ## 99336: Vatican 12.42754 41.90073 1627 100962 enclave Europe 2020-06-19 ## 99337: Vatican 12.43057 41.89756 1627 100963 enclave Europe 2020-06-19 ## 99338: Vatican 12.43916 41.89839 1627 100964 enclave Europe 2020-06-19 ## total_cases new_cases total_deaths total_tests_per_thousand ## 1: NA NA NA NA ## 2: NA NA NA NA ## 3: NA NA NA NA ## 4: NA NA NA NA ## 5: NA NA NA NA ## --- ## 99334: 12 0 NA NA ## 99335: 12 0 NA NA ## 99336: 12 0 NA NA ## 99337: 12 0 NA NA ## 99338: 12 0 NA NA Now, we are ready to plot the data: g&lt;-ggplot(my_map, aes(x=long,y=lat,group=group))+ geom_polygon(aes(fill=new_cases))+ geom_path()+ coord_map(xlim=c(-180,180))+ labs(x = NULL, y = NULL, title = paste(&quot;New confirmed cases worldwide: &quot;, today),fill=&quot;New Cases&quot;) g The default fill color scheme is not quite differentiable. We can use scale_fill_gradient() function to creates a two colour gradient (low-high): g+ scale_fill_gradient(low=&quot;white&quot;,high=&quot;red&quot;,na.value=&quot;grey90&quot;) In the above code, we set color scheme of white representing low quantity, red representing high quantity, the missing value is represented by grey. Here is another commonly used color scheme. In the code below, heat.colors(10) generate a heat color scheme from white, yellow to red with 10 steps in between. rev() reserves this color scheme because we want to use red to represent high quantity, and yellow to represent low quantity. g+ scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;) Now, the map looks OK, but we want to clean the extra grid and axis labels. We can do that by change the theme of the plot. Below is a reusable map theme that is readily to use. Define the theme of map: theme_map &lt;- function(...) { theme_minimal() + theme( axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), plot.background = element_rect(fill = &quot;white&quot;, color = NA), panel.background = element_rect(fill = &quot;gray99&quot;, color = NA), legend.background = element_rect(fill = &quot;white&quot;, color = NA), panel.border = element_blank(), legend.position = &quot;right&quot;, ... ) } Apply the self-defined map theme: g + scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map() Now, lets plot the total cases on the map: ggplot(my_map, aes(x=long,y=lat,group=group))+ geom_polygon(aes(fill=total_cases))+ geom_path()+ coord_map(xlim=c(-180,180))+ labs(x = NULL, y = NULL, title = paste(&quot;Total cases worldwide: &quot;, today), fill=&quot;Total Cases&quot;)+ scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map() ggsave(&quot;data/covid_total_cases.png&quot;) # save the chart into local file ## Saving 7 x 5 in image 16.2 Draw US map In most of the situation, we focus on US as well as states in US. Here we show how to create US/States map. First, we need to load the US map data. states_map=as.data.table(map_data(&quot;state&quot;)) head(states_map) ## long lat group order region subregion ## 1: -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2: -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3: -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4: -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5: -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6: -87.58806 30.32665 1 6 alabama &lt;NA&gt; Next, we can plot the US map. ggplot(data=states_map)+ geom_polygon(aes(x=long, y=lat, group=group), color=&quot;black&quot;, fill=&quot;gray90&quot;) + coord_map() + theme_map() We can add the information of each state on the map. E.g., we can fill each state to a color corresponding to its number of new COVID-19 confirmed cases. First, we need to download the COVID-19 data for each state from the web. # the data is in the following web-page url = &#39;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv&#39; covid_US=fread(url) Examine the data through head(): head(covid_US) ## date state fips cases deaths ## 1: 2020-01-21 Washington 53 1 0 ## 2: 2020-01-22 Washington 53 1 0 ## 3: 2020-01-23 Washington 53 1 0 ## 4: 2020-01-24 Illinois 17 1 0 ## 5: 2020-01-24 Washington 53 1 0 ## 6: 2020-01-25 California 6 1 0 Clean the data before any analysis. First, in map_state, the state name is lower case; here state name has upper case, we need to make it also lower case so that we can merge the two datasets. Second, we need to convert date from string to date. covid_US[,date:=ymd(date)] covid_US[,state:=tolower(state)] head(covid_US) ## date state fips cases deaths ## 1: 2020-01-21 washington 53 1 0 ## 2: 2020-01-22 washington 53 1 0 ## 3: 2020-01-23 washington 53 1 0 ## 4: 2020-01-24 illinois 17 1 0 ## 5: 2020-01-24 washington 53 1 0 ## 6: 2020-01-25 california 6 1 0 Merge the state map data with the covid-19 data. select_date=&quot;2020-07-04&quot; covid_US_today=covid_US[date==select_date] map_covid_state = merge(states_map,covid_US_today, by.x=&quot;region&quot;, by.y=&quot;state&quot;, all.x = TRUE) head(map_covid_state) ## region long lat group order subregion date fips cases ## 1: alabama -87.46201 30.38968 1 1 &lt;NA&gt; 2020-07-04 1 42862 ## 2: alabama -87.48493 30.37249 1 2 &lt;NA&gt; 2020-07-04 1 42862 ## 3: alabama -87.52503 30.37249 1 3 &lt;NA&gt; 2020-07-04 1 42862 ## 4: alabama -87.53076 30.33239 1 4 &lt;NA&gt; 2020-07-04 1 42862 ## 5: alabama -87.57087 30.32665 1 5 &lt;NA&gt; 2020-07-04 1 42862 ## 6: alabama -87.58806 30.32665 1 6 &lt;NA&gt; 2020-07-04 1 42862 ## deaths ## 1: 1007 ## 2: 1007 ## 3: 1007 ## 4: 1007 ## 5: 1007 ## 6: 1007 Next, we can plot the US map. ggplot(map_covid_state)+ geom_polygon(aes(x=long, y=lat, group=group, fill=cases), color=&quot;black&quot;) + coord_map() + scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map()+ labs(title=paste(&quot;Total confimed cases in US: &quot;, select_date, sep=&quot;&quot;), fill=&quot;New Cases&quot;) ggsave(&quot;data/covid_us_map.png&quot;) ## Saving 7 x 5 in image Sometime, we want to add text information on the map: ggplot(map_covid_state)+ geom_polygon(aes(x=long, y=lat, group=group, fill=cases), color=&quot;black&quot;) + coord_map() + scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map()+ labs(title=paste(&quot;Total confimed cases in US: &quot;, select_date, sep=&quot;&quot;), fill=&quot;New Cases&quot;) ggsave(&quot;data/covid_us_map.png&quot;) ## Saving 7 x 5 in image 16.3 draw US county map Sometime, we only want to zoom in to look at a subset of states at the county level. Here we show how to draw US county map. First, we need to extract the US county map data: county_map=as.data.table(map_data(&quot;county&quot;)) head(county_map) ## long lat group order region subregion ## 1: -86.50517 32.34920 1 1 alabama autauga ## 2: -86.53382 32.35493 1 2 alabama autauga ## 3: -86.54527 32.36639 1 3 alabama autauga ## 4: -86.55673 32.37785 1 4 alabama autauga ## 5: -86.57966 32.38357 1 5 alabama autauga ## 6: -86.59111 32.37785 1 6 alabama autauga Next, we can draw US county map: ggplot(data=county_map)+ geom_polygon(aes(x=long, y=lat, group=group), color=&quot;gray&quot;)+ coord_map() + theme_map()+ labs(title=&quot;US county map&quot;) Similarly, we can include county information to the map. Take covid-19 as an example: First, we need to download the data from the following address: covid_county=fread(&quot;https://github.com/nytimes/covid-19-data/raw/master/us-counties.csv&quot;) head(covid_county) ## date county state fips cases deaths ## 1: 2020-01-21 Snohomish Washington 53061 1 0 ## 2: 2020-01-22 Snohomish Washington 53061 1 0 ## 3: 2020-01-23 Snohomish Washington 53061 1 0 ## 4: 2020-01-24 Cook Illinois 17031 1 0 ## 5: 2020-01-24 Snohomish Washington 53061 1 0 ## 6: 2020-01-25 Orange California 6059 1 0 Clean up the data for analysis: covid_county[,date:=ymd(date)] covid_county[,county:=tolower(county)] covid_county[,state:=tolower(state)] head(covid_county) ## date county state fips cases deaths ## 1: 2020-01-21 snohomish washington 53061 1 0 ## 2: 2020-01-22 snohomish washington 53061 1 0 ## 3: 2020-01-23 snohomish washington 53061 1 0 ## 4: 2020-01-24 cook illinois 17031 1 0 ## 5: 2020-01-24 snohomish washington 53061 1 0 ## 6: 2020-01-25 orange california 6059 1 0 Merge with the map data # choose the date to map select_date=&quot;06-21-2020&quot; covid_county2&lt;-covid_county[date==mdy(select_date)] map_covid_county = merge(county_map,covid_county2, by.x=c(&quot;region&quot;,&quot;subregion&quot;), by.y=c(&quot;state&quot;,&quot;county&quot;), all.x = TRUE) head(map_covid_county) ## region subregion long lat group order date fips cases ## 1: alabama autauga -86.50517 32.34920 1 1 2020-06-21 1001 434 ## 2: alabama autauga -86.53382 32.35493 1 2 2020-06-21 1001 434 ## 3: alabama autauga -86.54527 32.36639 1 3 2020-06-21 1001 434 ## 4: alabama autauga -86.55673 32.37785 1 4 2020-06-21 1001 434 ## 5: alabama autauga -86.57966 32.38357 1 5 2020-06-21 1001 434 ## 6: alabama autauga -86.59111 32.37785 1 6 2020-06-21 1001 434 ## deaths ## 1: 9 ## 2: 9 ## 3: 9 ## 4: 9 ## 5: 9 ## 6: 9 Next, we can draw US county map: ggplot(map_covid_county)+ geom_polygon(aes(x=long, y=lat, fill=cases, group=group), color=&quot;gray80&quot;)+ coord_map() + scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map()+ labs(title=paste(&quot;Total confirmed cases in US: &quot;, select_date), fill=&quot;total cases&quot;) The county map is mostly useful when we want to zoom in to look at particular state. select_state=c(&quot;nebraska&quot;) ggplot(map_covid_county[region%in%select_state])+ geom_polygon(aes(x=long, y=lat, fill=cases, group=group), color=&quot;gray80&quot;)+ coord_map() + scale_fill_gradientn(colours=rev(heat.colors(10)),na.value=&quot;grey90&quot;)+ theme_map()+ labs(title=paste(&quot;Total confirmed cases in &quot;, select_state, &quot; : &quot;, select_date), fill=&quot;Total Cases&quot;) ggsave(&quot;data/covid_county_map.png&quot;) ## Saving 7 x 5 in image 16.4 Summary learn to make maps with ggplot2: world map, US state maps, US county maps learn to include information about on the map. apply data.table to join data.table. Use theme to adjust the appearance of the map "],["visualization-with-ggplot2---animation.html", "Chapter 17 Visualization with ggplot2 - Animation 17.1 Animation of scatter plot to show how two variables evolve over time simultaneously 17.2 Use a line chart to show how gdpPercap evolve over time 17.3 Animation with Bar chart", " Chapter 17 Visualization with ggplot2 - Animation Animation is a great visual way to tell a story with data. There is a time dimension in your data, i.e., the data is observed at multiple time point, you can consider to do an animation to show how your data evolve over time. First, we need to install the gganimate package for making annimation install.packages(&quot;gganimate&quot;) install.packages(&quot;gifski&quot;) library(gganimate) ## Warning: package &#39;gganimate&#39; was built under R version 4.0.5 library(gifski) ## Warning: package &#39;gifski&#39; was built under R version 4.0.5 library(data.table) We will study how to build animation with ggplot2 and gganimate packages. We will use the gapminder dataset for illustration. Read the gapminder.csv into R and examine the first few rows of the data using head(). gapminder=fread(&quot;data/gapminder.csv&quot;) head(gapminder) ## country continent year lifeExp pop gdpPercap ## 1: Afghanistan Asia 1952 28.801 8425333 779.4453 ## 2: Afghanistan Asia 1957 30.332 9240934 820.8530 ## 3: Afghanistan Asia 1962 31.997 10267083 853.1007 ## 4: Afghanistan Asia 1967 34.020 11537966 836.1971 ## 5: Afghanistan Asia 1972 36.088 13079460 739.9811 ## 6: Afghanistan Asia 1977 38.438 14880372 786.1134 The data shows the gdpDerap,lifeExpectation, population of different countries from 1952 to 2007. 17.1 Animation of scatter plot to show how two variables evolve over time simultaneously Make a scatter between lifeExp and gdpDerap for all the years. Use the population to denote the size of the point. p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) + geom_point(show.legend = FALSE, alpha = 0.7) + scale_x_continuous(trans = &quot;log10&quot;) + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;) p Create the gif to show each year a time: g&lt;-p + transition_time(year) + labs(title = &quot;Year: {frame_time}&quot;) # output GIF animate(g, renderer = gifski_renderer()) anim_save(&quot;data/gdp1.gif&quot;) Facets by continent: p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) + geom_point(show.legend = FALSE, alpha = 0.7) + scale_x_continuous(trans = &quot;log10&quot;) + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;) g&lt;-p + facet_wrap(~continent) + transition_time(year) + labs(title = &quot;Year: {frame_time}&quot;) # render into gif animate(g, renderer = gifski_renderer()) anim_save(&quot;data/gdp2.gif&quot;) Add text to the plot text_dat=gapminder[country%in%c(&quot;United States&quot;, &quot;China&quot;,&quot;United Kingdom&quot;,&quot;Japan&quot;)] p=ggplot(gapminder) + geom_point(aes(gdpPercap, lifeExp, size = pop, color = country), show.legend = FALSE, alpha = 0.7) + geom_text(data=text_dat, aes(gdpPercap, lifeExp, label=country), size=4, show.legend = FALSE) + scale_x_continuous(trans = &quot;log10&quot;) + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;) p g&lt;-p + transition_time(year) + labs(title = &quot;Year: {frame_time}&quot;) # render into gif h&lt;-animate(g, renderer = gifski_renderer()) h anim_save(&quot;data/gdp3.gif&quot;,h) The animations shows a few observations: Overall all, gdpPercap and lifeExp increase across the world over time. It is clear that in Africa, a group of country is ahead of others. Europe countries are similar. US and UK have always among the top country in terms of GDP and lifeExp Japan is the most health country China has a large population and grows very fast in lifeExp and GDP. 17.2 Use a line chart to show how gdpPercap evolve over time Here we create line chart to show how gdpPercap evolve over time. We will then add animation to the chart to gradually show the line evolve over time. ggplot(gapminder, aes(year, gdpPercap, color = country)) + geom_line(show.legend = FALSE, alpha = 0.7) + labs(x = &quot;Year&quot;, y = &quot;GDP per capita&quot;, color=&quot;country&quot;)+ theme_minimal() + facet_wrap(~continent) It would different to show all countries in one single chart. Suppose we are particular intereted in the following country: United States, China,United Kingdom,Japan, Korean. Use a line chart to show the gdpPercap of these countries evolve over time. sub_gapminder=gapminder[country%in%c(&quot;United States&quot;, &quot;China&quot;,&quot;United Kingdom&quot;,&quot;Japan&quot;,&quot;Korean&quot;)] ggplot(sub_gapminder, aes(year, gdpPercap, color = country)) + geom_line() + theme_minimal()+ labs(x = &quot;GDP per capita&quot;, y = &quot;GDP Per Captia&quot;) Let the line show gradually over time through animation: p=ggplot(sub_gapminder, aes(year, gdpPercap, color = country) ) + geom_line(show.legend = FALSE) + geom_text(aes(year, gdpPercap, label=country), vjust = 0.2, hjust =0, size=4, show.legend = FALSE) + theme_minimal()+ labs(x = &quot;GDP per capita&quot;, y = &quot;GDP Per Captia&quot;) # create annimation g&lt;-p + transition_reveal(year) h&lt;-animate(g, renderer = gifski_renderer()) h anim_save(&quot;data/gdp4.gif&quot;,h) Note that the value of hjust and vjust are only defined between 0 and 1: 0 means left-justified; 1 means right-justified; hjust controls horizontal justification and vjust controls vertical justification. 17.3 Animation with Bar chart We can make bar chart with animation to show the top n categories over time. E.g., we want to use an animation to show the top 10 country in terms of gdpPercap. First, we need to subset the dataset to obtaint the the top 10 country in terms of gdpPercap for each year. This can be done using the powerful subsetting function in data.table package. # select the top 10 country for each year gapminder10 = gapminder[order(year,-gdpPercap)][, .SD[1:10],by = year] # create rank of gdpPercap among country for each year gapminder10[, rank:= .N:1, by = year] head(gapminder10,15) ## year country continent lifeExp pop gdpPercap rank ## 1: 1952 Kuwait Asia 55.565 160000 108382.353 10 ## 2: 1952 Switzerland Europe 69.620 4815000 14734.233 9 ## 3: 1952 United States Americas 68.440 157553000 13990.482 8 ## 4: 1952 Canada Americas 68.750 14785584 11367.161 7 ## 5: 1952 New Zealand Oceania 69.390 1994794 10556.576 6 ## 6: 1952 Norway Europe 72.670 3327728 10095.422 5 ## 7: 1952 Australia Oceania 69.120 8691212 10039.596 4 ## 8: 1952 United Kingdom Europe 69.180 50430000 9979.508 3 ## 9: 1952 Bahrain Asia 50.939 120447 9867.085 2 ## 10: 1952 Denmark Europe 70.780 4334000 9692.385 1 ## 11: 1957 Kuwait Asia 58.033 212846 113523.133 10 ## 12: 1957 Switzerland Europe 70.560 5126000 17909.490 9 ## 13: 1957 United States Americas 69.490 171984000 14847.127 8 ## 14: 1957 Canada Americas 69.960 17010154 12489.950 7 ## 15: 1957 New Zealand Oceania 70.260 2229407 12247.395 6 p=ggplot(gapminder10) + geom_bar(aes(x=gdpPercap, y= factor(rank), fill = country), stat = &quot;identity&quot;, show.legend = FALSE)+ geom_text(aes(x = -1000, y = factor(rank), label = country), vjust = 0.2, hjust = 1, size = 4,show.legend = FALSE) + labs(x = &quot;GDP per capita&quot;, y = NULL) + scale_x_continuous(breaks = seq(0, 90000, 10000), expand = expansion(mult = c(.2, 0.05))) + theme(legend.position=&quot;none&quot;, axis.text.y = element_blank(), axis.title.y=element_blank(), axis.ticks.y=element_blank(), panel.background=element_blank()) g&lt;-p + transition_time(year) + labs(title = &quot;Year: {frame_time}&quot;) # nframes: sets the total number of frames for the gif; # fps: frames per second to control the speed of gif. h&lt;-animate(g, nframes = 100, fps = 10, renderer = gifski_renderer()) h anim_save(&quot;data/gdp5.gif&quot;,h) Note that: Put the labels on the left of the datapoints by changing the horizontal alignment via hjust = 1. I also added some space between the label and the axis by setting x = -1000 We have to add some space for the labels so that they are not cut when reaching the borders of the plot. This can be achieved by increasing the expansion of the scale via the expand argument of scale_x_continuous. I inreased the expansion at the lower end to 20 percent, while keeping the default 5 percent at the upper end. Finally, to prevent breaks with negative values when setting x = -1000 in geom_text I force the breaks to start at 0 via breaks = seq(0, 90000, 10000). nframes is the number of frames. The greater the number, the better the transition. This is similar to drawing cartoons. However, with more frames, the processing time is longer. R will take some more time and consume more power. fps is the amount of frame shown per second(default is 10) Since year is a distinct value, we can also set the animation of transition with discrete state. g&lt;-p + transition_states(year,transition_length = 3, state_length = 1) + labs(title = &quot;Year: {closest_state}&quot;) # output GIF h&lt;-animate(g, nframes = 100,fps = 15, renderer = gifski_renderer()) h anim_save(&quot;data/gdp6.gif&quot;,h) In above code, transition_length: The relative length of the transition. state_length: The relative length of the pause at the states. "],["simple-linear-regression.html", "Chapter 18 Simple Linear Regression 18.1 Examine and explore the data 18.2 Develop simple linear regression model 18.3 Overall model fittness 18.4 Use estimated model for prediction 18.5 Summary", " Chapter 18 Simple Linear Regression Here are the packages we will use in this chapter. install.packages(&quot;data.table&quot;) # for data manipulation install.packages(&quot;corrplot&quot;) # for ploting beautiful correlation matrix Load these packages into R: library(data.table) library(corrplot) ## corrplot 0.90 loaded Linear regression is to use a linear equation to approximate the relationship between dependent variable and single/multiple explanatory variables (also known as independent variables or predictors). The linear regression model with single predictor is called simple linear regression. The linear regression model with multiple predictors is called multiple linear regression. Lets illustrate the concept of linear regression through the following example of simple linear regression. The Boston house price dataset contains the price information of houses in the Boston suburbs areas. It also includes variables related to the neighbourhood, air pollution, per capita crime rate, index of accessibility to radial highways, etc, for evaluation of their impact on house price. Here is a brief description of the variables in the dataset: medv: median value of ower-occupied homes in $10,000 crim: per capita crime rate by town zn: proportion of residential land zoned for lots over 25,000 sq.ft. indus: proportion of non-retail business acres per town chas: charles river dummy variables nox: nitrogen oxides concentration (parts per 10 million) rm: average number of rooms per dwelling age: proportion of owner-occupied units built prior to 1940 dis: weighted mean of distances to five Boston employment centres rad: index of accessibility to radial highways tax: full-value property tax rate per $10,000 ptratio pupil-teacher ratio by town black: 1000*(Bk-0.63)^2 where Bk is the proportion of African American by town lstat: lower income of the population within the neighborhood (percent) location: a categorical variable to indicate the location of the area Note that, the medv is the dependent variable, which we want to explain and predict. The above scatter plot demonstrates that the relationship between medv and lstat: The house price tends to be higher in richer neighborhood. This is intutively true, but we want to further quantify this relationship. The goal of linear regression is to approximate and estimate this relationship through a linear equation. In particular, for this example, we want to approximate and estimate the relationship between medv and lstat through a linear equalition as below: \\[medv=\\beta_0+\\beta_1*lstat+\\epsilon\\] In the above linear regression model, medv is called dependent variable, the variable we want to explain and predict. In other words, we observe variation in medv (i.e., different houses have different price), and we want to explain what factors causes such variation in house price. lstat is called independent variable, which is used to explain and predict the dependent variable. In other words, we hypothesize that the houses have different prices is because they are located in region with different income status. \\(\\epsilon\\) is called residual terms. This represents the variation in medv that is left unexplained by the independent variable. In this particular example, it encompasses any related factors (such as size of the house, number of bedrooms, interest rate) that are not included in the model as predictors. The linear regression is a method to develop such linear equations and estimate their coefficents (i.e., \\(\\beta_0\\), \\(\\beta_1\\) in the above example) such that we can quantify the relationship between the dependent and independent variables. 18.1 Examine and explore the data Before developing any linear regression, it is a best practice to explore the data first to check whether there exists any kinds of linear relationship. As mentioned, we will use the Boston house price dataset for illustration. ### Load and clean the data Boston=fread(&quot;data/Boston.csv&quot;) # read the data in R as data.table head(Boston) # inspect the first few rows visually ## crim zn indus chas nox rm age dis rad tax ptratio black lstat ## 1: 0.00632 18 2.31 0 0.538 6.575 65.2 4.0900 1 296 15.3 396.90 4.98 ## 2: 0.02731 0 7.07 0 0.469 6.421 78.9 4.9671 2 242 17.8 396.90 9.14 ## 3: 0.02729 0 7.07 0 0.469 7.185 61.1 4.9671 2 242 17.8 392.83 4.03 ## 4: 0.03237 0 2.18 0 0.458 6.998 45.8 6.0622 3 222 18.7 394.63 2.94 ## 5: 0.06905 0 2.18 0 0.458 7.147 54.2 6.0622 3 222 18.7 396.90 5.33 ## 6: 0.02985 0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 5.21 ## location medv ## 1: west 24.0 ## 2: east 21.6 ## 3: west 34.7 ## 4: south 33.4 ## 5: south 36.2 ## 6: south 28.7 The data is tidy: each rows represents an observation, and each column is a variable. str(Boston) # examine the structure of the data ## Classes &#39;data.table&#39; and &#39;data.frame&#39;: 506 obs. of 15 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : int 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio : num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ location: chr &quot;west&quot; &quot;east&quot; &quot;west&quot; &quot;south&quot; ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... ## - attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; Since location (east, south, west, north) is actually a categorical variable, we need to change it as factor. # examine the frequency table of location variable table(Boston$location) ## ## east north south west ## 119 132 130 125 # change location from character as factor Boston[,location:=factor(location)] # examine the frequencey table of chas variable table(Boston$chas) ## ## 0 1 ## 471 35 # change chas (0/1 variable) from numeric to factor Boston[,chas:=factor(chas)] Examine whether there are some missing value in the data: sum(is.na(Boston)) ## [1] 0 Take a quick look at the summary of the data to see if there is any extreme values in each variable: summary(Boston) ## crim zn indus chas nox ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 0:471 Min. :0.3850 ## 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1: 35 1st Qu.:0.4490 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.5380 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.5547 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.6240 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :0.8710 ## rm age dis rad ## Min. :3.561 Min. : 2.90 Min. : 1.130 Min. :1.000 ## 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 1st Qu.:4.000 ## Median :6.208 Median : 77.50 Median : 3.207 Median :5.000 ## Mean :6.285 Mean : 68.57 Mean : 3.795 Mean :4.332 ## 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 3rd Qu.:5.000 ## Max. :8.780 Max. :100.00 Max. :12.127 Max. :5.000 ## tax ptratio black lstat location ## Min. :187.0 Min. :12.60 Min. : 0.32 Min. : 1.73 east :119 ## 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 1st Qu.: 6.95 north:132 ## Median :330.0 Median :19.05 Median :391.44 Median :11.36 south:130 ## Mean :408.2 Mean :18.46 Mean :356.67 Mean :12.65 west :125 ## 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 3rd Qu.:16.95 ## Max. :711.0 Max. :22.00 Max. :396.90 Max. :37.97 ## medv ## Min. : 5.00 ## 1st Qu.:17.02 ## Median :21.20 ## Mean :22.53 ## 3rd Qu.:25.00 ## Max. :50.00 No extreme value or obvious error are found in the data. We have the luxury to work with a very clear dataset. Next, we will introduce two tools to examine whether any linear relationship exists in the data: correlation matrix and scatter plot. 18.1.1 Correlation matrix Correlation coefficient is a single number between -1 and 1 to quantify the strength of the linear relationship between two variables. We can obtain the correlation between any two variable as below: # cor() is only takes numeric variables as arguments corr=cor(Boston[,-c(&quot;location&quot;,&quot;chas&quot;)]) # round the correlation coefficient with 2 digits round(corr, digits = 2) ## crim zn indus nox rm age dis rad tax ptratio black ## crim 1.00 -0.20 0.41 0.42 -0.22 0.35 -0.38 0.25 0.58 0.29 -0.39 ## zn -0.20 1.00 -0.53 -0.52 0.31 -0.57 0.66 -0.29 -0.31 -0.39 0.18 ## indus 0.41 -0.53 1.00 0.76 -0.39 0.64 -0.71 0.31 0.72 0.38 -0.36 ## nox 0.42 -0.52 0.76 1.00 -0.30 0.73 -0.77 0.41 0.67 0.19 -0.38 ## rm -0.22 0.31 -0.39 -0.30 1.00 -0.24 0.21 -0.12 -0.29 -0.36 0.13 ## age 0.35 -0.57 0.64 0.73 -0.24 1.00 -0.75 0.30 0.51 0.26 -0.27 ## dis -0.38 0.66 -0.71 -0.77 0.21 -0.75 1.00 -0.33 -0.53 -0.23 0.29 ## rad 0.25 -0.29 0.31 0.41 -0.12 0.30 -0.33 1.00 0.44 0.09 -0.21 ## tax 0.58 -0.31 0.72 0.67 -0.29 0.51 -0.53 0.44 1.00 0.46 -0.44 ## ptratio 0.29 -0.39 0.38 0.19 -0.36 0.26 -0.23 0.09 0.46 1.00 -0.18 ## black -0.39 0.18 -0.36 -0.38 0.13 -0.27 0.29 -0.21 -0.44 -0.18 1.00 ## lstat 0.46 -0.41 0.60 0.59 -0.61 0.60 -0.50 0.24 0.54 0.37 -0.37 ## medv -0.39 0.36 -0.48 -0.43 0.70 -0.38 0.25 -0.15 -0.47 -0.51 0.33 ## lstat medv ## crim 0.46 -0.39 ## zn -0.41 0.36 ## indus 0.60 -0.48 ## nox 0.59 -0.43 ## rm -0.61 0.70 ## age 0.60 -0.38 ## dis -0.50 0.25 ## rad 0.24 -0.15 ## tax 0.54 -0.47 ## ptratio 0.37 -0.51 ## black -0.37 0.33 ## lstat 1.00 -0.74 ## medv -0.74 1.00 corr is called correlation matrix: Each number indicates the correlation coefficient between the two variables on the corresponding row and column. As detailed as it is, the correlation matrix is however hard to read. We can add some visual element to help readers comprehending the correlation matrix. We can use corrplot() to plot the correlation matrix as below: # the correlation between x and y is the same as y and x # thus, we only need the upper or lower part of the matrix corrplot(corr, type = &quot;upper&quot;) Both the size and color of the circle indicates the magnitude of the correlation coefficients so that we can focus our attention on the pairs with correlation close to 1 or -1. We can set the method=number to show the correlation coefficient directly. # number.cex and tl.cex change the font size of the number and label, respectively corrplot(corr, type = &quot;upper&quot;, method = &quot;number&quot;, number.cex = .7, tl.cex=0.8) We can combine these two togother: corrplot.mixed(corr, number.cex = .7, tl.cex=0.8) The last column shows that medv has strong positive correlation with rm (number of rooms in the house), wiht \\(\\rho=0.7\\), and has strong negative correlation with lstat (\\(\\rho=-0.74\\)). This gives us the confidence to develop linear regression model to explain the variation in medv. 18.1.2 Scatter plot Scatter plot gives a visual clue of the correlation between two variables. We have learned how to make scatter plot with ggplot2 package. Here we will introduce a quick way of making scatter plot matrix. pairs(Boston[,c(&quot;indus&quot;,&quot;nox&quot;,&quot;rm&quot;,&quot;tax&quot;,&quot;ptratio&quot;,&quot;lstat&quot;,&quot;medv&quot;)], pch = 19, cex = 0.1, lower.panel=NULL) # cex sets the size of the points It becomes difficult to see when too many variables are putting in the scatter plot matrix all together. Thus, you can choose a few variables that has high correlation with the dependent variables for a better display. Now, draw your attention on the last column, which shows the relationship between medv and different independent variables. It appears that lstat and rm have a very strong linear relationship with medv. Both correlation matrix and scatter plot matrix indicates that there exists strong relationship between some of the independent variables and dependent variable. New, we will develop linear regression model to approximate and quantify that relationship. 18.2 Develop simple linear regression model The general form of simple linear regression equation is \\(y=\\beta_0+\\beta_1*x+\\epsilon\\). This implies two things. First, we are using the variation in x to explain variation in y. E.g., let y= students final grade of SCMA450, and x=lines of codes written by the students. Then the model implies that students SCMA450 final grade is different because of the number of lines they wrote during the study. Second, one additional unit of x will always result in the same amount of change (i.e., quantifies by \\(\\beta_1\\)). In the above example, each additional line of code written by the students will result in the same amount of improvement on their final grade. Such linear assumption works fine in many application and leads easy-to-understand interpretation. We will talk about how to address potential non-linear relationship in next chapter. Back to the Boston house price example, we are trying to explain why the house price varies at Boston. We hypothesize that lstat (i.e., the percent of low income population in the neighborhood) may result in different house price: Houses located in rich area (i.e., lstat is low) would likely have higher price. To test this hypothesis, we can build a simple linear regression model as follow: \\[medv=\\beta_0+\\beta_1*lstat+\\epsilon\\] 18.2.1 Estimation The model estimation is to find the coefficient \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the sum square error, which is known as the least square algorithm. Lets illustrate the least square algorithm. For a given lstat, based on the linear equation, we can compute the fitted value of medv according to: \\[medvFit=\\beta_0+\\beta_1*lstat\\] and the error term: \\[error=medv-\\beta_0-\\beta_1*lstat\\] We can better demonstrate the formula using the following tabular form: ## medv lstat medvFit error ## 1: 24.0 4.98 beta0+beta1*4.98 24-beta0-beta1*4.98 ## 2: 21.6 9.14 beta0+beta1*9.14 21.6-beta0-beta1*9.14 ## 3: 34.7 4.03 beta0+beta1*4.03 34.7-beta0-beta1*4.03 ## 4: 33.4 2.94 beta0+beta1*2.94 33.4-beta0-beta1*2.94 ## 5: 36.2 5.33 beta0+beta1*5.33 36.2-beta0-beta1*5.33 ## 6: 28.7 5.21 beta0+beta1*5.21 28.7-beta0-beta1*5.21 Given any value \\(\\beta_0\\) and \\(\\beta_1\\), based on the above formula in the table, we will get a series of fitted value and errors. We want to our model to best fit the data, meaning we want medvFit to be as close to medv and thus we want error to as small as possible. Thus, the best \\(\\beta_0\\) and \\(\\beta_1\\) should minimize the errors. The algorithm to find optimal \\(\\beta_0\\) and \\(\\beta_1\\) is called least square algoirthm. Mathematically, the least square algorithm will determine \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the residual sum square (RSS): \\[\\min_{\\beta_0,\\beta_1} RSS=\\sum_i(y_i-\\beta_0-\\beta_1x_i)^2\\] here \\(i\\) is the index of different observations (i.e., the index of rows in the above table). We do not to need to calcuate the optimal \\(\\beta_0\\) and \\(\\beta_1\\) mannually. The R code to estimate the above model is really simple. lm(formula=medv~lstat, data=Boston) ## ## Call: ## lm(formula = medv ~ lstat, data = Boston) ## ## Coefficients: ## (Intercept) lstat ## 34.55 -0.95 Here lm() is the function to estimate linear model. It takes two arguments: formula and data. medv~lstat represents the formula: medv is y-variable, with lstat as the x-variable. data=Boston specifies the dataset used. We typically save the estimated results first and use summary() to get more information about the estimation. fit1=lm(medv~lstat, data=Boston) summary(fit1) ## ## Call: ## lm(formula = medv ~ lstat, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.168 -3.990 -1.318 2.034 24.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.55384 0.56263 61.41 &lt;2e-16 *** ## lstat -0.95005 0.03873 -24.53 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.216 on 504 degrees of freedom ## Multiple R-squared: 0.5441, Adjusted R-squared: 0.5432 ## F-statistic: 601.6 on 1 and 504 DF, p-value: &lt; 2.2e-16 In fact, lm() function returns a list, and is saved into fit1 as we defined. We can take a look at the elements in fit1 list. names(fit1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; As seen, the fitted line is: \\[medvFit=34.55-0.95*lstat\\] 18.2.2 How to interpret the coefficient estimates? The estimated coefficients (\\(\\beta_0\\) and \\(\\beta_1\\)) are actually random variables, because their value depends on the data and the data are typically from random sampling. In this example, if we happen to sample a different set of houses at Bonston, we may get different coefficient estimates. Lets illustrate that by randomly drawing 100 houses from the Boston dataset and re-estimate the model based on the sub-sample. # Take 100 houses randomly from the Boston dataset Boston_sample=Boston[sample(1:506, 100, replace=FALSE)] # estimate coefficients based on the sub-sample fit_sub=lm(formula=medv~lstat, data=Boston_sample) # plot the sub-sample and fitted line plot(Boston_sample$lstat, Boston_sample$medv) points(Boston_sample$lstat, fitted(fit_sub), col=&quot;blue&quot;, type=&quot;l&quot;) Each time we run the above code, we randomly generate a new dataset and obtain a new estimated regression line. In this example, although the sample is randomly drawn and thus different, the shope of the line is quite rubust. This is because there is a strong linear relationship between medv and lstat, and thus this relationship should sustain in the random sample. In this case, we says the coefficient lstat is significant. Since the coefficient estimates are random variable, we can obtain their confidence interval as below: # get confidence interval with confidence level of 0.95 confint(fit1, level=0.95) ## 2.5 % 97.5 % ## (Intercept) 33.448457 35.6592247 ## lstat -1.026148 -0.8739505 This means, with probability of 95%, \\(\\beta_1\\) will be between -1.026148 and -0.8739505. Notice that, this interval does not contain 0, meaning that we are pretty sure the coefficient is not zero and has a consistent negative sign (i.e., the lstat has a significant impact on medv). In particular, the p-value is particularly used to determine whether the coefficient is significantly different from 0. p-value &lt; 5% is commonly considered that the coefficient is significantly different from 0. We will only intercept the impact of significant independent variable on dependent variable, because, as mentioned, we cannot differentiate the impact of insignifcant variable from 0. In this example, the estimated result suggests that: when lstat=0, then medv is 34.55; as lstat increases by 1, then medv will decreases on average by 0.95 (or 0.874~1.026). 18.3 Overall model fittness How good is our model? In other words, how does the linear line fit our data? Lets first examine the model fittness through a scatter plot. # plot the original data plot(Boston$lstat, Boston$medv) # fitted() function returns the fitted value points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;) As seen above, the blue line is the fitted line: for a given lstat, the blue line denotes our prediction of the corresponding medv. As seen, there is gap between the actual medv and our prediction. That is the error terms. The linear linear capture the linear relationship, but the error term is quite evident. In addition to the above visual inspection, we can use two metrics to determine how good the model is. R-squared, as well as the adjusted R-squared F-statistic and the correponding p-value R-squared (R2) measures the percent of variation in dependent variable that is explained by the model. The formula for R-squared is: \\[R2=1-\\frac{RSS}{\\sum(y_i-\\bar{y})^2}=1-\\frac{\\sum_i(y_i-\\beta_0-\\beta_1x_i)^2}{\\sum_{i}(y_i-\\bar{y})^2}\\] Here \\(\\sum_{i}(y_i-\\bar{y})^2\\) represents the total variation in y that we want to explain. \\(\\sum_i(y_i-\\beta_0-\\beta_1x_i)^2\\) represents the variation left unexplained by our linear regression model. \\(\\frac{\\sum_i(y_i-\\beta_0-\\beta_1x_i)^2}{\\sum_{i}(y_i-\\bar{y})^2}\\) represent the precents of variation in y that is left unexplained (i.e., the variation in the error terms). Since our goal is to explain the variation in dependent variable, a higher R-square means more variation is explained by the model. However, R-squared always increases as more independent variables are included in the model. Thus, we usually use adjusted R-squared instead, which will penalize the model complexity. Adding irrelevant independent variables increases R-squared, but will lower adjusted R-squared. We will come back to this when discussing model selection. F-statistic is a statistical test whether all the coefficients (except intercept) are jointly 0. If that is true, then the model is useless. The model with p-value of F-statistics less than 0.05 is considered a good model, i.e., at least some coefficients are not 0. Lets look at the model fittness for the Bonston house price example. summary(fit1) ## ## Call: ## lm(formula = medv ~ lstat, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.168 -3.990 -1.318 2.034 24.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.55384 0.56263 61.41 &lt;2e-16 *** ## lstat -0.95005 0.03873 -24.53 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.216 on 504 degrees of freedom ## Multiple R-squared: 0.5441, Adjusted R-squared: 0.5432 ## F-statistic: 601.6 on 1 and 504 DF, p-value: &lt; 2.2e-16 For this example, the R-squared is 0.5441, meaning 54.41% of the variation in medv is explained by the model; Adjusted R-squared is not much different, meaning the variables included are mostly relevant. The p-value of the F-statistic is less than 2.2e-16, meaning the model is very significant. 18.3.1 Regression Diagnoise In order for the estimated linear line to be the best fitted line, we need to make an assumption on the error terms \\(\\epsilon\\): \\(\\epsilon \\sim N(0, \\sigma^2 )\\), which means \\(\\epsilon\\) on average should be 0, regardless the value of x and y. \\(\\epsilon\\) has constant stdev deviation, regardless the value of x and y (No Heteroskedasticity). \\(\\epsilon\\) are not correlated (No Autocorrelation). In most applications, we are fine to assume these to be true. Even if these assumptions are not true, the point estimates of the coefficients is still unbiased; we just cannot trust their confidence interval. Nevertheless, there is a quick way to check whether these assumptions are true by plotting the residuals. par(mfrow=c(2,2)) # arrange subplot in 2 by 2 matrix plot(fit1) Draw your attention on the first subplot (the up-left one). It seems to suggest that the average of residuals does not always equal to 0: it seems to be above 0 when fitted value is very small or very large. This can also be seen from the plot of the fitted value. plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;) The above plot suggests that our linear model underestimates medv when lstat is either very small or very large. The above diagnose seems to suggests that the linear regression model may not provide a perfect approximation to the relationship between medv and lstat. We will discuss later how to address the non-linear relationship. 18.4 Use estimated model for prediction E.g., we want to predict what medv could be if lstat is 10:20. We can obtain both point estimates, as well as the 95% prediction interval. predict(fit1,data.table(lstat=c(10:20)), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 25.05335 24.47413 25.63256 ## 2 24.10330 23.54602 24.66057 ## 3 23.15325 22.60809 23.69841 ## 4 22.20320 21.65967 22.74673 ## 5 21.25315 20.70067 21.80563 ## 6 20.30310 19.73159 20.87461 ## 7 19.35305 18.75338 19.95272 ## 8 18.40300 17.76727 19.03873 ## 9 17.45295 16.77450 18.13140 ## 10 16.50290 15.77626 17.22955 ## 11 15.55285 14.77355 16.33216 E.g., looking at the first row (i.e., lstat=10), the prediction of medv is 25.05 or with 95% probability, medv should be between 22.47~25.63. 18.5 Summary Correlation matrix and scatter plot matrix are two quick ways to inspect whether linear relationship exists in your data. We can estiamte and quantify the linear relationship between dependent variable and independent variable using linear equation: \\(y=\\beta_0+\\beta_1x+\\epsilon\\), where \\(\\beta_1\\) measures the change of y related to each additional unit of x. This impact is linear becuase it does not depends on the value of x or y. The syntax for estimating linear regression in R is: lm(formula=y~x, data). Estimated coefficients are randomly variables. We learn how to deterine whether they are significant different from 0. Learn to access the model fitness through R-squared, adj-R2, and F-statistics. Learn how to use the estimated model for prediction. "],["log-tranformation-and-polynomial-terms.html", "Chapter 19 Log Tranformation and Polynomial Terms 19.1 Log-transformation 19.2 Quadratic regression 19.3 Polynomial regression 19.4 Summary 19.5 Exercise", " Chapter 19 Log Tranformation and Polynomial Terms In the previous chapter, we estimated a linear line to approximate the relationship between medv and lstat. However, the approximation is not perfect because the scatter plot seems to suggests a non-linear relationship betwee medv and lstat. Boston=fread(&quot;data/Boston.csv&quot;) plot(Boston$lstat,Boston$medv, xlab =&quot;lstat: percent of low income population in the neighborhood&quot;, ylab = &quot;medv: house price&quot;) fit1=lm(medv~lstat, data=Boston) points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;) The above plot suggests that our linear model underestimated medv when lstat is either very small or very large. There are two possible solutions to mitigate this issue: log-transformation quadratic and polynomial terms 19.1 Log-transformation Log-transformation can be used when the dependent/independent variable is right skewed, meaning the variable does not cluster around the center (i.e., the bell shape), but has more mass on the right of the spectrum. A good way to check whether an variable is skewed is through histogram: hist(Boston$lstat, breaks = 40) The histogram shows that lstat is right skewed since most of the mass is leaning towards right). Now, lets try log-transform the variable lstat and examine its histogram: hist(log(Boston$lstat), breaks = 40) Also notice that, as a result of log-transformation, the variation in the variable is reduced (the range of lstat is 0-40; but log(lstat) is 0.5-4). Thus, the log-transformation is also widely used for reducing the variation in the data. When the data is left-skewed, we need to use Box-cox transformation (which we will not discuss here). Here is what the regression looks like when we log-transform the lstat variable. fit2=lm(medv~log(lstat), data=Boston) summary(fit2) ## ## Call: ## lm(formula = medv ~ log(lstat), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.4599 -3.5006 -0.6686 2.1688 26.0129 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 52.1248 0.9652 54.00 &lt;2e-16 *** ## log(lstat) -12.4810 0.3946 -31.63 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.329 on 504 degrees of freedom ## Multiple R-squared: 0.6649, Adjusted R-squared: 0.6643 ## F-statistic: 1000 on 1 and 504 DF, p-value: &lt; 2.2e-16 We can use summary() to take a quick looked the regression results. As seen, the fitted line is: \\[medvFit=52.12-12.48*log(lstat)\\] Lets plot the fitted line to see the difference. # fitted() is the function to get the fitted value plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;, type = &quot;l&quot;) points(Boston$lstat, fitted(fit2), col=&quot;red&quot;, cex=0.5) After the log transformation, the model captures the non-linear relationship between medv and lstat, and seems to provide a better fit to the data. The underestimation at both the left/right end is to some extent mitigated. 19.2 Quadratic regression Alternatively, we can add the quadratic terms to capture the nonlinear relationship between medv and lstat. Formally, the quadratic regression is denoted as below: \\[y=\\beta_0+\\beta_1x+\\beta_2x^2+\\epsilon\\] Here the quadratic term \\(x^2\\) is included to capture the non-linear relationship between x and y. In R, the quadratic term is represented by I(x^2). fit3=lm(medv~lstat+I(lstat^2),data=Boston) summary(fit3) ## ## Call: ## lm(formula = medv ~ lstat + I(lstat^2), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2834 -3.8313 -0.5295 2.3095 25.4148 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42.862007 0.872084 49.15 &lt;2e-16 *** ## lstat -2.332821 0.123803 -18.84 &lt;2e-16 *** ## I(lstat^2) 0.043547 0.003745 11.63 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.524 on 503 degrees of freedom ## Multiple R-squared: 0.6407, Adjusted R-squared: 0.6393 ## F-statistic: 448.5 on 2 and 503 DF, p-value: &lt; 2.2e-16 According the estimated result, the fitted line is: \\[medvFit= 42.86-2.33*lstat+0.04*lstat^2\\] Now, lets plot the fitted line to see the difference. plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;, type = &quot;l&quot;) points(Boston$lstat, fitted(fit2), col=&quot;red&quot;, cex=0.5) points(Boston$lstat, fitted(fit3), col=&quot;green&quot;, cex=0.5) The quadartic model corrects (probably over corrects) the underestimation of medv when lstat is too small or too large. However, it may overestimate medv when lstat is large. Thus, we may want to add higher order polynomial terms to aviod that. 19.3 Polynomial regression Polynomial is a genearlization of quadratic regression. Formally, the \\(k^{th}\\) order polynomial regression model is generally denoted as below: \\[y=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+...+\\beta_kx^k+\\epsilon\\] The polynomial terms \\(x^2\\), ,\\(x^k\\) are used to capture the non-linear relationship between x and y. This can be easily estimated in R: # 6th order polynomial regression fit4=lm(medv~poly(lstat,6), data=Boston) summary(fit4) ## ## Call: ## lm(formula = medv ~ poly(lstat, 6), data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.7317 -3.1571 -0.6941 2.0756 26.8994 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22.5328 0.2317 97.252 &lt; 2e-16 *** ## poly(lstat, 6)1 -152.4595 5.2119 -29.252 &lt; 2e-16 *** ## poly(lstat, 6)2 64.2272 5.2119 12.323 &lt; 2e-16 *** ## poly(lstat, 6)3 -27.0511 5.2119 -5.190 3.06e-07 *** ## poly(lstat, 6)4 25.4517 5.2119 4.883 1.41e-06 *** ## poly(lstat, 6)5 -19.2524 5.2119 -3.694 0.000245 *** ## poly(lstat, 6)6 6.5088 5.2119 1.249 0.212313 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.212 on 499 degrees of freedom ## Multiple R-squared: 0.6827, Adjusted R-squared: 0.6789 ## F-statistic: 178.9 on 6 and 499 DF, p-value: &lt; 2.2e-16 Now, lets plot the fitted line to see the difference. # fitted() is the function to get the fitted value plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit1), col=&quot;blue&quot;, type = &quot;l&quot;) points(Boston$lstat, fitted(fit2), col=&quot;red&quot;, cex=0.5) points(Boston$lstat, fitted(fit3), col=&quot;green&quot;, cex=0.5) points(Boston$lstat, fitted(fit4), col=&quot;yellow&quot;, cex=0.5) The \\(6^{th}\\) order polynomial regression seems to provide a good fit to the data. You may be wondering, now we have four different model to predict medv. For a given lstat, each model predicts a differnt medv. Which model should we trust? Which model is the best model? We will come back to this question in the model selection chapter. 19.4 Summary log-transformation can correct the right-skewness of the variable, and reduce variation in the variable. We can use log-transformation and polynoimal terms to capture non-linear relationship between dependent variable and independent variables. 19.5 Exercise Use nox (nitrogen oxides concentration (parts per 10 million)) to explain medv through a linear regression model, i.e., \\[medv=\\beta_0+\\beta_1 nox+\\epsilon\\] You need to try the following models: Model 1: linear regression without any transformation Model 2: log-transformation on nox Model 3: add quadartic term of nox Model 4: \\(10^{th}\\) order polynomial regression 19.5.1 Solution Lets first examine the data visually. corr=cor(Boston[,c(&quot;medv&quot;,&quot;nox&quot;)]) corrplot.mixed(corr, number.cex = .7, tl.cex=0.8) plot(Boston$nox, Boston$medv) There seems to exist a negative linear relationship between nox and medv: house price in region with higher nox (more air pollution) has lower price. Lets quantify such linear relationship: # estimate four different models fit1=lm(formula=medv~nox, data=Boston) fit2=lm(formula=medv~log(nox), data=Boston) fit3=lm(formula=medv~nox+I(nox^2), data=Boston) fit4=lm(formula=medv~poly(nox,10), data=Boston) # generate the fitted value for each model plot(Boston$nox, Boston$medv) points(Boston$nox, fitted(fit1), col=&quot;blue&quot;, type = &quot;l&quot;) points(Boston$nox, fitted(fit2), col=&quot;red&quot;, cex=0.5) points(Boston$nox, fitted(fit3), col=&quot;green&quot;, cex=0.5) points(Boston$nox, fitted(fit4), col=&quot;yellow&quot;, cex=0.5) It seems model 1-3 provide similar fit, however model 4 (polynomial-10) seems to have a problem of over-fitting: the model is too specific to the training data, and cannot be generalizable for out-of-sample prediction. Thus, it is not always the best to have a complicated model. We will come back to the problem of over-fitting in the model selection chapter. "],["mutiple-linear-regression.html", "Chapter 20 Mutiple Linear Regression 20.1 Mutiple linear regression 20.2 Categorical variables 20.3 Interprate coefficients - Keeping all other variables the same 20.4 Multicollinearity 20.5 Excerise", " Chapter 20 Mutiple Linear Regression # Load packages into R: library(data.table) library(corrplot) library(ggplot2) In this chapter, we will study multiple linear regression, i.e., including more than one independent variable into the regression model. Load and clean the Boston house price data. Boston=fread(&quot;data/Boston.csv&quot;) # change location from character as factor Boston[,location:=factor(location)] # change chas (0/1 variable) from numeric to factor Boston[,chas:=factor(chas)] 20.1 Mutiple linear regression Use Boston house dataset as an example. In addition to lstat, we want to include age (i.e., proportion of owner-occupied units built prior to 1940) into the model to predict medv. The age variable captures how established the neighborhood is. The resulting model is thus: \\[medv=\\beta_0+\\beta_1*lstat+\\beta_2*age+\\epsilon\\] In, R, we can easily estimate the above model as follow: # multiple linear regression with lstat and age as predictors. fit1=lm(medv~lstat+age,data=Boston) summary(fit1) ## ## Call: ## lm(formula = medv ~ lstat + age, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.981 -3.978 -1.283 1.968 23.158 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.22276 0.73085 45.458 &lt; 2e-16 *** ## lstat -1.03207 0.04819 -21.416 &lt; 2e-16 *** ## age 0.03454 0.01223 2.826 0.00491 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.173 on 503 degrees of freedom ## Multiple R-squared: 0.5513, Adjusted R-squared: 0.5495 ## F-statistic: 309 on 2 and 503 DF, p-value: &lt; 2.2e-16 As seen, the coefficient of both lstat and age are significant (because their p-value is less than 0.05) in explaining the variation in house price. Lets plot the fitted value to say how the model fits the house price data better than a simple linear regression model. # the simple regression with only lstat as predictor fit0=lm(medv~lstat, data=Boston) plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit0), col=&quot;black&quot;, type = &quot;l&quot;) points(Boston$lstat, fitted(fit1), col=&quot;red&quot;, cex=0.5) When including age, the fitted line is no longer a straight line. This is because for houses with same lstat may be located in region with different age, thus having different medv. Thus, multiple linear regression utilize variation in multiple independent variables to explain variation in dependent varaible. We can further include all independent variables in the data to construct the multiple linear regression. Here, . is a special character which denotes all independent variables in the dataset so that we do not need to type all variables. # multiple regression with all predictors fit2=lm(medv~.,data=Boston) summary(fit2) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.3580 -2.8810 -0.6743 1.7674 27.4797 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.817e+01 5.102e+00 5.522 5.46e-08 *** ## crim -6.652e-02 3.221e-02 -2.065 0.039448 * ## zn 4.365e-02 1.428e-02 3.056 0.002364 ** ## indus -5.592e-02 6.109e-02 -0.915 0.360405 ## chas1 3.053e+00 8.720e-01 3.501 0.000505 *** ## nox -1.555e+01 3.877e+00 -4.011 6.98e-05 *** ## rm 4.105e+00 4.200e-01 9.774 &lt; 2e-16 *** ## age -2.011e-03 1.343e-02 -0.150 0.880963 ## dis -1.483e+00 2.035e-01 -7.287 1.28e-12 *** ## rad 3.848e-01 2.390e-01 1.610 0.107971 ## tax 1.507e-04 2.504e-03 0.060 0.952039 ## ptratio -8.025e-01 1.329e-01 -6.039 3.08e-09 *** ## black 8.419e-03 2.731e-03 3.083 0.002166 ** ## lstat -5.216e-01 5.173e-02 -10.084 &lt; 2e-16 *** ## locationnorth 4.462e-01 6.151e-01 0.725 0.468563 ## locationsouth -6.476e-01 6.197e-01 -1.045 0.296563 ## locationwest -6.480e-01 6.265e-01 -1.034 0.301527 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.824 on 489 degrees of freedom ## Multiple R-squared: 0.7336, Adjusted R-squared: 0.7249 ## F-statistic: 84.17 on 16 and 489 DF, p-value: &lt; 2.2e-16 plot(Boston$lstat, Boston$medv) points(Boston$lstat, fitted(fit0), col=&quot;black&quot;, type = &quot;l&quot;) points(Boston$lstat, fitted(fit1), col=&quot;red&quot;, cex=0.5) points(Boston$lstat, fitted(fit2), col=&quot;blue&quot;, cex=0.5) The plot above demonstrates the advantage of multiple linear regression. Including more predictors means less variation is left in the error terms; thus we can better explain the variation in dependent variables through the model. There is a very useful shortcut to construct multiple linear regression. E.g., we want to construct a regression model with all independent variables except dis (weighted mean of distances to five Boston employment centres). As the correlation matrix shows, dis is highly correlated with indus, nox and age. In other words, the variation in dis is likely captured by indus, nox and age; including dis may potentially cause multicollinearity. In R, .-dis mean all independent variable except dis: fit3=lm(medv~.-dis,data=Boston) summary(fit3) ## ## Call: ## lm(formula = medv ~ . - dis, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.2404 -2.8241 -0.8356 1.6835 28.9419 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.8713686 5.0113756 2.968 0.003149 ** ## crim -0.0368025 0.0336076 -1.095 0.274027 ## zn 0.0014007 0.0137296 0.102 0.918783 ## indus 0.0334688 0.0629428 0.532 0.595151 ## chas1 3.1689788 0.9170118 3.456 0.000596 *** ## nox -7.2861413 3.8996165 -1.868 0.062299 . ## rm 4.5156560 0.4377778 10.315 &lt; 2e-16 *** ## age 0.0267528 0.0134972 1.982 0.048026 * ## rad 0.3180850 0.2511572 1.266 0.205944 ## tax 0.0001155 0.0026337 0.044 0.965047 ## ptratio -0.8915436 0.1391838 -6.406 3.52e-10 *** ## black 0.0088820 0.0028717 3.093 0.002095 ** ## lstat -0.5376894 0.0543571 -9.892 &lt; 2e-16 *** ## locationnorth 0.3880958 0.6469543 0.600 0.548863 ## locationsouth -0.9745294 0.6501057 -1.499 0.134509 ## locationwest -0.6973450 0.6589279 -1.058 0.290439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.074 on 490 degrees of freedom ## Multiple R-squared: 0.7047, Adjusted R-squared: 0.6957 ## F-statistic: 77.96 on 15 and 490 DF, p-value: &lt; 2.2e-16 20.2 Categorical variables Now, lets draw our attention on the variable location. This is a categorical variable in R, and it takes only 4 value; east, north, south, west. Note that, we have saved this variable as factor, which is the data type in R to save categorical variables. We can check the frequency of the location variable as belows: table(Boston$location) ## ## east north south west ## 119 132 130 125 In the previous model, we see that R automatically create three dummy variables: locationnorth, locationsouth, locationwest to represents the locations. The dummy variables take value of only 0/1. E.g., locationnorth=1 means the location is north. You many wondering why R does not create locationeast dummy. This is because the location=east can be represented by locationnorth locationsouth and locationwest all equal to 0. The location=east is called the base case. In general, for a categorical variable with n levels, n-1 dummy variables will be created, and the left level is the base level. To have a better understand of the categorical variables in regression, lets regress medv on lstat and location: fit4=lm(medv~ lstat+location, data=Boston) summary(fit4) ## ## Call: ## lm(formula = medv ~ lstat + location, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.219 -4.033 -1.411 2.067 24.597 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.94062 0.76892 45.441 &lt;2e-16 *** ## lstat -0.95546 0.03874 -24.660 &lt;2e-16 *** ## locationnorth 0.46252 0.78379 0.590 0.555 ## locationsouth -1.29300 0.78799 -1.641 0.101 ## locationwest -0.43233 0.79546 -0.543 0.587 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.2 on 501 degrees of freedom ## Multiple R-squared: 0.5492, Adjusted R-squared: 0.5456 ## F-statistic: 152.6 on 4 and 501 DF, p-value: &lt; 2.2e-16 Again, R automatically create three dummy variables: locationnorth, locationsouth, locationwest to represents the locations. How to interpret the coefficients of locationnorth, locationsouth, locationwest? fit_dt=data.table(lstat=Boston$lstat, location=Boston$location, medv_fit=fitted(fit4)) head(fit_dt) ## lstat location medv_fit ## 1: 4.98 west 29.75011 ## 2: 9.14 east 26.20773 ## 3: 4.03 west 30.65779 ## 4: 2.94 south 30.83857 ## 5: 5.33 south 28.55502 ## 6: 5.21 south 28.66968 ggplot()+ geom_point(data=Boston, aes(lstat,medv, color=location), alpha=0.5)+ geom_line(data=fit_dt[location==&quot;north&quot;], aes(lstat,medv_fit, color=location))+ geom_line(data=fit_dt[location==&quot;south&quot;], aes(lstat,medv_fit, color=location))+ geom_line(data=fit_dt[location==&quot;west&quot;], aes(lstat,medv_fit, color=location))+ geom_line(data=fit_dt[location==&quot;east&quot;], aes(lstat,medv_fit, color=location))+ theme_minimal() The above chart demonstrates the coefficients of categorical variables location visually. Each color represents a particular location, the fitted line for different location has different intercept to capture the fact that the average house price at different location is different. Formally, the coefficient of locationnorth (0.463) means the average house price in the north is 0.463 higher than that in the base case (i.e., in the east). The coefficient of locationsouth (-1.293) means the average house price in the south is 1.293 lower than that in the base case (i.e., in the east). The coefficient of locationwest (-0.432) means the average house price in the south is 0.43233 lower than that in the base case (i.e., in the east). The great thing about R is that we do not need to manually define dummy variables to represent a categorical variable. R will do that automatically. You just need to understand the coefficients of the dummy variables. However, you must first save the categorical variable into factor to tell R that is a categorical variable. 20.3 Interprate coefficients - Keeping all other variables the same Now, lets look into the estimated coefficients of the mutliple linear regression model. For the first model, \\(medv=\\beta_0+\\beta_1lstat+\\beta_2age+\\epsilon\\), our estimates suggests the following best linear line: \\[medvFit=33.22-1.032lstat+0.034age+\\epsilon\\] Both lstat and age has p-value less than 5%, thus they both are significant predictors. Here -1.032l represents the marginal impact of lstat on medv: keeping everything else the same, if medv increases by 1, then medv will decreases by -1.0321. Similarly, keeping everything else the same, if age increase by 1, medv will increase by 0.034. It is very important to note the phrase keeping everything else the same. It means we only allow one variable to change a time when interpreting the coefficients. However, it is sometime hard to keep all other variables unchanged. In many cases it is even not be possible. E.g., we want to predict the medv using their \\(lstat\\), \\(lstat^2\\), \\(sqft\\) and \\(bedrooms\\). \\[medv=\\beta_0+\\beta_1*lstat+\\beta_2 *lstat^2+\\beta_3*sqft+\\beta4*bedrooms+\\epsilon\\] However, it is hard to argument keeping \\(sqft\\) unchanged while changing \\(bedrooms\\). And you cannot keep \\(medv^2\\) unchanged while changing \\(lstat\\). Thus, you need to be cautions to determine whether keeping all other variable unchanged is possible when interpreting the coefficients. 20.4 Multicollinearity Our linear regression model will have the multicollinearity problem when two or more independent variables are highly correlated because the regression does not know which variable to credit with changes in dependent variable. Lets look at one example. Suppose we are trying regress height of a person on his/her left foot and right foot: \\(height=\\beta_0+\\beta_1*left+\\beta_2* right+\\epsilon\\) # simulate the data right=rnorm(50, 20.5, 2) left=right+rnorm(50, 0, 0.5) height=left*7.9+rnorm(50,0,3) height_dat=data.table(right, left, height) head(height_dat) ## right left height ## 1: 19.26421 19.47743 157.1016 ## 2: 15.91814 15.78225 125.3999 ## 3: 20.85503 20.84592 161.5192 ## 4: 20.97105 20.27620 162.2886 ## 5: 18.20132 18.18502 143.9461 ## 6: 23.90023 24.77126 191.7123 Lets examine the correlation matrix and scatter plot matrix: corr=cor(height_dat) corrplot(corr, type = &quot;upper&quot;, method = &quot;number&quot;, number.cex = .7, tl.cex=0.8) pairs(height_dat, pch = 19, cex = 0.1, lower.panel=NULL) # cex sets the size of the points As seen in the scatter plot, both right and left are highly correlated with height, i.e., both are good predictor of height. However, right and left are also highly correlated. If we regress heigh on both left and right, we have: fit_test=lm(height~left+right, data=height_dat) summary(fit_test) ## ## Call: ## lm(formula = height ~ left + right, data = height_dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.6050 -2.3737 0.3324 1.7371 8.1379 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2345 4.4606 0.501 0.619 ## left 8.2889 0.9246 8.965 9.63e-12 *** ## right -0.4999 0.9763 -0.512 0.611 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.96 on 47 degrees of freedom ## Multiple R-squared: 0.9681, Adjusted R-squared: 0.9668 ## F-statistic: 714.2 on 2 and 47 DF, p-value: &lt; 2.2e-16 The result is not what we expected becuase the coefficient of right is negative. This is because left and right are almost always changing together, thus the regression model does not know which one to credit with changes in height. The possible solution is to drop the one of the independent variable that is highly correlated with other independent variables. This will not lead to much loss of information because the variation in the dropped variable is mostly captured by the ones that are highly correlated with the dropped variable. While in most cases it is impossible to see absolutely no correlations among independent variables. But we will begin to worry about multicollinearity if correlation between independent variable is &gt; 0.70 or &lt; -0.70. We definitely need to drop some independent variables if correlation between them is &gt; 0.90 or &lt; -0.90. While, it seems that we are again facing the problem of selecting the best set of independent variables into our regression model. This time, we are selecting variables to aviod multicollinearity. In the next chapter, we will start to discuss how to select the best linear regression model. 20.5 Excerise chas is another categorical variable to indicate whether the house is on the charles river. Develop a linear regression model with lstat and chas, and interpret the coefficient of chas. 20.5.1 Examine the frequency of chas table(Boston$chas) ## ## 0 1 ## 471 35 20.5.2 Visualize the house price with ggplot2 Create a scatter plot between medv and lstat, mark the color of points based on chas. ggplot()+ geom_point(data=Boston, aes(lstat,medv, color=chas), alpha=0.5)+ theme_minimal() 20.5.3 estimate the linear regression model fit=lm(medv~lstat+chas,data=Boston) summary(fit) ## ## Call: ## lm(formula = medv ~ lstat + chas, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.782 -3.798 -1.286 1.769 24.870 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.09412 0.56067 60.809 &lt; 2e-16 *** ## lstat -0.94061 0.03804 -24.729 &lt; 2e-16 *** ## chas1 4.91998 1.06939 4.601 5.34e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.095 on 503 degrees of freedom ## Multiple R-squared: 0.5626, Adjusted R-squared: 0.5608 ## F-statistic: 323.4 on 2 and 503 DF, p-value: &lt; 2.2e-16 20.5.4 plot the fitted line fit_dt=data.table(lstat=Boston$lstat, chas=Boston$chas, medv_fit=fitted(fit)) head(fit_dt) ## lstat chas medv_fit ## 1: 4.98 0 29.40987 ## 2: 9.14 0 25.49692 ## 3: 4.03 0 30.30345 ## 4: 2.94 0 31.32872 ## 5: 5.33 0 29.08065 ## 6: 5.21 0 29.19353 ggplot()+ geom_point(data=Boston, aes(lstat,medv, color=chas), alpha=0.5)+ geom_line(data=fit_dt[chas==1], aes(lstat,medv_fit, color=chas))+ geom_line(data=fit_dt[chas==0], aes(lstat,medv_fit, color=chas))+ theme_minimal() The ultimate question: what is the coefficient of chase is? Is it significant? What does it mean? "],["best-subset-model-selection.html", "Chapter 21 Best Subset Model Selection 21.1 Best Subset Model Selection 21.2 Deterimining the best model with adj-R2, Cp, AIC, and BIC 21.3 Forward/Backward selection 21.4 Summary:", " Chapter 21 Best Subset Model Selection So far, we have built many different models to predict Boston house price. For any given house, each model predicts a different medv. You must be wondering: which model should I trust and use eventually? Today, we will learn the technique to select the best models. We will continue to use the Boston house dataset for illustration for consistence. The Boston dataset has 14 independent variables and 1 dependent variable. Boston=fread(&quot;data/Boston.csv&quot;) Boston[,location:=factor(location)] Boston[,chas:=factor(chas)] dim(Boston) ## [1] 506 15 Each variable can be either included or not included in the model. Accordingly, we will have \\(2^14\\) number of possible models. The goal of model selection is to determine which subset of variable should be used for the linear regression model to predict medv. In general, suppose we have p independent variable, \\[y=\\beta_0+ \\beta_1x_1+ ...+\\beta_p*x_p + \\epsilon\\] It is not necessarily the best to include all \\(x_1, ...x_p\\) into the model due to potential irrelevant variable or multicollinearity. Therefore, there are together \\(2^p\\) model candidate because each variable can be included or not included. Our task is find the best one! This seems an impossible mission. But, with the R Package leaps, we can perform an exhaustive search for all the \\(2^p\\) model candidate and identify the the best subsets of x-variables for predicting y. This package is developed by Thomas Lumley, a statistical professor at University of Auckland. Install the leaps package. install.packages(&quot;leaps&quot;) Load the package into R: library(leaps) ## Warning: package &#39;leaps&#39; was built under R version 4.0.5 21.1 Best Subset Model Selection Best subset regression algorithm exhausts all the \\(2^p\\) combination to find the best model for each model size of \\(d=1,2.., p\\). To be specific, for model size of 1 (only 1 x-variable will be included), the algorithm will return the best one variable to predict y-variable; for model size of 2 (2 x-variables will be included), the algorithm will return the best two variables to predict y-variable Here is the R code for best subset regression: bestFit=regsubsets(medv~.,data=Boston,nvmax=18) # save the regression results bestFit ## Subset selection object ## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 18) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: exhaustive Here nvmax sets the maximum number of variable to include. We set nvmax =16 or larger since we have 14 independent variable (R will automatically define 3 location dummies). use summary() to get a quick summary of the regression results bestFit_sum=summary(bestFit) bestFit_sum ## Subset selection object ## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 18) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: exhaustive ## crim zn indus chas1 nox rm age dis rad tax ptratio black lstat ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## locationnorth locationsouth locationwest ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; Here summary() reports the best subset of variables for each model size. Here is how we read the output report: each row represents a model (with the row ID correponsonds size of the model), an asterisk specifies that the variable is included in the corresponding model. E.g., when n=1 (model with only 1 variables), the best variable to include is lstat; when n=2 (model with two variables), the best variables to include is lstat and rm; and so on so forth. Thus, we have narrowed down from \\(2^p\\) possible models to 16 models with distinct model sizes. 21.2 Deterimining the best model with adj-R2, Cp, AIC, and BIC Now, the question is to pin down the best model among the remaining 16 models. We will discuss the three measures (i.e., adj-R2, Cp, and BIC) for selecting the best model. These three metrics measure how good the model is in fitting the data while penalizing the complexity of the model. bestFit_sum is a list that contains these measures (i.e., adj-R2, Cp, and BIC). Lets first examine the objects in the list: names(bestFit_sum) ## [1] &quot;which&quot; &quot;rsq&quot; &quot;rss&quot; &quot;adjr2&quot; &quot;cp&quot; &quot;bic&quot; &quot;outmat&quot; &quot;obj&quot; 21.2.1 R-squared is not a good criteria for model selection As mentioned previously, R-sq always inreases as more variables are included in the model. We can plot the R-sq of the 16 models to demonstrate that: plot(bestFit_sum$rsq, xlab = &quot;number of variables&quot;, ylab=&quot;R-square&quot;, type=&quot;b&quot;) # return the model index of maximum R-sq opt_id=which.max(bestFit_sum$rsq) # add the maximum R-sq on the plot points(opt_id,bestFit_sum$rsq[opt_id],pch=20,col=&quot;red&quot;) The above chart shows that R-sq always favor model with larger number of variables, i.e., the model with all independent variables included. As mentioned, this is not always the best because some variables may be totally irrelevant or cause multicollinearity issue. 21.2.2 Select the best model using Adjusted R2 As mentioned, R-squared always increases as more independent variables are included in the model. Adjusted R-squared, instead, will penalize the model complexity. Adding irrelevant independent variables increases R-squared, but will lower adjusted R-squared. The formula for adj-R2 is \\[AdjR2=1-(1-R2)\\frac{n-1}{n-d-1}\\] Here R2 is the R-squared, n is the sample size, and d is the number of predictors. Keeping everything the same, as d increases, Adj-R2 will decreases; thus Adj-R2 will penalize the complexity of the model. A higher Adj-R2 means a better model. Lets plot the adj-R2 for different model from the best subset regression result: # plot the Adj-R2 for different models of size 1 to 16 plot(bestFit_sum$adjr2,xlab=&quot;number of variables&quot;,ylab=&quot;Adj R2&quot;,type=&quot;b&quot;) # return the model index of maximum adj-R2 opt_id=which.max(bestFit_sum$adjr2) points(opt_id,bestFit_sum$adjr2[opt_id],pch=20,col=&quot;red&quot;) As the above plot shows, according to adj-R2, the best model is the 11th model, i.e., the model with 11 independent variables. We can check the variables and estiamted coefficients of the best model as below: coef(bestFit,11) ## (Intercept) crim zn chas1 nox ## 27.996958891 -0.067235171 0.043898771 2.994908911 -17.389642253 ## rm dis rad ptratio black ## 4.158308888 -1.438223278 0.404644153 -0.827872396 0.008366116 ## lstat locationnorth ## -0.524336874 0.842588270 21.2.3 Select the best model using Cp Mallowss Cp is another commonly used metric to assess the fit of a linear regression model. A smaller value of Cp means that the model is relatively precise and thus a better model. \\[ Cp=\\frac{1}{n}(RSS+2d\\hat{\\sigma}^2)\\] where \\(\\hat{\\sigma}^2\\) refers to the estimated variance of residual when all predictors are included. This terms represents the penality to the model complexity (i.e., Cp increases as \\(d\\) increases). # plot the Cp for different models of size 1 to 16 plot(bestFit_sum$cp, xlab=&quot;number of variables&quot;,ylab=&quot;Cp&quot;,type=&quot;b&quot;) # return the index of minimum Cp opt_id=which.min(bestFit_sum$cp) points(opt_id,bestFit_sum$cp[opt_id],pch=20, col=&quot;red&quot;) In this case, the Cp measure suggests the same model as adj-R2: the model with 11 variables. The AIC (Akaike information criterion) is other commonly used criterion for model selection. In the case of the best subset linear regression model, the formula for AIC is as below: \\[AIC = \\frac{1}{n\\hat{\\sigma}^2}(RSS+2d\\hat{\\sigma}^2)\\] As seen, the AIC (Akaike information criterion) criterion is proportional to Cp. Thus, will yeild the same model selection as Cp. 21.2.4 Select the best model using BIC BIC (Bayesian Information Criterion) is another commonly used measure to select model. The lower BIC means a better model. The formula to calcuate BIC for linear regression model is as follow: \\[BIC=\\frac{1}{n}(RSS+log(n)d\\hat{\\sigma}^2)\\] where \\(n\\) is the number of observation in the estiamtion dataset, \\(d\\) is the number of predictors in the model, and log() is the natural logarithm. Notice that BIC replaces the \\(2d\\hat{\\sigma}^2\\) used by Cp with a \\(log(n)d\\hat{\\sigma}^2\\) term, where n is the number of observations. Since \\(log(n)\\)&gt;2 for any \\(n&gt;7\\), the BIC generally places a heavier penalty on models with more variables, and hence results in the selection of smaller models than Cp. # plot the BIC for different models of size 1 to 16 plot(bestFit_sum$bic,xlab=&quot;number of variables&quot;,ylab=&quot;BIC&quot;,type=&quot;b&quot;) # return the index of minimum BIC opt_id=which.min(bestFit_sum$bic) points(opt_id,bestFit_sum$bic[opt_id],pch=20,col=&quot;red&quot;) As mentioned, the BIC is more aggregative in penalizing model complexity. Thus, BIC suggests a simpler model with 8 variables. We can examine the resulting model as below: coef(bestFit,8) ## (Intercept) zn chas1 nox rm ## 30.316950269 0.037808067 3.111061718 -16.687427958 4.116082349 ## dis ptratio black lstat ## -1.382714038 -0.881851067 0.009403764 -0.543125369 Another way to examine the Adj-R2/Cp/BIC measure for different model size as well as the variable in the models is through the following plot: plot(bestFit,scale=&quot;adjr2&quot;) Here is how to read the plot: Each rows represents a model; if the variable is blank on that row, then the variable is not included in that model. The y-axis corresponds the adj-R2 for that model. The best model is the one with highest adj-R2, i.e., the top row. We can create the same plot with Cp on the y-axis. plot(bestFit,scale=&quot;Cp&quot;) Similarly, each rows represents a model; if the variable is blank on that row, then the variable is not included in that model. The y-axis corresponds the Cp for that model. The best model is the one with lowest Cp, i.e., the top row. We can create the same plot with BIC on the y-axis. plot(bestFit,scale=&quot;bic&quot;) Similarly, each rows represents a model; if the variable is blank on that row, then the variable is not included in that model. The y-axis corresponds the BIC for that model. The best model is the one with lowest Cp, i.e., the top row. As seen, the BIC suggests a simpler model than adj-R2 and Cp do. Lets summerize. For a dataset with p predictors, there are \\(2^p\\) possible model combination. We use the best subset regression to narrow down to \\(p\\) possible models. Each of these \\(p\\) models corresponds to a distinct model size from \\(d=1, 2,..., p\\). It is not always true that the model with most variables is the best because some variables may be irrelevant or cause multicollinearity (meaning the variation in these variables has been captured by the other variables). We can use Adj-R2, Cp, and BIC to select the best model. BIC would penalize the model complexity more aggressively and thus resulting simpler model. Utimately, it is your call to go with which model (a taste of combination of science and art). I would prefer a simpler model (model suggests by BIC) due to its simplicity. coef(bestFit,8) ## (Intercept) zn chas1 nox rm ## 30.316950269 0.037808067 3.111061718 -16.687427958 4.116082349 ## dis ptratio black lstat ## -1.382714038 -0.881851067 0.009403764 -0.543125369 21.3 Forward/Backward selection The best subset algorithm does not scale up very well, thus limits its application. As \\(p\\) goes up, the number of models need to be estimated (i.e., \\(2^p\\)) increases exponentially. Especially when your dataset has many rows, evaluating one single model takes much time and computing power. The plot shows the number of linear models to be estimated for best subset algorithm: plot(c(1:20),2^c(1:20), type=&quot;b&quot;, xlab = &quot;number of variables&quot;, ylab=&quot;number of possible models&quot;) Two heuristics are typically used as a compromise: forward selection and backward selection. Forward selection will add variable sequentially: it starts with adding the one independent variable that fit the data best; the variable will always be kept and then adding the second independent variables which together fit the data best; then the third,  The Forward selection only requires estimating \\(p^2\\) models and thus can be scale up well. Here is a comparison between The plot shows the number of linear models to be estimated for best subset and forward selection algorithm: plot(c(1:20),2^c(1:20), type=&quot;b&quot;, xlab = &quot;number of variables&quot;, ylab=&quot;number of possible models&quot;) lines(c(1:20),c(1:20)^2, col=&quot;blue&quot;) Now, lets see how Forward selection works in R. fwd_fit=regsubsets(medv~., data=Boston, nvmax=18, method=&quot;forward&quot;) fwd_fit_sum=summary(fwd_fit) fwd_fit_sum ## Subset selection object ## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 18, method = &quot;forward&quot;) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: forward ## crim zn indus chas1 nox rm age dis rad tax ptratio black lstat ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## locationnorth locationsouth locationwest ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; summary() reports the best set of variables for each model size. Each row represents a model (with the row ID correponsonds size of the model), an asterisk specifies that a given variable is included in the corresponding model. When n=1 (i.e., model with only 1 variables), lstat is added to the model, and will not be removed in the remaining process; When n=2 (i.e., model with only 2 variables), rm is added to the model and will remain in the model in the remaining process; and so on so forth. Now, we have successfully narrowed down to the 16 different models with distinct size. Again, we can use adj-R2, Cp or BIC to select the best model. plot(fwd_fit,scale=&quot;bic&quot;) # plot the Cp for different models of size 1 to 16 plot(fwd_fit_sum$bic,xlab=&quot;number of variables&quot;,ylab=&quot;BIC&quot;,type=&quot;b&quot;) # return the index of minimum BIC opt_id=which.min(fwd_fit_sum$bic) points(opt_id,fwd_fit_sum$bic[opt_id],pch=20,col=&quot;red&quot;) # examine the variables and coefficients of the best model: coef(fwd_fit,id=opt_id) ## (Intercept) zn chas1 nox rm ## 30.316950269 0.037808067 3.111061718 -16.687427958 4.116082349 ## dis ptratio black lstat ## -1.382714038 -0.881851067 0.009403764 -0.543125369 We can also select the best model based on Cp or Adj-R2. It is a common mistake to confuse the model fit results (e.g., fwd_fit) and its summary (e.g., fwd_fit_sum). One quick way to help you remember when to use fwd_fit and when to use fwd_fit_sum is: fwd_fit saves the models themselves and fwd_fit_sum saves the how good these models are. Thus, when we need to retrieve coefficients information, we use fwd_fit; and when we want to retrieve R2, adj-R2, Cp and BIC, we use fwd_fit_sum. Backward selection is the reverse of forward selection: all variables are included in the model first, and the variables are removed sequentially to reduce the model complexity. The foreward/backward selection is essentially the same, and we can simply use forward selection. Note that there is one situation that we should not use backward selection: number of rows is less than number of variables (because including all variables will not be feasible because the resulting model cannot be estimated). back_fit=regsubsets(medv~., data=Boston, nvmax=18, method=&quot;backward&quot;) back_fit_sum=summary(back_fit) back_fit_sum ## Subset selection object ## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 18, method = &quot;backward&quot;) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: backward ## crim zn indus chas1 nox rm age dis rad tax ptratio black lstat ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## locationnorth locationsouth locationwest ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; 21.4 Summary: It is not always best to include every variable into the linear regression model. We need to select the best subset of variables to include. Best subset algoirthm exhausts every possible combination to select the best model of each size. We can use adj-R2, Cp, AIC, and BIC to select the best model from the output of the best subset regression. A model with higher Adj-R2, or lower Cp/AIC or lower BIC is a better model. Best subset model selection cannot be scaled up, so we use forward selection, which will add variables sequentially. Backward selection is equivalent to forward selection, but can be only used when number of rows &gt; number of variables. "],["model-selection-with-trainingtest-set.html", "Chapter 22 Model Selection with Training/Test Set 22.1 Create training/test set 22.2 Model selection 22.3 Model 4 has the problem of over-fitting 22.4 Other Commonly Used Metrics for Prediction performance 22.5 Summary", " Chapter 22 Model Selection with Training/Test Set Load the packages needed for this chapter: # Load packages into R: library(data.table) library(leaps) Using Adj-R, Cp, AIC, and BIC is convenient, but only work for linear models. Here we introduce another approach which can be used to select the best among any types of models: Use training set and test set for model selection. This is a very widely used method in any statistical and machine learning applications. The ultimate goal of any predictive models is to predict the future. Thus, to determine which model is best, the straightforward criteria for model selection is how well the model can predict the future. However, we typically do not wait for the future to unfold and then use this additional data to test our model. This is time consuming and the future may have been quite different from the time when the original dataset is collected. Therefore, a more feasible way is to divide the data into two sub-datasets of training set and test set: training set: the data used to estimate (also called train) the model. test set: the data that is not used in training the model and thus used to test the prediction performance of the model. The prediction performance of the model over the training set is called the in-sample prediction performance; while the prediction performance of the model over the test set is called out-of-sample prediction performance. Because the model is specifically calibrated to the training set, thus the in-sample prediction performance might be slightly better than out-of-sample. However, if the out-of-sample prediction is much worse than the in-sample prediction, then we have a problem of over-fitting: the model is too specific to the training data, and cannot be generalizable for out-of-sample prediction. The training/test model selection method is good way to detect and avoid over-fitting. 22.1 Create training/test set There is no definitive rule on how much proportation of data goes to training set or test set. A typical rule of thumb is 80% of data goes to training set and 20% goes to test set. Boston=fread(&quot;data/Boston.csv&quot;) # load the data into R Boston[,location:=factor(location)] Boston[,chas:=factor(chas)] num_row=nrow(Boston) # check the number of rows # Random select the training set set.seed(1) # set the rand seed to ensure replicability train_size=round(0.8*num_row,0) # set the size of training set # randomly select train_size=300 numbers from the sequence of 1 to nrow(Boston)=506 train=sample(1:num_row, train_size, replace=FALSE) # the index of rows selected as training set head(train) ## [1] 505 324 167 129 418 471 # the training set: Boston[train] head(Boston[train]) ## crim zn indus chas nox rm age dis rad tax ptratio black lstat ## 1: 0.10959 0 11.93 0 0.573 6.794 89.3 2.3889 1 273 21.0 393.45 6.48 ## 2: 0.28392 0 7.38 0 0.493 5.708 74.3 4.7211 5 287 19.6 391.13 11.74 ## 3: 2.01019 0 19.58 0 0.605 7.929 96.2 2.0459 5 403 14.7 369.30 3.70 ## 4: 0.32543 0 21.89 0 0.624 6.431 98.8 1.8125 4 437 21.2 396.90 15.39 ## 5: 25.94060 0 18.10 0 0.679 5.304 89.1 1.6475 5 666 20.2 127.36 26.64 ## 6: 4.34879 0 18.10 0 0.580 6.167 84.0 3.0334 5 666 20.2 396.90 16.29 ## location medv ## 1: south 22.0 ## 2: west 18.5 ## 3: north 50.0 ## 4: east 18.0 ## 5: west 10.4 ## 6: south 19.9 # the test set: Boston[-train] head(Boston[-train]) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1: 0.02985 0.0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## 2: 0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 395.60 ## 3: 0.21124 12.5 7.87 0 0.524 5.631 100.0 6.0821 5 311 15.2 386.63 ## 4: 0.17004 12.5 7.87 0 0.524 6.004 85.9 6.5921 5 311 15.2 386.71 ## 5: 0.22489 12.5 7.87 0 0.524 6.377 94.3 6.3467 5 311 15.2 392.52 ## 6: 0.11747 12.5 7.87 0 0.524 6.009 82.9 6.2267 5 311 15.2 396.90 ## lstat location medv ## 1: 5.21 south 28.7 ## 2: 12.43 west 22.9 ## 3: 29.93 north 16.5 ## 4: 17.10 north 18.9 ## 5: 20.45 north 15.0 ## 6: 13.27 north 18.9 22.2 Model selection Lets revisit the simple linear model in the first chapter. We have talked about four different models: Model 1: Simple linear model Model 2: log-transformation Model 3: quadratic terms Model 4: \\(6^{th}\\) order polynomial terms Some of these are not the typical linear regression model, so we cannot rely on Adj-R2, CP, AIC, and BIC to select the best model. We will show how to use the training/test set for model selection. 22.2.1 estimate the model using training set fit1=lm(formula=medv~lstat, data=Boston[train]) fit2=lm(formula=medv~log(lstat), data=Boston[train]) fit3=lm(formula=medv~lstat+I(lstat^2), data=Boston[train]) fit4=lm(formula=medv~poly(lstat,6), data=Boston[train]) Note that, instead of using the whole Boston dataset, we use the training set (i.e., Boston[train]) to estimate each model. 22.2.2 evaluate the models out-of-sample prediction using test set The next step is to evaluate the out-of-sample prediction performance of each model over the test set. Lets first evaluate the prediction performance for model 1: the simple linear model. # take a quick look of the coefficents of estimated model 1: coef(fit1) ## (Intercept) lstat ## 35.1909859 -0.9839224 Model 1 use lstat to predict medv according to the linear equation: \\[medv=35.19-0.984*lstat\\] To use the estimated model for prediction over test set, we need to make the test set in the appropriate format so that we can apply the above model on test set: # use the same formula as model 1 to create test set in the appropriate format test_set1=model.matrix(medv ~ lstat,data=Boston[-train,]) head(test_set1) ## (Intercept) lstat ## 1 1 5.21 ## 2 1 12.43 ## 3 1 29.93 ## 4 1 17.10 ## 5 1 20.45 ## 6 1 13.27 Note that model.matrix() function constructs the matrix of test set according to the formula. Accordingly, the test set will have the same structure of the training set. Given the value of lstat in test_set1, we can predict the corresponding medv. For example, on the first row in test_set1, lstat=4.03; based on the estimated model 1, the prediction is \\[medv=35.19*1-0.984*5.21=30.063\\] Similarly, we can generate prediction of medv for each row in the test set. We can use do this using matrix multiplication operator %*%: pred1=test_set1%*%coef(fit1) You have now generated the prediction over the test set based on the model 1. Now we need to evaluate how close our prediction is to the actual medv. The most common measure of prediction accurary is the Mean Square Error (MSE) and rMSE (root of MSE). MSE is defined as \\[MSE=\\sum_{i=1}^{N} (\\hat{y}_i-y_i)^2/N\\] where N is the number of observations in the test set, \\(\\hat{y}_i\\) denotes the prediction of \\(y_i\\) Once we compute MSE, we can easily derive rMSE as follow: \\(rMSE=\\sqrt{MSE}\\), which has the same unit as the dependent variable y. Thus, the out-of-sample MSE and rMSE for model 1 is thus: mse1=mean((Boston$medv[-train]-pred1)^2) rmse1=sqrt(mse1) Now, we can evaluate the out-of-sample prediction performance for model 2, 3, and 4. # model 2 test_set2=model.matrix(medv ~ log(lstat),data=Boston[-train,]) head(test_set2) ## (Intercept) log(lstat) ## 1 1 1.650580 ## 2 1 2.520113 ## 3 1 3.398861 ## 4 1 2.839078 ## 5 1 3.017983 ## 6 1 2.585506 coef(fit2) ## (Intercept) log(lstat) ## 53.01142 -12.78838 pred2=test_set2%*%coef(fit2) mse2=mean((Boston$medv[-train]-pred2)^2) rmse2=sqrt(mse2) # model 3 test_set3=model.matrix(medv ~ lstat+I(lstat^2),data=Boston[-train,]) head(test_set3) ## (Intercept) lstat I(lstat^2) ## 1 1 5.21 27.1441 ## 2 1 12.43 154.5049 ## 3 1 29.93 895.8049 ## 4 1 17.10 292.4100 ## 5 1 20.45 418.2025 ## 6 1 13.27 176.0929 coef(fit3) ## (Intercept) lstat I(lstat^2) ## 43.91219487 -2.44764033 0.04632758 pred3=test_set3%*%coef(fit3) mse3=mean((Boston$medv[-train]-pred3)^2) rmse3=sqrt(mse3) # model 4 test_set4=model.matrix(medv ~poly(lstat,6),data=Boston[-train,]) head(test_set4) ## (Intercept) poly(lstat, 6)1 poly(lstat, 6)2 poly(lstat, 6)3 poly(lstat, 6)4 ## 1 1 -0.109382860 0.10342524 -0.06311194 -0.006450109 ## 2 1 -0.009075806 -0.08091071 0.07363182 0.041093316 ## 3 1 0.234050709 0.22029148 0.09853260 -0.036788445 ## 4 1 0.055804241 -0.10413246 -0.04675712 0.100022817 ## 5 1 0.102345602 -0.07433528 -0.13675073 -0.002239732 ## 6 1 0.002594267 -0.09065044 0.05790804 0.066203752 ## poly(lstat, 6)5 poly(lstat, 6)6 ## 1 0.07026449 -0.10551735 ## 2 -0.09630236 0.02686326 ## 3 -0.15483663 -0.19814064 ## 4 0.04191693 -0.09942813 ## 5 0.12910801 0.04331152 ## 6 -0.09009815 -0.01076696 coef(fit4) ## (Intercept) poly(lstat, 6)1 poly(lstat, 6)2 poly(lstat, 6)3 poly(lstat, 6)4 ## 22.846914 -141.041172 62.799446 -24.130962 20.693769 ## poly(lstat, 6)5 poly(lstat, 6)6 ## -17.518414 3.118383 pred4=test_set4%*%coef(fit4) mse4=mean((Boston$medv[-train]-pred4)^2) rmse4=sqrt(mse4) Now, we have compute the out-of-sample performance measured by rMSE for each model, we can pick the model with the minimum rMSE, which will be our best model: # print the rMSE for the four models c(rmse1,rmse2,rmse3,rmse4) ## [1] 5.471329 4.973909 5.305180 10.804961 barplot(c(rmse1,rmse2,rmse3,rmse4)) As seen, model 2 (the log-transformation) is the winner. We can plot the prediction over the test set to see the result visually: plot(Boston$lstat[-train], Boston$medv[-train]) points(Boston$lstat[-train], pred1,col=&quot;blue&quot;) points(Boston$lstat[-train], pred2, col=&quot;black&quot;) points(Boston$lstat[-train], pred3, col=&quot;red&quot;) points(Boston$lstat[-train], pred4, col=&quot;yellow&quot;) As seen, the log-transformation models seems to be the best model for prediction over test set. The quadratic model and polynomial clearly suffer from over-fitting  these cannot be generaliziable on the test set. 22.3 Model 4 has the problem of over-fitting plot(Boston$lstat[train], Boston$medv[train], cex=0.5) points(Boston$lstat[train], fitted(fit4), cex=0.5) points(Boston$lstat[-train], Boston$medv[-train], col=&quot;blue&quot;, cex=0.5) points(Boston$lstat[-train], pred4, col=&quot;blue&quot;,cex=0.3) 22.4 Other Commonly Used Metrics for Prediction performance In addition to MSE and rMSE, we usually report the following two metrics for the prediction performance: MAE (Mean Absolute Error): measures the average gap between our prediction and the actual y-variable. MAPE (mean absolute percent error): mean the average percent gap between our prediction and the actual y-variable. Lets compute and report the MAE and MAPE for our best model: # Mean Absolute Error mae=mean(abs(Boston$medv[-train]-pred2)) mae ## [1] 3.799601 # Mean Absolute Percent Error mape=mean( abs(Boston$medv[-train]-pred2)/ Boston$medv[-train] ) mape ## [1] 0.1931064 This suggests that our prediction is on average off by 3.8 or 19.% as compared to the actual medv value. Well, a positive way to look at this is that there is plenty room for improvment. Remember, we only include one variable yet. In the next chapter, we will use training and set to select the model from the best subset regression. 22.5 Summary The ultimate criteria for model selection is the out-of-sample prediction performance. We can divide the dataset into training set and test set. The training set is used to estimate the model; the test set is used to evaluate the out-of-sample prediction. The common measure of prediction performance for model selection is rMSE and MSE. We also report MAE and MAPE to get a sense of how off/good our prediction is. "],["best-subset-model-selection-with-trainingtest-set.html", "Chapter 23 Best Subset Model Selection with Training/Test Set 23.1 Create training/test set 23.2 Evaluate the Best subset regression through training/test set 23.3 Exercise", " Chapter 23 Best Subset Model Selection with Training/Test Set As mentioned, it is not always best to include every variable into the linear regression model. We need to select the best subset of variables to include. The best subset algorithm exhausts every possible combination to select the best model of each size. However, this algorithm cannot be scaled up, so we use forward selection, which will add variables sequentially. The forward model selection will generate \\(p\\) different models with distinct model size from \\(1, 2, ..., p\\), where \\(p\\) is the total number of independent variables. Thus, we need to choose the best model among these \\(p\\) models. Previously, we used adj-R2, Cp, AIC and BIC for model selection. In this chapter, we will use training and test set to select the best model. 23.1 Create training/test set There is no definitive rule on how much proportation of data goes to training set or test set. A typical rule of thumb is 80% of data goes to training set and 20% goes to test set. Boston=fread(&quot;data/Boston.csv&quot;) # load the data into R Boston[,location:=factor(location)] Boston[,chas:=factor(chas)] num_row=nrow(Boston) # check the number of rows # Random select the training set set.seed(1) # set the rand seed to ensure replicability train_size=round(0.8*num_row,0) # set the size of training set # randomly select train_size=300 numbers from the sequence of 1 to nrow(Boston)=506 train=sample(1:num_row, train_size, replace=FALSE) # the training set: Boston[train] head(Boston[train]) ## crim zn indus chas nox rm age dis rad tax ptratio black lstat ## 1: 0.10959 0 11.93 0 0.573 6.794 89.3 2.3889 1 273 21.0 393.45 6.48 ## 2: 0.28392 0 7.38 0 0.493 5.708 74.3 4.7211 5 287 19.6 391.13 11.74 ## 3: 2.01019 0 19.58 0 0.605 7.929 96.2 2.0459 5 403 14.7 369.30 3.70 ## 4: 0.32543 0 21.89 0 0.624 6.431 98.8 1.8125 4 437 21.2 396.90 15.39 ## 5: 25.94060 0 18.10 0 0.679 5.304 89.1 1.6475 5 666 20.2 127.36 26.64 ## 6: 4.34879 0 18.10 0 0.580 6.167 84.0 3.0334 5 666 20.2 396.90 16.29 ## location medv ## 1: south 22.0 ## 2: west 18.5 ## 3: north 50.0 ## 4: east 18.0 ## 5: west 10.4 ## 6: south 19.9 # the test set: Boston[-train] head(Boston[-train]) ## crim zn indus chas nox rm age dis rad tax ptratio black ## 1: 0.02985 0.0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 394.12 ## 2: 0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 395.60 ## 3: 0.21124 12.5 7.87 0 0.524 5.631 100.0 6.0821 5 311 15.2 386.63 ## 4: 0.17004 12.5 7.87 0 0.524 6.004 85.9 6.5921 5 311 15.2 386.71 ## 5: 0.22489 12.5 7.87 0 0.524 6.377 94.3 6.3467 5 311 15.2 392.52 ## 6: 0.11747 12.5 7.87 0 0.524 6.009 82.9 6.2267 5 311 15.2 396.90 ## lstat location medv ## 1: 5.21 south 28.7 ## 2: 12.43 west 22.9 ## 3: 29.93 north 16.5 ## 4: 17.10 north 18.9 ## 5: 20.45 north 15.0 ## 6: 13.27 north 18.9 23.2 Evaluate the Best subset regression through training/test set # find best subset based on training data fwd_fit=regsubsets(medv~., data=Boston[train,], nvmax=18, method=&quot;forward&quot;) fwd_fit_sum=summary(fwd_fit) fwd_fit_sum ## Subset selection object ## Call: regsubsets.formula(medv ~ ., data = Boston[train, ], nvmax = 18, ## method = &quot;forward&quot;) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## lstat FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: forward ## crim zn indus chas1 nox rm age dis rad tax ptratio black lstat ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## locationnorth locationsouth locationwest ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; Again, summary(fwd_fit) narrows down to 16 different models with distinct model size. Now lets examine which model results in the best out-of-sample predication over the test set. Construct test set in the same format of training set: test_set=model.matrix(medv ~ .,data=Boston[-train,]) head(test_set) ## (Intercept) crim zn indus chas1 nox rm age dis rad tax ptratio ## 1 1 0.02985 0.0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 ## 2 1 0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 ## 3 1 0.21124 12.5 7.87 0 0.524 5.631 100.0 6.0821 5 311 15.2 ## 4 1 0.17004 12.5 7.87 0 0.524 6.004 85.9 6.5921 5 311 15.2 ## 5 1 0.22489 12.5 7.87 0 0.524 6.377 94.3 6.3467 5 311 15.2 ## 6 1 0.11747 12.5 7.87 0 0.524 6.009 82.9 6.2267 5 311 15.2 ## black lstat locationnorth locationsouth locationwest ## 1 394.12 5.21 0 1 0 ## 2 395.60 12.43 0 0 1 ## 3 386.63 29.93 1 0 0 ## 4 386.71 17.10 1 0 0 ## 5 392.52 20.45 1 0 0 ## 6 396.90 13.27 1 0 0 Note: model.matrix() is a function to create a matrix which has the same columns as the regression dataset based on a formula. Compute the out-of-sample prediction error based on test set: #Initialize the vector for saving the mse (mean square error) err over the test set: test_set_mse=rep(NA,16) for(i in 1:16){ coefi=coef(fwd_fit,id=i) # compute the out-of-sample prediction for model i pred=test_set[,names(coefi)]%*%coefi # compute the MSE for model i test_set_mse[i]=mean((Boston$medv[-train]-pred)^2) } Calculate rMSE and choose the best model to minimize rMSE: test_set_rmse=sqrt(test_set_mse) plot(test_set_rmse,ylab=&quot;Root MSE&quot;, type=&quot;b&quot;) opt_id=which.min(test_set_rmse) points(opt_id,test_set_rmse[opt_id],pch=20,col=&quot;red&quot;) Observation: which model to choose?  the model with 11 variables, it minimizes the out-of-sample prediction. # obtain the model parameter for the best model coef(fwd_fit,id=opt_id) ## (Intercept) crim zn chas1 nox ## 22.322359524 -0.042315405 0.037742595 3.464838748 -14.551045654 ## rm dis rad ptratio black ## 4.472611255 -1.343541190 0.464600981 -0.748560817 0.009929678 ## lstat locationnorth ## -0.559735879 1.123935544 Plot the optimal out-of-sample prediction: coefi=coef(fwd_fit,id=opt_id) opt_pred=test_set[,names(coefi)]%*%coefi plot(Boston$lstat[-train], Boston$medv[-train]) points(Boston$lstat[-train], opt_pred,col=&quot;blue&quot;) # Mean Absolute Error mean(abs(Boston$medv[-train]-opt_pred)) ## [1] 3.237467 # Mean Absolute Percent Error mean(abs(Boston$medv[-train]-opt_pred)/Boston$medv[-train]) ## [1] 0.1799541 23.3 Exercise # pre-process the data to include log-transformation, polynoimal terms formula=as.formula(medv~.-lstat+log(lstat)) # find best subset based on training data fwd_fit=regsubsets(formula, data=Boston[train,], nvmax=18, method=&quot;forward&quot;) fwd_fit_sum=summary(fwd_fit) fwd_fit_sum ## Subset selection object ## Call: regsubsets.formula(formula, data = Boston[train, ], nvmax = 18, ## method = &quot;forward&quot;) ## 16 Variables (and intercept) ## Forced in Forced out ## crim FALSE FALSE ## zn FALSE FALSE ## indus FALSE FALSE ## chas1 FALSE FALSE ## nox FALSE FALSE ## rm FALSE FALSE ## age FALSE FALSE ## dis FALSE FALSE ## rad FALSE FALSE ## tax FALSE FALSE ## ptratio FALSE FALSE ## black FALSE FALSE ## locationnorth FALSE FALSE ## locationsouth FALSE FALSE ## locationwest FALSE FALSE ## log(lstat) FALSE FALSE ## 1 subsets of each size up to 16 ## Selection Algorithm: forward ## crim zn indus chas1 nox rm age dis rad tax ptratio black ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## locationnorth locationsouth locationwest log(lstat) ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; # Construct test set in the same format of training set: test_set=model.matrix(formula, data=Boston[-train,]) head(test_set) ## (Intercept) crim zn indus chas1 nox rm age dis rad tax ptratio ## 1 1 0.02985 0.0 2.18 0 0.458 6.430 58.7 6.0622 3 222 18.7 ## 2 1 0.08829 12.5 7.87 0 0.524 6.012 66.6 5.5605 5 311 15.2 ## 3 1 0.21124 12.5 7.87 0 0.524 5.631 100.0 6.0821 5 311 15.2 ## 4 1 0.17004 12.5 7.87 0 0.524 6.004 85.9 6.5921 5 311 15.2 ## 5 1 0.22489 12.5 7.87 0 0.524 6.377 94.3 6.3467 5 311 15.2 ## 6 1 0.11747 12.5 7.87 0 0.524 6.009 82.9 6.2267 5 311 15.2 ## black locationnorth locationsouth locationwest log(lstat) ## 1 394.12 0 1 0 1.650580 ## 2 395.60 0 0 1 2.520113 ## 3 386.63 1 0 0 3.398861 ## 4 386.71 1 0 0 2.839078 ## 5 392.52 1 0 0 3.017983 ## 6 396.90 1 0 0 2.585506 Note: model.matrix() is a function to create a matrix which has the same columns as the regression dataset based on a formula. Compute the out-of-sample prediction error based on test set: #Initialize the vector for saving the mse (mean square error) err over the test set: test_set_mse=rep(NA,16) for(i in 1:16){ coefi=coef(fwd_fit,id=i) pred=test_set[,names(coefi)]%*%coefi test_set_mse[i]=mean((Boston$medv[-train]-pred)^2) } Calculate rMSE and choose the best model to minimize rMSE: test_set_rmse=sqrt(test_set_mse) plot(test_set_rmse,ylab=&quot;Root MSE&quot;, type=&quot;b&quot;) opt_id=which.min(test_set_rmse) points(opt_id,test_set_rmse[opt_id],pch=20,col=&quot;red&quot;) Plot the optimal out-of-sample prediction: # obtain the model parameter for the best model coefi=coef(fwd_fit,id=opt_id) opt_pred=test_set[,names(coefi)]%*%coefi plot(Boston$lstat[-train], Boston$medv[-train]) points(Boston$lstat[-train], opt_pred,col=&quot;blue&quot;) # Mean Absolute Error mean(abs(Boston$medv[-train]-opt_pred)) ## [1] 3.075017 # Mean Absolute Percent Error mean(abs(Boston$medv[-train]-opt_pred)/Boston$medv[-train]) ## [1] 0.1634978 "],["decision-tree.html", "Chapter 24 Decision Tree", " Chapter 24 Decision Tree library(rpart) library(rattle) A decision tree is a flowchart-like structure in which each internal node represents a test on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules. Below it is an example of a decision tree for loan application. An applicant is evaluated at each node and branched into different branched based on the given rules. The end-node will then decide whether the application will be approved. For example, the first node (i.e., the first decision criteria) is whether the applicant is aged above or below 40. The applicant will be channeled to different branch based on this criteria. Figure 24.1: Decision tree for loan application In the prediction context, such decision tree can be used to predict, e.g., whether the applicant will default. The decision tree is very simple to understand and easy to interpret. The trees can be also visualized and provide insight on how the prediction model work. You may be wondering, how can we know what should be right decision rules, i.e., the structure of the decision tree? In fact, the optional decision tree is known to be computational impossible. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. When we are predicting a binary outcome, e.g., whether applicant will default, this is called classification problem. The corresponding tree is called decision tree. When we are predicting continous outcome, e.g., house price, that is called regression tree. In R, we can also construct decision tree using the rpart package. Lets first take a look at this famous problem of predicting who will survive from the Titanic. Refer to kaggle for more detailed information about this competition of predicting survival of Titanic passengers. # load the training and test dataset train &lt;- fread(&#39;data/titanic/train.csv&#39;) test &lt;- fread(&quot;data/titanic/test.csv&quot;) full=rbind(train, test, fill=TRUE) head(full) ## PassengerId Survived Pclass ## 1: 1 0 3 ## 2: 2 1 1 ## 3: 3 1 3 ## 4: 4 1 1 ## 5: 5 0 3 ## 6: 6 0 3 ## Name Sex Age SibSp Parch ## 1: Braund, Mr. Owen Harris male 22 1 0 ## 2: Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3: Heikkinen, Miss. Laina female 26 0 0 ## 4: Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 5: Allen, Mr. William Henry male 35 0 0 ## 6: Moran, Mr. James male NA 0 0 ## Ticket Fare Cabin Embarked ## 1: A/5 21171 7.2500 S ## 2: PC 17599 71.2833 C85 C ## 3: STON/O2. 3101282 7.9250 S ## 4: 113803 53.1000 C123 S ## 5: 373450 8.0500 S ## 6: 330877 8.4583 Q # clean data table(full$Embarked) ## ## C Q S ## 2 270 123 914 full[Embarked==&quot;&quot;, Embarked:=&quot;S&quot;] # replace missing Embarked with the most frequent value ## examine the cabin information table(full$Cabin) ## ## A10 A11 A14 A16 ## 1014 1 1 1 1 ## A18 A19 A20 A21 A23 ## 1 1 1 1 1 ## A24 A26 A29 A31 A32 ## 1 1 1 1 1 ## A34 A36 A5 A6 A7 ## 3 1 1 1 1 ## A9 B10 B101 B102 B11 ## 1 1 1 1 1 ## B18 B19 B20 B22 B24 ## 2 1 2 2 1 ## B26 B28 B3 B30 B35 ## 1 2 1 1 2 ## B36 B37 B38 B39 B4 ## 1 1 1 1 1 ## B41 B42 B45 B49 B5 ## 2 1 2 2 2 ## B50 B51 B53 B55 B52 B54 B56 B57 B59 B63 B66 B58 B60 ## 1 3 1 5 3 ## B61 B69 B71 B73 B77 ## 1 2 2 1 2 ## B78 B79 B80 B82 B84 B86 ## 2 1 1 1 1 ## B94 B96 B98 C101 C103 C104 ## 1 4 3 1 1 ## C105 C106 C110 C111 C116 ## 1 2 1 1 2 ## C118 C123 C124 C125 C126 ## 1 2 2 2 2 ## C128 C130 C132 C148 C2 ## 1 1 1 1 2 ## C22 C26 C23 C25 C27 C28 C30 C31 ## 4 6 1 1 2 ## C32 C39 C45 C46 C47 ## 2 1 1 2 1 ## C49 C50 C51 C52 C53 ## 1 1 1 2 1 ## C54 C55 C57 C6 C62 C64 C65 ## 2 2 2 2 2 ## C68 C7 C70 C78 C80 ## 2 2 1 4 2 ## C82 C83 C85 C86 C87 ## 1 2 2 2 1 ## C89 C90 C91 C92 C93 ## 2 1 1 2 2 ## C95 C97 C99 D D10 D12 ## 1 1 1 4 2 ## D11 D15 D17 D19 D20 ## 1 2 2 2 2 ## D21 D22 D26 D28 D30 ## 2 1 2 2 2 ## D33 D34 D35 D36 D37 ## 2 1 2 2 2 ## D38 D40 D43 D45 D46 ## 1 1 1 1 1 ## D47 D48 D49 D50 D56 ## 1 1 1 1 1 ## D6 D7 D9 E10 E101 ## 1 1 1 1 3 ## E12 E121 E17 E24 E25 ## 1 2 1 2 2 ## E31 E33 E34 E36 E38 ## 2 2 3 1 1 ## E39 E41 E40 E44 E45 E46 ## 1 1 2 1 2 ## E49 E50 E52 E58 E60 ## 1 2 1 1 1 ## E63 E67 E68 E77 E8 ## 1 2 1 1 2 ## F F E46 F E57 F E69 F G63 ## 1 1 1 1 2 ## F G73 F2 F33 F38 F4 ## 2 4 4 1 4 ## G6 T ## 5 1 full[Cabin==&quot;&quot;,Cabin:=&quot;N&quot;] # if Cabin is missing, replace with N full[,Cabin:=str_extract(Cabin,&quot;[:alpha:]&quot;)] # extract the first letter of Cabin number full[Cabin%in%c(&quot;G&quot;,&quot;T&quot;),Cabin:=&quot;N&quot;] # Cabin G and T do not have enough observation and reset to &quot;N&quot; table(full$Cabin) ## ## A B C D E F N ## 22 65 94 46 41 21 1020 # check the missing value pattern summary(full) ## PassengerId Survived Pclass Name ## Min. : 1 Min. :0.0000 Min. :1.000 Length:1309 ## 1st Qu.: 328 1st Qu.:0.0000 1st Qu.:2.000 Class :character ## Median : 655 Median :0.0000 Median :3.000 Mode :character ## Mean : 655 Mean :0.3838 Mean :2.295 ## 3rd Qu.: 982 3rd Qu.:1.0000 3rd Qu.:3.000 ## Max. :1309 Max. :1.0000 Max. :3.000 ## NA&#39;s :418 ## Sex Age SibSp Parch ## Length:1309 Min. : 0.17 Min. :0.0000 Min. :0.000 ## Class :character 1st Qu.:21.00 1st Qu.:0.0000 1st Qu.:0.000 ## Mode :character Median :28.00 Median :0.0000 Median :0.000 ## Mean :29.88 Mean :0.4989 Mean :0.385 ## 3rd Qu.:39.00 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :80.00 Max. :8.0000 Max. :9.000 ## NA&#39;s :263 ## Ticket Fare Cabin Embarked ## Length:1309 Min. : 0.000 Length:1309 Length:1309 ## Class :character 1st Qu.: 7.896 Class :character Class :character ## Mode :character Median : 14.454 Mode :character Mode :character ## Mean : 33.295 ## 3rd Qu.: 31.275 ## Max. :512.329 ## NA&#39;s :1 # We have a lot of missing data in the Age feature (263/1309) # examine rows with missing age full[is.na(Age)] %&gt;% head() ## PassengerId Survived Pclass Name Sex Age SibSp ## 1: 6 0 3 Moran, Mr. James male NA 0 ## 2: 18 1 2 Williams, Mr. Charles Eugene male NA 0 ## 3: 20 1 3 Masselmani, Mrs. Fatima female NA 0 ## 4: 27 0 3 Emir, Mr. Farred Chehab male NA 0 ## 5: 29 1 3 O&#39;Dwyer, Miss. Ellen &quot;&quot;Nellie&quot;&quot; female NA 0 ## 6: 30 0 3 Todoroff, Mr. Lalio male NA 0 ## Parch Ticket Fare Cabin Embarked ## 1: 0 330877 8.4583 N Q ## 2: 0 244373 13.0000 N S ## 3: 0 2649 7.2250 N C ## 4: 0 2631 7.2250 N C ## 5: 0 330959 7.8792 N Q ## 6: 0 349216 7.8958 N S full=full[, Age2:=mean(Age,na.rm=TRUE)][is.na(Age), Age:=Age2][,-c(&quot;Age2&quot;)] # replace missing age with average # replace missing fare with average fare full[is.na(Fare)] ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket ## 1: 1044 NA 3 Storey, Mr. Thomas male 60.5 0 0 3701 ## Fare Cabin Embarked ## 1: NA N S full=full[, Fare2:=mean(Fare,na.rm=TRUE)][is.na(Fare), Fare:=Fare2][,-c(&quot;Fare2&quot;)] # The title of the passenger can affect his survive: # extract title full[,Title := gsub(&#39;(.*, )|(\\\\..*)&#39;, &#39;&#39;, Name)] full[Title %in% c(&#39;Mlle&#39;,&#39;Ms&#39;,&#39;Mme&#39;,&#39;Lady&#39;,&#39;Dona&#39;), Title:= &#39;Miss&#39;] full[Title %in% c(&#39;Capt&#39;,&#39;Col&#39;,&#39;Don&#39;,&#39;Dr&#39;,&#39;Jonkheer&#39;,&#39;Major&#39;,&#39;Rev&#39;,&#39;Sir&#39;,&#39;the Countess&#39;), Title:= &#39;Officer&#39;] # Let&#39;s see how many unique levels for each variables apply(full,2, function(x) length(unique(x))) ## PassengerId Survived Pclass Name Sex Age ## 1309 3 3 1307 2 99 ## SibSp Parch Ticket Fare Cabin Embarked ## 7 8 929 282 7 3 ## Title ## 5 # Group Parch and SibSp into categorical variables table(full$Parch) ## ## 0 1 2 3 4 5 6 9 ## 1002 170 113 8 6 6 2 2 table(full$SibSp) ## ## 0 1 2 3 4 5 8 ## 891 319 42 20 22 6 9 full[,Parch2:=ifelse(Parch&gt;2,3,Parch), by=.(PassengerId)] full[,SibSp2:=ifelse(SibSp&gt;4,4,SibSp), by=.(PassengerId)] # Let&#39;s move the features Survived, Pclass, Sex, Embarked to be factors cols&lt;-c(&quot;Pclass&quot;,&quot;Sex&quot;,&quot;Embarked&quot;,&quot;Title&quot;,&quot;Cabin&quot;,&quot;SibSp2&quot;,&quot;Parch2&quot;) full[,(cols):=lapply(.SD, as.factor),.SDcols=cols] full[,Survived:=factor(Survived,levels=c(0,1), labels = c(&quot;D&quot;,&quot;S&quot;))] summary(full) ## PassengerId Survived Pclass Name Sex ## Min. : 1 D :549 1:323 Length:1309 female:466 ## 1st Qu.: 328 S :342 2:277 Class :character male :843 ## Median : 655 NA&#39;s:418 3:709 Mode :character ## Mean : 655 ## 3rd Qu.: 982 ## Max. :1309 ## ## Age SibSp Parch Ticket ## Min. : 0.17 Min. :0.0000 Min. :0.000 Length:1309 ## 1st Qu.:22.00 1st Qu.:0.0000 1st Qu.:0.000 Class :character ## Median :29.88 Median :0.0000 Median :0.000 Mode :character ## Mean :29.88 Mean :0.4989 Mean :0.385 ## 3rd Qu.:35.00 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :80.00 Max. :8.0000 Max. :9.000 ## ## Fare Cabin Embarked Title Parch2 SibSp2 ## Min. : 0.000 A: 22 C:270 Master : 61 0:1002 0:891 ## 1st Qu.: 7.896 B: 65 Q:123 Miss :267 1: 170 1:319 ## Median : 14.454 C: 94 S:916 Mr :757 2: 113 2: 42 ## Mean : 33.295 D: 46 Mrs :197 3: 24 3: 20 ## 3rd Qu.: 31.275 E: 41 Officer: 27 4: 37 ## Max. :512.329 F: 21 ## N:1020 Once we have cleaned our data, we can start to grow decision tree. # grow tree fit &lt;- rpart(Survived ~ Pclass+Sex+Age+Fare+Cabin+Title, method=&quot;class&quot;, data=full) printcp(fit) # display the results ## ## Classification tree: ## rpart(formula = Survived ~ Pclass + Sex + Age + Fare + Cabin + ## Title, data = full, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] Age Fare Pclass Title ## ## Root node error: 342/891 = 0.38384 ## ## n=891 (418 observations deleted due to missingness) ## ## CP nsplit rel error xerror xstd ## 1 0.456140 0 1.00000 1.00000 0.042446 ## 2 0.052632 1 0.54386 0.54386 0.035472 ## 3 0.014620 3 0.43860 0.43860 0.032658 ## 4 0.010000 4 0.42398 0.45906 0.033253 plotcp(fit) # visualize cross-validation results summary(fit) # detailed summary of splits ## Call: ## rpart(formula = Survived ~ Pclass + Sex + Age + Fare + Cabin + ## Title, data = full, method = &quot;class&quot;) ## n=891 (418 observations deleted due to missingness) ## ## CP nsplit rel error xerror xstd ## 1 0.45614035 0 1.0000000 1.0000000 0.04244576 ## 2 0.05263158 1 0.5438596 0.5438596 0.03547203 ## 3 0.01461988 3 0.4385965 0.4385965 0.03265801 ## 4 0.01000000 4 0.4239766 0.4590643 0.03325316 ## ## Variable importance ## Title Sex Fare Age Pclass Cabin ## 34 29 12 10 9 5 ## ## Node number 1: 891 observations, complexity param=0.4561404 ## predicted class=D expected loss=0.3838384 P(node) =1 ## class counts: 549 342 ## probabilities: 0.616 0.384 ## left son=2 (539 obs) right son=3 (352 obs) ## Primary splits: ## Title splits as RRLRL, improve=132.75740, (0 missing) ## Sex splits as RL, improve=124.42630, (0 missing) ## Pclass splits as RRL, improve= 43.78183, (0 missing) ## Cabin splits as LRRRRRL, improve= 43.53355, (0 missing) ## Fare &lt; 10.48125 to the left, improve= 37.94194, (0 missing) ## Surrogate splits: ## Sex splits as RL, agree=0.953, adj=0.881, (0 split) ## Age &lt; 15.5 to the right, agree=0.689, adj=0.213, (0 split) ## Fare &lt; 15.1729 to the left, agree=0.645, adj=0.102, (0 split) ## Cabin splits as LRLRLRL, agree=0.617, adj=0.031, (0 split) ## ## Node number 2: 539 observations ## predicted class=D expected loss=0.1632653 P(node) =0.6049383 ## class counts: 451 88 ## probabilities: 0.837 0.163 ## ## Node number 3: 352 observations, complexity param=0.05263158 ## predicted class=S expected loss=0.2784091 P(node) =0.3950617 ## class counts: 98 254 ## probabilities: 0.278 0.722 ## left son=6 (172 obs) right son=7 (180 obs) ## Primary splits: ## Pclass splits as RRL, improve=38.436470, (0 missing) ## Cabin splits as RRRRRRL, improve=15.333150, (0 missing) ## Fare &lt; 48.2 to the left, improve=11.322120, (0 missing) ## Age &lt; 11.5 to the left, improve= 3.220768, (0 missing) ## Sex splits as RL, improve= 1.939510, (0 missing) ## Surrogate splits: ## Fare &lt; 25.69795 to the left, agree=0.767, adj=0.523, (0 split) ## Cabin splits as RRRRRRL, agree=0.750, adj=0.488, (0 split) ## Age &lt; 29.94057 to the left, agree=0.659, adj=0.302, (0 split) ## Title splits as LL-R-, agree=0.605, adj=0.192, (0 split) ## Sex splits as RL, agree=0.557, adj=0.093, (0 split) ## ## Node number 6: 172 observations, complexity param=0.05263158 ## predicted class=D expected loss=0.4825581 P(node) =0.1930415 ## class counts: 89 83 ## probabilities: 0.517 0.483 ## left son=12 (44 obs) right son=13 (128 obs) ## Primary splits: ## Fare &lt; 23.35 to the right, improve=18.138250, (0 missing) ## Age &lt; 38.5 to the right, improve= 4.112016, (0 missing) ## Sex splits as RL, improve= 0.538206, (0 missing) ## Title splits as LR-R-, improve= 0.538206, (0 missing) ## Surrogate splits: ## Sex splits as RL, agree=0.779, adj=0.136, (0 split) ## Title splits as LR-R-, agree=0.779, adj=0.136, (0 split) ## Age &lt; 11.5 to the left, agree=0.756, adj=0.045, (0 split) ## ## Node number 7: 180 observations ## predicted class=S expected loss=0.05 P(node) =0.2020202 ## class counts: 9 171 ## probabilities: 0.050 0.950 ## ## Node number 12: 44 observations ## predicted class=D expected loss=0.09090909 P(node) =0.04938272 ## class counts: 40 4 ## probabilities: 0.909 0.091 ## ## Node number 13: 128 observations, complexity param=0.01461988 ## predicted class=S expected loss=0.3828125 P(node) =0.1436588 ## class counts: 49 79 ## probabilities: 0.383 0.617 ## left son=26 (7 obs) right son=27 (121 obs) ## Primary splits: ## Age &lt; 36.5 to the right, improve=3.332073, (0 missing) ## Sex splits as LR, improve=2.050809, (0 missing) ## Title splits as RL-L-, improve=2.050809, (0 missing) ## Fare &lt; 7.8875 to the right, improve=1.178727, (0 missing) ## ## Node number 26: 7 observations ## predicted class=D expected loss=0.1428571 P(node) =0.007856341 ## class counts: 6 1 ## probabilities: 0.857 0.143 ## ## Node number 27: 121 observations ## predicted class=S expected loss=0.3553719 P(node) =0.1358025 ## class counts: 43 78 ## probabilities: 0.355 0.645 # plot tree fancyRpartPlot(fit) Each node displays the proportion of death and survival, as well as the precent of the total samples. For example, in the root node (the node on the top), about 38% of passengers will survive and 62% of passengers will die. The decision split the root node based on the title of passenger. If a passenger has title start with Mr or Officer, then they will be split to node 2. At node 2, the survival rate is only 16%, while at node 3, the survival rate is 72%. This makes sense because in the movie we see most crew sacrifice their lives to save others. At node 3, we really start to see how social status affect passengers survival probability. Being a upper class significantly improve your survival probability. The survival probability for passenger of class 1 or 2 has survival probability of 95%, while for average passenger the survival probability is only 48%. This is the power of data speaking! # confirm the survival rate full[Title%in%c(&quot;Mr&quot;,&quot;Officer&quot;)]$Survived%&gt;%table() ## . ## D S ## 451 88 full[!Title%in%c(&quot;Mr&quot;,&quot;Officer&quot;)]$Survived%&gt;%table() ## . ## D S ## 98 254 # confirm the survival rate for social class full[!Title%in%c(&quot;Mr&quot;,&quot;Officer&quot;) &amp; Pclass %in% c(1,2) ]$Survived%&gt;%table() ## . ## D S ## 9 171 Lets take Boston house as an example. Boston=fread(&quot;data/Boston.csv&quot;) Boston[,location:=factor(location)] Boston[,chas:=factor(chas)] # grow tree fit &lt;- rpart(medv ~ lstat+chas+rm, method=&quot;anova&quot;, data=Boston) printcp(fit) # display the results ## ## Regression tree: ## rpart(formula = medv ~ lstat + chas + rm, data = Boston, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] lstat rm ## ## Root node error: 42716/506 = 84.42 ## ## n= 506 ## ## CP nsplit rel error xerror xstd ## 1 0.452744 0 1.00000 1.00421 0.083135 ## 2 0.171172 1 0.54726 0.64623 0.060973 ## 3 0.071658 2 0.37608 0.42455 0.047450 ## 4 0.034288 3 0.30443 0.35162 0.045405 ## 5 0.021294 4 0.27014 0.31865 0.043072 ## 6 0.018024 5 0.24884 0.31028 0.043186 ## 7 0.015851 6 0.23082 0.29637 0.041413 ## 8 0.010000 7 0.21497 0.29096 0.040537 plotcp(fit) # visualize cross-validation results summary(fit) # detailed summary of splits ## Call: ## rpart(formula = medv ~ lstat + chas + rm, data = Boston, method = &quot;anova&quot;) ## n= 506 ## ## CP nsplit rel error xerror xstd ## 1 0.45274420 0 1.0000000 1.0042141 0.08313508 ## 2 0.17117244 1 0.5472558 0.6462307 0.06097273 ## 3 0.07165784 2 0.3760834 0.4245511 0.04745025 ## 4 0.03428819 3 0.3044255 0.3516220 0.04540510 ## 5 0.02129389 4 0.2701373 0.3186509 0.04307227 ## 6 0.01802372 5 0.2488434 0.3102767 0.04318598 ## 7 0.01585116 6 0.2308197 0.2963670 0.04141270 ## 8 0.01000000 7 0.2149686 0.2909642 0.04053677 ## ## Variable importance ## rm lstat chas ## 58 42 1 ## ## Node number 1: 506 observations, complexity param=0.4527442 ## mean=22.53281, MSE=84.41956 ## left son=2 (430 obs) right son=3 (76 obs) ## Primary splits: ## rm &lt; 6.941 to the left, improve=0.45274420, (0 missing) ## lstat &lt; 9.725 to the right, improve=0.44236500, (0 missing) ## chas splits as LR, improve=0.03071613, (0 missing) ## Surrogate splits: ## lstat &lt; 4.83 to the right, agree=0.891, adj=0.276, (0 split) ## ## Node number 2: 430 observations, complexity param=0.1711724 ## mean=19.93372, MSE=40.27284 ## left son=4 (175 obs) right son=5 (255 obs) ## Primary splits: ## lstat &lt; 14.4 to the right, improve=0.42222770, (0 missing) ## rm &lt; 6.5455 to the left, improve=0.14428770, (0 missing) ## chas splits as LR, improve=0.03078799, (0 missing) ## Surrogate splits: ## rm &lt; 5.858 to the left, agree=0.688, adj=0.234, (0 split) ## ## Node number 3: 76 observations, complexity param=0.07165784 ## mean=37.23816, MSE=79.7292 ## left son=6 (46 obs) right son=7 (30 obs) ## Primary splits: ## rm &lt; 7.437 to the left, improve=0.50515690, (0 missing) ## lstat &lt; 4.68 to the right, improve=0.33189140, (0 missing) ## chas splits as LR, improve=0.01614064, (0 missing) ## Surrogate splits: ## lstat &lt; 3.99 to the right, agree=0.776, adj=0.433, (0 split) ## chas splits as LR, agree=0.645, adj=0.100, (0 split) ## ## Node number 4: 175 observations, complexity param=0.02129389 ## mean=14.956, MSE=19.27572 ## left son=8 (76 obs) right son=9 (99 obs) ## Primary splits: ## lstat &lt; 19.83 to the right, improve=0.26964970, (0 missing) ## rm &lt; 5.567 to the left, improve=0.07509700, (0 missing) ## chas splits as LR, improve=0.07207135, (0 missing) ## Surrogate splits: ## rm &lt; 5.5505 to the left, agree=0.726, adj=0.368, (0 split) ## ## Node number 5: 255 observations, complexity param=0.03428819 ## mean=23.3498, MSE=26.0087 ## left son=10 (235 obs) right son=11 (20 obs) ## Primary splits: ## lstat &lt; 4.91 to the right, improve=0.22084090, (0 missing) ## rm &lt; 6.543 to the left, improve=0.21720990, (0 missing) ## chas splits as LR, improve=0.06223827, (0 missing) ## ## Node number 6: 46 observations, complexity param=0.01585116 ## mean=32.11304, MSE=41.29592 ## left son=12 (7 obs) right son=13 (39 obs) ## Primary splits: ## lstat &lt; 9.65 to the right, improve=0.35644260, (0 missing) ## rm &lt; 7.3 to the right, improve=0.05938584, (0 missing) ## ## Node number 7: 30 observations ## mean=45.09667, MSE=36.62832 ## ## Node number 8: 76 observations ## mean=12.35395, MSE=16.03617 ## ## Node number 9: 99 observations ## mean=16.95354, MSE=12.57481 ## ## Node number 10: 235 observations, complexity param=0.01802372 ## mean=22.65064, MSE=17.97365 ## left son=20 (113 obs) right son=21 (122 obs) ## Primary splits: ## lstat &lt; 9.715 to the right, improve=0.18227780, (0 missing) ## rm &lt; 6.543 to the left, improve=0.14702280, (0 missing) ## chas splits as LR, improve=0.02949653, (0 missing) ## Surrogate splits: ## rm &lt; 6.0285 to the left, agree=0.706, adj=0.389, (0 split) ## chas splits as RL, agree=0.532, adj=0.027, (0 split) ## ## Node number 11: 20 observations ## mean=31.565, MSE=47.18727 ## ## Node number 12: 7 observations ## mean=23.05714, MSE=61.85673 ## ## Node number 13: 39 observations ## mean=33.73846, MSE=20.24391 ## ## Node number 20: 113 observations ## mean=20.76991, MSE=6.503342 ## ## Node number 21: 122 observations ## mean=24.39262, MSE=22.28708 # plot tree fancyRpartPlot(fit) Similarly, lets examine the decision tree for Boston house price. At the top note, we have 506 houses and the average price is 23. Thus without additional information, our prediction of hourse price will be 23. Next, we split the house based on rm&lt;6.9. For house with rm&lt;6.9 (average number of rooms per dwelling&lt;6.9), their average price will be 20, and for house with rm&gt;=6.9, their average house price is 37. This makes a lot of sense since the size of a house is the most important factor in determining its value. We can verify the value of these house as follow: Boston$medv%&gt;%mean() ## [1] 22.53281 Boston[rm&gt;=6.9]$medv%&gt;%mean() ## [1] 37.1 "],["maching-learning-with-caret.html", "Chapter 25 Maching Learning with Caret 25.1 Random Forest 25.2 Random Forest", " Chapter 25 Maching Learning with Caret The caret package (short for Classification And REgression Training) contains functions to streamline the model training process for complex regression and classification problems. It integrates all activities related to model development in a streamlined workflow for nearly every major ML algorithm available in R. In particular, we will use random forest algorithm through caret for prediction. install.packages(c(&quot;caret&quot;,&quot;ranger&quot;,&quot;rpart&quot;,&quot;rattle&quot;)) library(data.table) library(leaps) library(rpart) # build decision tree library(rattle) # plot decision tree library(ranger) # random forest library(caret) library(stringr) A random forest is a machine learning technique thats used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems. In a simple non-technique terms, the random forest algorithm builds multiple decision trees with different structures. Each tree has its own prediction. The final prediction is to aggregate the decision from all trees. For binary outcome, the aggregation is done through majority vote; For continuous outcome, the aggregation is through averaging. 25.1 Random Forest Use random forest algorithm to predict house price Boston=fread(&quot;data/Boston.csv&quot;) Boston[,location:=factor(location)] Boston[,chas:=factor(chas)] nrow(Boston) # check the number of rows ## [1] 506 # set the rand seed to ensure replicability set.seed(1) # set the size of training set train_size=round(0.8*nrow(Boston),0) # randomly select 405 numbers from sequence of 1 to 506 train=sample(1:nrow(Boston), train_size, replace=FALSE) model1 &lt;- train( log(medv)~., Boston[train], method = &quot;ranger&quot; ) plot(model1) pred = exp(predict(model1, Boston[-train])) error = pred - Boston[-train]$medv # Calculate RMSE rmse= sqrt(mean(error ^ 2)) # Mean Absolute Error mae=mean(abs(Boston[-train]$medv-pred)) # Mean Absolute Percent Error mape=mean(abs(Boston[-train]$medv-pred)/Boston[-train]$medv) c(rmse,mae, mape) ## [1] 3.5733190 2.2802331 0.1166085 # plot the actual and prediction plot(Boston[-train]$medv,pred) Use random forest algorithm to predict survival of titanic passengers train &lt;- fread(&#39;data/titanic/train.csv&#39;) test &lt;- fread(&quot;data/titanic/test.csv&quot;) full=rbind(train, test, fill=TRUE) head(full) ## PassengerId Survived Pclass ## 1: 1 0 3 ## 2: 2 1 1 ## 3: 3 1 3 ## 4: 4 1 1 ## 5: 5 0 3 ## 6: 6 0 3 ## Name Sex Age SibSp Parch ## 1: Braund, Mr. Owen Harris male 22 1 0 ## 2: Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38 1 0 ## 3: Heikkinen, Miss. Laina female 26 0 0 ## 4: Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35 1 0 ## 5: Allen, Mr. William Henry male 35 0 0 ## 6: Moran, Mr. James male NA 0 0 ## Ticket Fare Cabin Embarked ## 1: A/5 21171 7.2500 S ## 2: PC 17599 71.2833 C85 C ## 3: STON/O2. 3101282 7.9250 S ## 4: 113803 53.1000 C123 S ## 5: 373450 8.0500 S ## 6: 330877 8.4583 Q # clean data table(full$Embarked) ## ## C Q S ## 2 270 123 914 full[Embarked==&quot;&quot;, Embarked:=&quot;S&quot;] # replace missing Embarked with the most frequent value ## examine the cabin information table(full$Cabin) ## ## A10 A11 A14 A16 ## 1014 1 1 1 1 ## A18 A19 A20 A21 A23 ## 1 1 1 1 1 ## A24 A26 A29 A31 A32 ## 1 1 1 1 1 ## A34 A36 A5 A6 A7 ## 3 1 1 1 1 ## A9 B10 B101 B102 B11 ## 1 1 1 1 1 ## B18 B19 B20 B22 B24 ## 2 1 2 2 1 ## B26 B28 B3 B30 B35 ## 1 2 1 1 2 ## B36 B37 B38 B39 B4 ## 1 1 1 1 1 ## B41 B42 B45 B49 B5 ## 2 1 2 2 2 ## B50 B51 B53 B55 B52 B54 B56 B57 B59 B63 B66 B58 B60 ## 1 3 1 5 3 ## B61 B69 B71 B73 B77 ## 1 2 2 1 2 ## B78 B79 B80 B82 B84 B86 ## 2 1 1 1 1 ## B94 B96 B98 C101 C103 C104 ## 1 4 3 1 1 ## C105 C106 C110 C111 C116 ## 1 2 1 1 2 ## C118 C123 C124 C125 C126 ## 1 2 2 2 2 ## C128 C130 C132 C148 C2 ## 1 1 1 1 2 ## C22 C26 C23 C25 C27 C28 C30 C31 ## 4 6 1 1 2 ## C32 C39 C45 C46 C47 ## 2 1 1 2 1 ## C49 C50 C51 C52 C53 ## 1 1 1 2 1 ## C54 C55 C57 C6 C62 C64 C65 ## 2 2 2 2 2 ## C68 C7 C70 C78 C80 ## 2 2 1 4 2 ## C82 C83 C85 C86 C87 ## 1 2 2 2 1 ## C89 C90 C91 C92 C93 ## 2 1 1 2 2 ## C95 C97 C99 D D10 D12 ## 1 1 1 4 2 ## D11 D15 D17 D19 D20 ## 1 2 2 2 2 ## D21 D22 D26 D28 D30 ## 2 1 2 2 2 ## D33 D34 D35 D36 D37 ## 2 1 2 2 2 ## D38 D40 D43 D45 D46 ## 1 1 1 1 1 ## D47 D48 D49 D50 D56 ## 1 1 1 1 1 ## D6 D7 D9 E10 E101 ## 1 1 1 1 3 ## E12 E121 E17 E24 E25 ## 1 2 1 2 2 ## E31 E33 E34 E36 E38 ## 2 2 3 1 1 ## E39 E41 E40 E44 E45 E46 ## 1 1 2 1 2 ## E49 E50 E52 E58 E60 ## 1 2 1 1 1 ## E63 E67 E68 E77 E8 ## 1 2 1 1 2 ## F F E46 F E57 F E69 F G63 ## 1 1 1 1 2 ## F G73 F2 F33 F38 F4 ## 2 4 4 1 4 ## G6 T ## 5 1 full[Cabin==&quot;&quot;,Cabin:=&quot;N&quot;] # if Cabin is missing, replace with N full[,Cabin:=str_extract(Cabin,&quot;[:alpha:]&quot;)] # extract the first letter of Cabin number full[Cabin%in%c(&quot;G&quot;,&quot;T&quot;),Cabin:=&quot;N&quot;] # Cabin G and T do not have enough observation and reset to &quot;N&quot; table(full$Cabin) ## ## A B C D E F N ## 22 65 94 46 41 21 1020 # check the missing value pattern summary(full) ## PassengerId Survived Pclass Name ## Min. : 1 Min. :0.0000 Min. :1.000 Length:1309 ## 1st Qu.: 328 1st Qu.:0.0000 1st Qu.:2.000 Class :character ## Median : 655 Median :0.0000 Median :3.000 Mode :character ## Mean : 655 Mean :0.3838 Mean :2.295 ## 3rd Qu.: 982 3rd Qu.:1.0000 3rd Qu.:3.000 ## Max. :1309 Max. :1.0000 Max. :3.000 ## NA&#39;s :418 ## Sex Age SibSp Parch ## Length:1309 Min. : 0.17 Min. :0.0000 Min. :0.000 ## Class :character 1st Qu.:21.00 1st Qu.:0.0000 1st Qu.:0.000 ## Mode :character Median :28.00 Median :0.0000 Median :0.000 ## Mean :29.88 Mean :0.4989 Mean :0.385 ## 3rd Qu.:39.00 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :80.00 Max. :8.0000 Max. :9.000 ## NA&#39;s :263 ## Ticket Fare Cabin Embarked ## Length:1309 Min. : 0.000 Length:1309 Length:1309 ## Class :character 1st Qu.: 7.896 Class :character Class :character ## Mode :character Median : 14.454 Mode :character Mode :character ## Mean : 33.295 ## 3rd Qu.: 31.275 ## Max. :512.329 ## NA&#39;s :1 # We have a lot of missing data in the Age feature (263/1309) # examine rows with missing age full[is.na(Age)] %&gt;% head() ## PassengerId Survived Pclass Name Sex Age SibSp ## 1: 6 0 3 Moran, Mr. James male NA 0 ## 2: 18 1 2 Williams, Mr. Charles Eugene male NA 0 ## 3: 20 1 3 Masselmani, Mrs. Fatima female NA 0 ## 4: 27 0 3 Emir, Mr. Farred Chehab male NA 0 ## 5: 29 1 3 O&#39;Dwyer, Miss. Ellen &quot;&quot;Nellie&quot;&quot; female NA 0 ## 6: 30 0 3 Todoroff, Mr. Lalio male NA 0 ## Parch Ticket Fare Cabin Embarked ## 1: 0 330877 8.4583 N Q ## 2: 0 244373 13.0000 N S ## 3: 0 2649 7.2250 N C ## 4: 0 2631 7.2250 N C ## 5: 0 330959 7.8792 N Q ## 6: 0 349216 7.8958 N S full=full[, Age2:=mean(Age,na.rm=TRUE)][is.na(Age), Age:=Age2][,-c(&quot;Age2&quot;)] # replace missing age with average # replace missing fare with average fare full[is.na(Fare)] ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket ## 1: 1044 NA 3 Storey, Mr. Thomas male 60.5 0 0 3701 ## Fare Cabin Embarked ## 1: NA N S full=full[, Fare2:=mean(Fare,na.rm=TRUE)][is.na(Fare), Fare:=Fare2][,-c(&quot;Fare2&quot;)] # The title of the passenger can affect his survive: # extract title full[,Title := gsub(&#39;(.*, )|(\\\\..*)&#39;, &#39;&#39;, Name)] full[Title %in% c(&#39;Mlle&#39;,&#39;Ms&#39;,&#39;Mme&#39;,&#39;Lady&#39;,&#39;Dona&#39;), Title:= &#39;Miss&#39;] full[Title %in% c(&#39;Capt&#39;,&#39;Col&#39;,&#39;Don&#39;,&#39;Dr&#39;,&#39;Jonkheer&#39;,&#39;Major&#39;,&#39;Rev&#39;,&#39;Sir&#39;,&#39;the Countess&#39;), Title:= &#39;Officer&#39;] # Let&#39;s see how many unique levels for each variables apply(full,2, function(x) length(unique(x))) ## PassengerId Survived Pclass Name Sex Age ## 1309 3 3 1307 2 99 ## SibSp Parch Ticket Fare Cabin Embarked ## 7 8 929 282 7 3 ## Title ## 5 # Group Parch and SibSp into categorical variables table(full$Parch) ## ## 0 1 2 3 4 5 6 9 ## 1002 170 113 8 6 6 2 2 table(full$SibSp) ## ## 0 1 2 3 4 5 8 ## 891 319 42 20 22 6 9 full[,Parch2:=ifelse(Parch&gt;2,3,Parch), by=.(PassengerId)] full[,SibSp2:=ifelse(SibSp&gt;4,4,SibSp), by=.(PassengerId)] # Let&#39;s move the features Survived, Pclass, Sex, Embarked to be factors cols&lt;-c(&quot;Pclass&quot;,&quot;Sex&quot;,&quot;Embarked&quot;,&quot;Title&quot;,&quot;Cabin&quot;,&quot;SibSp2&quot;,&quot;Parch2&quot;) full[,(cols):=lapply(.SD, as.factor),.SDcols=cols] full[,Survived:=factor(Survived,levels=c(0,1), labels = c(&quot;D&quot;,&quot;S&quot;))] summary(full) ## PassengerId Survived Pclass Name Sex ## Min. : 1 D :549 1:323 Length:1309 female:466 ## 1st Qu.: 328 S :342 2:277 Class :character male :843 ## Median : 655 NA&#39;s:418 3:709 Mode :character ## Mean : 655 ## 3rd Qu.: 982 ## Max. :1309 ## ## Age SibSp Parch Ticket ## Min. : 0.17 Min. :0.0000 Min. :0.000 Length:1309 ## 1st Qu.:22.00 1st Qu.:0.0000 1st Qu.:0.000 Class :character ## Median :29.88 Median :0.0000 Median :0.000 Mode :character ## Mean :29.88 Mean :0.4989 Mean :0.385 ## 3rd Qu.:35.00 3rd Qu.:1.0000 3rd Qu.:0.000 ## Max. :80.00 Max. :8.0000 Max. :9.000 ## ## Fare Cabin Embarked Title Parch2 SibSp2 ## Min. : 0.000 A: 22 C:270 Master : 61 0:1002 0:891 ## 1st Qu.: 7.896 B: 65 Q:123 Miss :267 1: 170 1:319 ## Median : 14.454 C: 94 S:916 Mr :757 2: 113 2: 42 ## Mean : 33.295 D: 46 Mrs :197 3: 24 3: 20 ## 3rd Qu.: 31.275 E: 41 Officer: 27 4: 37 ## Max. :512.329 F: 21 ## N:1020 25.2 Random Forest LT=nrow(train) LT2=nrow(full) formula=Survived~ Pclass+Sex+Age+Fare+Cabin+Title+SibSp2+Parch2 # Create trainControl object: myControl myControl &lt;- trainControl( method = &quot;cv&quot;, number = 5, summaryFunction = twoClassSummary, classProbs = TRUE, # IMPORTANT! verboseIter = TRUE ) # Train glm with custom trainControl: model model &lt;- train( formula, tuneLength = 4, full[1:LT], method = &quot;ranger&quot;, trControl = myControl ) ## Warning in train.default(x, y, weights = w, ...): The metric &quot;Accuracy&quot; was not ## in the result set. ROC will be used instead. ## + Fold1: mtry= 2, min.node.size=1, splitrule=gini ## - Fold1: mtry= 2, min.node.size=1, splitrule=gini ## + Fold1: mtry= 8, min.node.size=1, splitrule=gini ## - Fold1: mtry= 8, min.node.size=1, splitrule=gini ## + Fold1: mtry=15, min.node.size=1, splitrule=gini ## - Fold1: mtry=15, min.node.size=1, splitrule=gini ## + Fold1: mtry=22, min.node.size=1, splitrule=gini ## - Fold1: mtry=22, min.node.size=1, splitrule=gini ## + Fold1: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold1: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold1: mtry= 8, min.node.size=1, splitrule=extratrees ## - Fold1: mtry= 8, min.node.size=1, splitrule=extratrees ## + Fold1: mtry=15, min.node.size=1, splitrule=extratrees ## - Fold1: mtry=15, min.node.size=1, splitrule=extratrees ## + Fold1: mtry=22, min.node.size=1, splitrule=extratrees ## - Fold1: mtry=22, min.node.size=1, splitrule=extratrees ## + Fold2: mtry= 2, min.node.size=1, splitrule=gini ## - Fold2: mtry= 2, min.node.size=1, splitrule=gini ## + Fold2: mtry= 8, min.node.size=1, splitrule=gini ## - Fold2: mtry= 8, min.node.size=1, splitrule=gini ## + Fold2: mtry=15, min.node.size=1, splitrule=gini ## - Fold2: mtry=15, min.node.size=1, splitrule=gini ## + Fold2: mtry=22, min.node.size=1, splitrule=gini ## - Fold2: mtry=22, min.node.size=1, splitrule=gini ## + Fold2: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold2: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold2: mtry= 8, min.node.size=1, splitrule=extratrees ## - Fold2: mtry= 8, min.node.size=1, splitrule=extratrees ## + Fold2: mtry=15, min.node.size=1, splitrule=extratrees ## - Fold2: mtry=15, min.node.size=1, splitrule=extratrees ## + Fold2: mtry=22, min.node.size=1, splitrule=extratrees ## - Fold2: mtry=22, min.node.size=1, splitrule=extratrees ## + Fold3: mtry= 2, min.node.size=1, splitrule=gini ## - Fold3: mtry= 2, min.node.size=1, splitrule=gini ## + Fold3: mtry= 8, min.node.size=1, splitrule=gini ## - Fold3: mtry= 8, min.node.size=1, splitrule=gini ## + Fold3: mtry=15, min.node.size=1, splitrule=gini ## - Fold3: mtry=15, min.node.size=1, splitrule=gini ## + Fold3: mtry=22, min.node.size=1, splitrule=gini ## - Fold3: mtry=22, min.node.size=1, splitrule=gini ## + Fold3: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold3: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold3: mtry= 8, min.node.size=1, splitrule=extratrees ## - Fold3: mtry= 8, min.node.size=1, splitrule=extratrees ## + Fold3: mtry=15, min.node.size=1, splitrule=extratrees ## - Fold3: mtry=15, min.node.size=1, splitrule=extratrees ## + Fold3: mtry=22, min.node.size=1, splitrule=extratrees ## - Fold3: mtry=22, min.node.size=1, splitrule=extratrees ## + Fold4: mtry= 2, min.node.size=1, splitrule=gini ## - Fold4: mtry= 2, min.node.size=1, splitrule=gini ## + Fold4: mtry= 8, min.node.size=1, splitrule=gini ## - Fold4: mtry= 8, min.node.size=1, splitrule=gini ## + Fold4: mtry=15, min.node.size=1, splitrule=gini ## - Fold4: mtry=15, min.node.size=1, splitrule=gini ## + Fold4: mtry=22, min.node.size=1, splitrule=gini ## - Fold4: mtry=22, min.node.size=1, splitrule=gini ## + Fold4: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold4: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold4: mtry= 8, min.node.size=1, splitrule=extratrees ## - Fold4: mtry= 8, min.node.size=1, splitrule=extratrees ## + Fold4: mtry=15, min.node.size=1, splitrule=extratrees ## - Fold4: mtry=15, min.node.size=1, splitrule=extratrees ## + Fold4: mtry=22, min.node.size=1, splitrule=extratrees ## - Fold4: mtry=22, min.node.size=1, splitrule=extratrees ## + Fold5: mtry= 2, min.node.size=1, splitrule=gini ## - Fold5: mtry= 2, min.node.size=1, splitrule=gini ## + Fold5: mtry= 8, min.node.size=1, splitrule=gini ## - Fold5: mtry= 8, min.node.size=1, splitrule=gini ## + Fold5: mtry=15, min.node.size=1, splitrule=gini ## - Fold5: mtry=15, min.node.size=1, splitrule=gini ## + Fold5: mtry=22, min.node.size=1, splitrule=gini ## - Fold5: mtry=22, min.node.size=1, splitrule=gini ## + Fold5: mtry= 2, min.node.size=1, splitrule=extratrees ## - Fold5: mtry= 2, min.node.size=1, splitrule=extratrees ## + Fold5: mtry= 8, min.node.size=1, splitrule=extratrees ## - Fold5: mtry= 8, min.node.size=1, splitrule=extratrees ## + Fold5: mtry=15, min.node.size=1, splitrule=extratrees ## - Fold5: mtry=15, min.node.size=1, splitrule=extratrees ## + Fold5: mtry=22, min.node.size=1, splitrule=extratrees ## - Fold5: mtry=22, min.node.size=1, splitrule=extratrees ## Aggregating results ## Selecting tuning parameters ## Fitting mtry = 8, splitrule = gini, min.node.size = 1 on full training set # Print model to console # Higher AUC (area under curve is better model) plot(model) # in-sample prediction p=predict(model, full, type = &quot;prob&quot;) p_survival=ifelse(p$S&gt;0.5, 1, 0) # Create confusion matrix confusionMatrix(factor(p_survival[1:LT]),factor(train$Survived)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 536 39 ## 1 13 303 ## ## Accuracy : 0.9416 ## 95% CI : (0.9242, 0.9561) ## No Information Rate : 0.6162 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8748 ## ## Mcnemar&#39;s Test P-Value : 0.0005265 ## ## Sensitivity : 0.9763 ## Specificity : 0.8860 ## Pos Pred Value : 0.9322 ## Neg Pred Value : 0.9589 ## Prevalence : 0.6162 ## Detection Rate : 0.6016 ## Detection Prevalence : 0.6453 ## Balanced Accuracy : 0.9311 ## ## &#39;Positive&#39; Class : 0 ## # sensitivity: correct negative # Specificity: correct positive rate # out-of-sample prediction submission=data.frame(PassengerId=test$PassengerId, Survived=p_survival[(LT+1):LT2]) fwrite(submission,&quot;data/titanic/submission.csv&quot;) The overall in-sample prediction accuracy is 94.2%, meaning our prediction in 94.2% cases are correct. The accuracy should be broken into two additional measures: sensitivity and specificity. Sensitivity is the metric that evaluates a models ability to predict true positives of each available category. In our case, we predict 97.45% of the death (the case of 0). Specificity: the proportion of observed negatives that were predicted to be negatives. In our case, we predict 88.89% of the survival (the case of 1). Now, we can submit the prediction to kaggle and our score is 0.75837, which is ranked 47668. In order to improve the ranking, there are typically two ways: feature engineering: preprocess the column in the model try a different prediction model. You will learn more predictive model in more advanced classes. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
